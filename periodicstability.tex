\documentclass[thesis.tex]{subfiles}

\begin{document}

\iffulldocument\else
	\chapter{KdV5 with periodic boundary conditions}
\fi

\section{Proof of Stability Theorems}

\subsection{Gap and Conjugation Lemmas}

We first state and prove a gap and conjugation lemmas. This follows almost exactly from \cite{Zumbrun2009}, except we allow the parameter vector $\Lambda$ to live in a Banach space rather than in a subset of $\C^m$. The notation in the two lemmas is the same as in \cite{Zumbrun2009}. First, we state and prove the gap lemma.

% Gap lemma
\begin{lemma}[Gap Lemma]\label{gaplemma}
Let $W \in \C^N$, and consider the family of ODEs on $\R$
\begin{equation}\label{LambdaEVP}
W(x)' = A(x; \Lambda) W
\end{equation}
where $\Lambda \in \Omega$ is a parameter vector and $\Omega$ is a Banach space. Assume that
\begin{enumerate}
	\item The map $\Lambda \mapsto A(\cdot; \Lambda)$ is analytic in $\Lambda$.
	\item $A(x; \Lambda) \rightarrow A_\pm(\lambda)$ (independent of $\Lambda$) as $x \rightarrow \pm \infty$, and there exists $\delta > 0$ such that for $|\Lambda| < \delta$ we have the uniform exponential decay estimates 
	\begin{align}\label{ALambdadecay}
	\left| \frac{\partial^k}{\partial x^k} A(x; \Lambda) - A_\pm(\Lambda) \right| 
	&\leq C e^{-\theta |x|} && 0 \leq k \leq K
	\end{align}
	where $\alpha > 0$, $C > 0$, and $K$ is a nonnegative integer.
\end{enumerate}
Suppose $V^-(\Lambda)$ is an eigenvector of $A_-(\Lambda)$ with corresponding eigenvalue $\mu(\Lambda)$, both analytic in $\Lambda$. Then there exists a unique solution of \ref{LambdaEVP} of the form 
\begin{equation}
W(x; \Lambda) = V(x; \Lambda) e^{\mu(\Lambda)x}
\end{equation}
where $V$ is $C^1$ in $x$ and analytic in $\Lambda$ for $|\Lambda| < \delta$, and for any fixed $\tilde{\theta} < \theta$
\begin{align}
V(x; \Lambda) = V^-(\Lambda) + \mathcal{O}(e^{-\tilde{\theta}|x|}|V^-(\Lambda)|) && x \in \R^-
\end{align}
A similar result holds for $A_+(\lambda)$ on $\R^+$.

\begin{proof}
This proof follows \cite{Zumbrun2009}, with the main difference being that the parameter vector $\Lambda$ is in a general Banach space instead of a subset of $C^p$. Let $W(x; \Lambda) = V(x; \Lambda) e^{\mu(\Lambda) x}$. Substituting this into \eqref{LambdaEVP} and simplifying, we obtain the equivalent ODE
\begin{equation}\label{VEVP}
V(x; \Lambda)' = (A_-(\Lambda) - \mu(\Lambda)I)V(x; \Lambda) + \Theta(x; \Lambda) V(x; \Lambda)
\end{equation}
where $\Theta(x; \Lambda) = (A(x; \Lambda) - A_-(\Lambda)) = \mathcal{O}(e^{-\theta|x|})$ by \eqref{ALambdadecay}. Choose any $\tilde{\theta} < \theta_1 < \theta$ such that for $|\Lambda| < \delta$, the real part of the spectrum of $A_-(\Lambda)$ lies either to the left or to the right of the vertical line $\text{Re}(\nu) = \text{Re}(\mu(\Lambda) + \theta_1$ in the complex plane. Since the eigenvalues of $A_(\Lambda)$ are analytic in $\Lambda$, this is possible.

For $|\Lambda| < \delta$, define the spectral projections $P(\Lambda)$ and $Q(\Lambda)$, where $P(\Lambda)$ projects onto the direct sum of all eigenspaces of $A_-(\Lambda)$ corresponding to eigenvalues $\nu$ with $\text{Re}(\nu) < \text{Re}(\mu(\Lambda) + \theta_1$, and $Q(\Lambda)$ projects onto the direct sum of all eigenspaces of $A_-(\Lambda)$ corresponding to eigenvalues $\nu$ with $\text{Re}(\nu) > \text{Re}(\mu(\Lambda) + \theta_1$. $P(\Lambda)$ and $Q(\Lambda)$ are analytic in $\Lambda$ for $|\Lambda| < \delta$, and from the definition of $\theta_1$, we have the estimates
\begin{align*}
\left|e^{(A_-(\Lambda) - \mu(\Lambda)I)x}P \right| &\leq C e^{\theta_1 x} && x \geq 0 \\
\left|e^{(A_-(\Lambda) - \mu(\Lambda)I)x}Q \right| &\leq C e^{\theta_1 x} && x \leq 0
\end{align*}
Note that $P(\Lambda) + Q(\Lambda) = I$. Define the map $T$ on $L^\infty(-\infty, -M]$ by
\begin{align*}
TV(x; \Lambda) &= V^-(\Lambda) 
+ \int_{-\infty}^x e^{(A_-(\Lambda) - \mu(\Lambda)I)(x-y)}P\Theta(y; \Lambda) V(y; \Lambda) dy \\
&- \int_x^{-M} e^{(A_-(\Lambda) - \mu(\Lambda)I)(x-y)}Q\Theta(y; \Lambda) V(y; \Lambda) dy
\end{align*}
Taking the absolute value of both sides, for $x \leq 0$
\begin{align*}
|TV(x; \Lambda)| &\leq |V^-(\Lambda)| + C ||V(x; \Lambda)||_{L^\infty(-\infty, -M]}
\left( \int_{-\infty}^x e^{\theta_1 (x - y)} e^{\theta y} dy + \int_x^{-M} e^{\theta_1 (x - y)} e^{\theta y} dy \right) \\
&\leq |V^-(\Lambda)| + C ||V(x; \Lambda)||_{L^\infty(-\infty, -M]} e^{\theta_1 x} \int_{-\infty}^M e^{(\theta - \theta_1) y} dy \\
&= \leq |V^-(\Lambda)| + C ||V(x; \Lambda)||_{L^\infty(-\infty, -M]} e^{\theta_1 x} \frac{e^{-(\theta - \theta_1)M}}{\theta - \theta_1}\\
&\leq |V^-(\Lambda)| + C ||V(x; \Lambda)||_{L^\infty(-\infty, -M]} e^{\theta_1 x} e^{-(\theta - \theta_1)M} \\
&\leq |V^-(\Lambda)| + C ||V(x; \Lambda)||_{L^\infty(-\infty, -M]} e^{-(\theta - \theta_1)M} \\
& < \infty
\end{align*}
Since the RHS is independent of $x$, we have $T: L^\infty(-\infty, -M] \rightarrow L^\infty(-\infty, -M]$. 

Next, we show $T$ is a contraction.
\begin{align*}
|TV_1(x; \Lambda) - TV_2(x; \Lambda)| &\leq C ||V_1(x; \Lambda) - V_2(x; \Lambda)||_{L^\infty(-\infty, -M]} e^{\theta_1 x} \frac{e^{-(\theta - \theta_1)M}}{\theta - \theta_1}\\
\end{align*}
Since $e^{-(\theta - \theta_1)M} \rightarrow 0$ as $m \rightarrow \infty$, for sufficiently large $M$ we have 
\begin{align*}
|TV_1(x; \Lambda) - TV_2(x; \Lambda)|_{L^\infty(-\infty, -M]} &\leq \frac{1}{2} ||V_1(x; \Lambda) - V_2(x; \Lambda)||_{L^\infty(-\infty, -M]} 
\end{align*}
Thus the map $T$ is a contraction. Since $L^\infty(-\infty, -M]$ is a Banach space, by the Banach fixed point theorem, the map $T$ has a unique fixed point $V = TV$, i.e. we have a function $V \in L^\infty(-\infty, -M]$ such that 
\begin{align*}
V(x; \lambda) &= V^-(\Lambda) 
+ \int_{-\infty}^x e^{(A_-(\Lambda) - \mu(\Lambda)I)x}P\Theta(y; \Lambda) V(y; \Lambda) dy 
- \int_x^{-M} e^{(A_-(\Lambda) - \mu(\Lambda)I)x}Q\Theta(y; \Lambda) V(y; \Lambda) dy
\end{align*}

Differentiating this with respect to $x$, we obtain
\begin{align*}
V'(x; \Lambda) &= P\Theta(x; \Lambda) V(x; \Lambda) +
(A_-(\Lambda) - \mu(\Lambda)I) \int_{-\infty}^x e^{(A_-(\Lambda) - \mu(\Lambda)I)(x-y)}P\Theta(y; \Lambda) V(y; \Lambda) dy \\
&-(-Q\Theta(x; \Lambda) V(x; \Lambda))
-(A_-(\Lambda) - \mu(\Lambda)I) \int_x^{-M} e^{(A_-(\Lambda) - \mu(\Lambda)I)(x-y)}Q\Theta(y; \Lambda) V(y; \Lambda) dy \\
&= P\Theta(x; \Lambda) V(x; \Lambda) + Q\Theta(y; \Lambda) V(x; \Lambda) + (A_-(\Lambda) - \mu(\Lambda)I)(T V(x; \lambda) - V^-(\Lambda) ) \\
&= (P + Q)\Theta(x; \Lambda) V(x; \Lambda) + (A_-(\Lambda) - \mu(\Lambda)I)(V(x; \lambda) - V^-(\Lambda) ) \\
&= (A_-(\Lambda) - \mu(\Lambda)I)V(x; \lambda) + \Theta(x; \Lambda) V(x; \Lambda) - (A_-(\Lambda) - \mu(\Lambda)I)V^-(\Lambda) \\
&= (A_-(\Lambda) - \mu(\Lambda)I)V(x; \lambda) + \Theta(x; \Lambda) V(x; \Lambda)
\end{align*}
where we used the fact that $TV = V$ and $(A_-(\Lambda) - \mu(\Lambda)I)V^-(\Lambda) = 0$. Thus $V(x; \Lambda$ solves \eqref{VEVP}. Since $TV = V$, we let $V_1 = V$ and $V_2 = 0$ in the above to get the estimate
\begin{align*}
|V(x; \Lambda) - V^-(\Lambda)| &= |T(V(x; \Lambda)) - T(0)| \\
&\leq C ||V(x; \Lambda) - 0||_{L^\infty(-\infty, -M]} e^{\theta_1 x} \\
\end{align*}

Similarly, for sufficiently large $M$, we have
\begin{align*}
|V(x; \Lambda)| - |V^-(\Lambda)| &\leq | |V(x; \Lambda)| - |V^-(\Lambda)| | \\
&\leq |V(x; \Lambda) - V^-(\Lambda)| \\
&= |T(V(x; \Lambda)) - T(0)| \\
&\leq \frac{1}{2} ||V(x; \Lambda)||_{L^\infty(-\infty, -M]}
\end{align*}
thus it follows that
\begin{align*}
||V(x; \Lambda)||_{L^\infty(-\infty, -M]} \leq 2 |V^-(\Lambda)|
\end{align*}

Combining these, we have
\begin{align*}
|V(x; \Lambda) - V^-(\Lambda)| &\leq C e^{\tilde{\theta} x}|V^-(\Lambda)| \\
\end{align*}
from which we get
\begin{align*}
|V(x; \Lambda) = V^-(\Lambda) + \mathcal{O}( e^{\tilde{\theta} x}|V^-(\Lambda)| )\\
\end{align*}

Although $V(x; \Lambda)$ is only defined for $x \leq -M$, we extend $V(x; \Lambda)$ to all of $R^-$ using the evolution operator for the system.
\end{proof}
\end{lemma}

As a corollary to this, we state and prove the Conjugation Lemma, which allows us to make a smooth change of coordinates to convert the linear ODE $Z'(x) = A^\pm(x) Z(x)$ into a constant coefficient system.

% Conjugation lemma
\begin{lemma}[Conjugation Lemma]
Let $W \in \C^N$, and consider the family of ODEs on $\R$
\begin{equation}\label{EVPconj}
W(x)' = A(x; \Lambda) W(x) + F(x) 
\end{equation}
where $\Lambda \in \Omega$ is a parameter vector and $\Omega$ is a Banach space. Take the same assumptions as in the Gap Lemma, i.e. 
\begin{enumerate}
	\item The map $\Lambda \mapsto A(\cdot; \Lambda)$ is analytic in $\Lambda$.
	\item $A(x; \Lambda) \rightarrow A_\pm(\lambda)$ (independent of $\Lambda$) as $x \rightarrow \pm \infty$, and there exists $\delta > 0$ such that for $|\Lambda| < \delta$ we have the uniform exponential decay estimates 
	\begin{align}
	\left| \frac{\partial^k}{\partial x^k} A(x; \Lambda) - A_\pm(\Lambda) \right| 
	&\leq C e^{-\theta |x|} && 0 \leq k \leq K
	\end{align}
	where $\alpha > 0$, $C > 0$, and $K$ is a nonnegative integer.
\end{enumerate}
Then in a neighborhood of any $\Lambda_0 \in \Omega$ there exist invertible linear transformations
\begin{align*}
P_+(x, \Lambda) &= I + \Theta_+(x, \Lambda) \\
P_-(x, \Lambda) &= I + \Theta_-(x, \Lambda) 
\end{align*}
defined on $\R^+$ and $\R^-$, respectively, such that
\begin{enumerate}[(i)]
\item The change of coordinates $W = P_\pm Z$ reduces \eqref{EVPconj} to the equations on $\R^\pm$
\begin{align}\label{conjZ}
Z'(x) = A^\pm(\Lambda) Z(x) + P_\pm(x, \Lambda)^{-1} F(x)
\end{align}

\item For any fixed $0 < \tilde{\theta} < \theta$, $0 \leq k \leq K+1$, and $j \geq 0$ we have the decay rates
\begin{align*}
\left| \partial_\Lambda^j \partial_x^k \Theta_\pm \right| \leq C(j, k)e^{-\tilde{\theta}|x|}
\end{align*}
\end{enumerate}
\begin{proof}
We will prove this for the case where $F(x) = 0$. The form of the conjugated system easily follows for general $F$. We will also only consider the case on $\R^-$. The other case is similar. 

Let $W = P_-(x, \Lambda) Z$, where we will determine $P_-(x, \Lambda)$ later. Suppose that \eqref{conjZ} holds. Substituting \eqref{conjZ} into \eqref{EVPconj}, we get
\begin{align*}
[P_-(x, \Lambda) Z(x)]' &= A(x; \Lambda)(P_-(x, \Lambda) Z(x)) \\
P_-'(x, \Lambda) Z(x) + P_-(x, \Lambda) Z'(x)
&= A(x; \Lambda)P_-(x, \Lambda) Z(x) \\
P_-'(x, \Lambda) Z(x) + P_-(x, \Lambda) A_- Z(x)
&= A(x; \Lambda)P_-(x, \Lambda) Z(x)
\end{align*}
Rearranging this, we obtain
\begin{equation}
P_-'(x, \Lambda) Z(x)
= [A(x; \Lambda)P_-(x, \Lambda) - P_-(x, \Lambda) A_-]Z(x)
\end{equation}

Suppose now that
\[
P_-'(x, \Lambda) = A(x; \Lambda)P_-(x, \Lambda) - P_-(x, \Lambda) A_-
\]
Upon making the substitution $W = P_-(x, \Lambda) Z$, \eqref{EVPconj} reduces to
\begin{align*}
[P_-(x, \Lambda) Z(x)]' &= A(x; \Lambda)(P_-(x, \Lambda) Z(x)) \\
P_-'(x, \Lambda) Z(x) + P_-(x, \Lambda) Z'(x)
&= A(x; \Lambda)P_-(x, \Lambda) Z(x) \\
(A(x; \Lambda)P_-(x, \Lambda) - P_-(x, \Lambda) A_-)Z(x) + P_-(x, \Lambda) Z'(x)
&= A(x; \Lambda)P_-(x, \Lambda) Z(x) \\
A(x; \Lambda)P_-(x, \Lambda)Z(x) - P_-(x, \Lambda) A_- Z(x) + P_-(x, \Lambda) Z'(x)
&= A(x; \Lambda)P_-(x, \Lambda) Z(x) \\
P_-(x, \Lambda) Z'(x) &= P_-(x, \Lambda) A_- Z(x) \\
Z'(x) &= A_- Z(x)
\end{align*}
where the last line follows if $P_-(x, \Lambda)$ is invertible. Thus we only need to verify that this is the case. In other words, we need to find $P_-(x, \Lambda)$ such that
\[
P_-'(x, \Lambda) = A(x; \Lambda)P_-(x, \Lambda) - P_-(x, \Lambda) A_-
\]
We note that the this equation has the form 
\begin{equation}\label{solvePminus}
P_-'(x, \Lambda) = \mathcal{A}(x; \Lambda) P_-(x, \Lambda)
\end{equation}
where $\mathcal{A}(x; \Lambda)$ is the linear operator
\[
\mathcal{A}(x; \Lambda) P = A(x; \Lambda) P - P A_-
\]
By our assumptions on $A(x; \Lambda)$, $\mathcal{A} \rightarrow \mathcal{A}_-$ as $x \rightarrow -\infty$, where the limiting linear operator $\mathcal{A}_-$ is defined by
\[
\mathcal{A}_- P = A_- P - P A_-
\]
The limiting operator has analytic eigenvalue/eigenvector pair $0, I$ for all $\Lambda$, thus by the Gap Lemma, there exists a solution of \eqref{solvePminus} of the form 
\begin{equation*}
P_-(x, \Lambda) = I + \mathcal{O}(e^{-\tilde{\theta}|x|})
\end{equation*}
In other words, 
\begin{equation*}
P_-(x, \Lambda) = I + \Theta_-(x, \Lambda)
\end{equation*}
where 
\begin{equation}\label{Thetabound}
|\Theta_-(x, \Lambda)| \leq C e^{-\tilde{\theta}|x|}
\end{equation}
The $x$-derivative bound follow from the derivative bounds in the Gap Lemma, and the $\Lambda$-derivative bounds follow from standard analytic function theory.

Finally, we need to show that $P_-(x, \Lambda)$ is invertible for all $x \in \R^-$. Using \eqref{Thetabound}, we can find $M$ sufficiently large and negative such that for all $x \leq M$,
\[
|\Theta_-(x, \Lambda)| < 1/2
\]
It follows that $P_-(x, \Lambda)$ is invertible for $X \leq M$. To extend invertibility to all $x \in \R^-$, suppose that $P_-(x, \Lambda)^{-1}$ exists for all $x \in R^-$. Then, differentiating $P_-(x, \Lambda)^{-1} P_-(x, \Lambda) = I$ and solving for $[P_-(x, \Lambda)^{-1}]'$ (as in the proof of the inverse function theorem), we have (suppressing the dependence on $\Lambda$ for convenience)
\begin{align*}
(P_-^{-1})'(x) &= -P_-^{-1}(x)P_-'(x)P_-^{-1}(x) \\
&= -P_-^{-1}(x)( A(x)P_-(x) - P_-(x) A_-)P_-^{-1}(x) \\
&= A_- P_-^{-1}(x) - A(x) P_-^{-1}(x)
\end{align*}
We have a solution to this ODE for $x \leq M$, and by variation of constants, this ODE has a unique solution for all $x \in \R^-$. Thus $P_-(x, \Lambda)^{-1}$ is obtained for all $x \in \R^-$ by evolving this ODE forward from an initial condition at some $x \leq M$. In this manner, we have shown that $P_-(x, \Lambda)^{-1}$ exists for all $x \in \R^-$.
\end{proof}
\end{lemma}

\subsection{Proof of Lemma \ref{eigA0lemma}}

Before we prove Lemma \ref{eigA0lemma}, we will prove a general result from finite dimensional linear algebra.

\begin{lemma}\label{kernelprojlemma}
Let $A$ be an $n \times n$ matrix with simple kernel spanned by $v$, and let $\ker A^* = \text{span}(w)$. Then 
\begin{enumerate}[(i)]
\item We can scale $v$ and/or $w$ so that $\langle v, w \rangle = 1$.
\item Having chosen the scaling in part (i), the projection on the kernel of $A$ is given by
\[
P_{\ker A} = \langle w, \cdot \rangle v
\]
\end{enumerate}
\begin{proof}
For part (i), if $\langle v, w \rangle = 0$, then $v \perp \ker A^*$, thus by the Fredholm alternative we can solve $A u = v$. This is not possible since $A$ has simple kernel. Since $\langle v, w \rangle \neq 0$ we can scale $v$ and/or $w$ so that $\langle v, w \rangle = 1$.

For part (ii), Let $0, \lambda_2, \dots, \lambda_n$ be the eigenvalues (with multiplicity) of $A$ with corresponding generalized eigenvectors $v, v_2, \dots, v_n$. Let $P u = \langle w, u \rangle v$. First, let $u \in \ker A$. Then $u = \alpha v$ for $\alpha \in \R$, thus 
\[
P u = \langle w, \alpha v \rangle v = \alpha \langle w, v \rangle v = \alpha v = u
\]
Since $A$ is diagonalizable, its eigenfunctions form a basis for $\R^n$. Let $u \in R^n$ and write $u$ in the eigenbasis as
\[
u = \alpha v + \sum_{k=2}^n \alpha_k v_k
\]
For $k = 2, \dots, n$, since $\lambda_k \neq 0$
\begin{align*}
\langle w, v_k \rangle &= \langle w, \frac{1}{\lambda_k}\lambda_k v_k \rangle \\
&= \langle w, \frac{1}{\lambda_k} A v_k \rangle \\
&= \langle w, A \frac{1}{\lambda_k} v_k \rangle \\
&= \langle A^* w, \frac{1}{\lambda_k} v_k \rangle \\
&= 0
\end{align*}
Thus we have
\[
P u = \alpha v
\]
\end{proof}
\end{lemma}

Now we prove Lemma \ref{eigA0lemma}. $DF(0)$ is a $2m \times 2m$ constant coefficient matrix of the form
\begin{equation}\label{DF0}
DF(0) = \begin{pmatrix}
0 & 1 & 0 & \dots & 0 & 0 \\
0 & 0 & 1 & \dots & 0 & 0 \\
& &  & \ddots &  & & \\
0 & 0 & 0 & \dots & 0 & 1 \\
-c & 0 & c_2 &
 \dots & c_{2m-2} & 0
\end{pmatrix}
\end{equation}
where $c_k = \partial_{u_{k+1}}f(0)$. The reversibility relations \eqref{frev} imply that the coefficients $c_k$ in the bottom row are all 0 for $k$ odd. In addition, $c_0 = -c$. The characteristic polynomial of $DF(0)$ is
\begin{equation}\label{charpolyDF0}
p_1(\nu) = \nu^{2m} - c_{2m-2} \nu^{2m-2} - \dots - c_2 \nu^2 + c
\end{equation}

$A(0)$ is the $(2m+1) \times (2m+1)$ constant coefficient matrix
\begin{equation}\label{A0}
A(0) = \begin{pmatrix}
0 & 1 & \dots & 0 & 0 \\
0 & 0 & \dots & 0 & 0 \\
& & \ddots  \\
0 & 0 & \dots & 0 & 1 \\
0 & -c & \dots & c_{2m-2} & 0
\end{pmatrix}
\end{equation}
where the constants $c_i$ are the same as in $DF(0)$. The characteristic polynomial of $A(0)$ is 
\begin{equation}\label{charpolyA0}
p_2(\nu) = -\nu^{2m+1} + c_{2m-2} \nu^{2m-1} + \dots + c_2 \nu^3- c \nu = -\nu p_1(\nu)
\end{equation}
Thus $A(0)$ has the same eigenvalues as $DF(0)$ as well as an additional eigenvalue at 0. The eigenvalues of $DF(0)$ are characterized in Hypothesis \ref{hypeqhyp}.

By assumption, $A(0)$ has simple kernel. It can be verified directly that 
\begin{align*}
V_0 &= (-1/c, 0, 0, \dots, 0)^T \\
W_0 &= (-c, 0, c_2, 0, c_4, 0, \dots, 0, c_{2m-2}, 0, 1)^T 
\end{align*}
are eigenvectors of $A(0)$ and $-A(0)^*$ (respectively) corresponding to eigenvalue 0; we have scaled $V_0$ so that $\langle V_0, W_0 \rangle = 1$. The expression for the projection on the kernel of $A(0)$ comes from Lemma \ref{kernelprojlemma}.

% nondegeneracy lemma
\subsection{Proof of Lemma \ref{nondegenlemma}}
Since $Q(x)$ is a homoclinic orbit in the intersection of $W^u(0)$ and $W^s(0)$, $\R Q'(0) \subset T_{Q(0)}W^s(0) \cap T_{Q(0)}W^u(0)$. If the intersection were more than one-dimensional, there would exist another exponentially localized solution $V(x) = (v_1, \dots, v_{2m}, v_{2m+1})^T$ to \eqref{vareq2}. It follows that $\tilde{V}(x) = (v_1, \dots, v_{2m})^T$ would be an exponentially localized solution to \eqref{vareq1}, which contradicts \eqref{nondegencond}.

% solutions to variational equation
\subsection{Proof of Lemma \ref{varsolutions}}

We employ the following dimension counting argument. Recall that $\dim W^{s/u}(0) = m$ and $\dim W^c(0) = 1$, thus $\dim W^{cs}(0) = m + 1$, and $\dim W^{cu}(0) = m + 1$. $\Psi(0) \perp T_{Q(0)}W^{cs}(0) + T_{Q(0)}W^{cu}(0)$, so $\dim T_{Q(0)}W^{cs}(0) + T_{Q(0)}W^{cu}(0) \leq 2m$. By counting dimensions, this implies that $\dim T_{Q(0)}W^{cs}(0) \cap T_{Q(0)}W^{cu}(0) = 2$. Since $\dim T_{Q(0)}W^s(0) \cap T_{Q(0)}W^s(0) = 1$ by Lemma \ref{nondegenlemma}, we conclude that there exists $Y_0 \in T_{Q(0)}W^{cs}(0) \cap T_{Q(0)}W^{cu}(0)$ which is linearly independent from $Q'(0)$, and $Y^0 \notin T_{Q(0)}W^s(0) \cap T_{Q(0)}W^s(0)$.

Using the Gap Lemma, we can find solutions $V^\pm(x)$ to the variational equation \eqref{vareq2} on $\R^\pm$ such that
\begin{equation}\label{Vplusminus}
V^\pm(x) = V_0 + \mathcal{O}(e^{-(\alpha - \epsilon)|x|}|V_0|)
\end{equation}
for some $\epsilon > 0$, so $V^\pm(x) \rightarrow V_0$ as $x \rightarrow \pm \infty$. These will not be unique, since, for example, we can add any component in $Y^+ \oplus \R Q'(0)$ to $V^+(0)$ and keep the same decay property. Since $V^\pm(x)$ remains bounded but does not decay to 0, $V^\pm(0)$ must have a component in $Y^0$ and cannot have a component in $Y^-$. If $V^+(0)$ has any component in $Y^+ \oplus \R Q'(0)$, we can subtract it out and keep the decay property in \eqref{Vplusminus}. Thus we can take $V^+(0) \in Y^0$. Similarly, we can take $V^0(0) \in Y^0$. Since both initial conditions are in $Y^0$, $V^\pm(x)$ remain bounded for all $x$.

By reversibility, $RV^+(-x)$ is also a solution to the variational equation on $R^-$, with 
\begin{align*}
R V^+(-x) &= R V_0 + \mathcal{O}(e^{-(\alpha - \epsilon)|x|}|V_0|) \\
&= V_0 + \mathcal{O}(e^{-(\alpha - \epsilon)|x|}|V_0|)
\end{align*}
where $R V_0 = V_0$ since all components of $V_0$ are 0 except for the first. Reversing time swaps $Y^+$ and $Y^-$, while leaving $\R Q'(0)$ and $Y^0$ intact. Thus $RV^+(0) \in Y_0$, and so both $V^+(0)$ and $RV^+(0)$ are in the one-dimensional space $Y^0$; since the odd numbered components of $V^+(0)$ and $RV^+(0)$ are the equal and $Y^0$ is one-dimensional, it follows that $V^+(0) = RV^+(0)$.

Since we have the same initial condition at $x = 0$ for $V^+(x)$ and $RV^+(-x)$, this implies that $V^+(-x) = RV^+(x)$. Let $V^c(x) = V^+(x)$. Then $V^c(x)$ is a bounded solution to the variational equation with is symmetric with respect to the reversor $R$, i.e. $V^c(-x) = RV^c(x)$. Furthermore, $V^c(x) \rightarrow V_0$ as $x \rightarrow \pm \infty$.

% solutions to variational equation
\subsection{Proof of Lemma \ref{adjsolutions}}

The linear operator $A(Q(x))$ has the form 
\begin{equation}\label{DefAQ}
A(Q(x)) = \begin{pmatrix}
0 & 1 & \dots & 0 & 0 \\
0 & 0 & \dots & 0 & 0 \\
& & \ddots  \\
0 & 0 & \dots & 0 & 1 \\
h_1(x) & h_2(x) & \dots & h_{2m}(x) & h_{2m+1}(x)
\end{pmatrix}
\end{equation} 
where
\begin{equation*}
\begin{aligned}
h_1(x) &= \sum_{k=1}^{2m}\partial^2_{u_1 u_k} f(Q(x))\partial_x^k q(x) \\
h_j(x) &= \sum_{k=1}^{2m}\partial^2_{u_j u_k} f(Q(x))\partial_x^k q(x) + \partial_{u_{j-1}}f(Q(x)) && j = 2, \dots, 2m \\
h_{2m+1}(x) &= \partial_{u_{2m}}f(Q(x))
\end{aligned}
\end{equation*}
Using the reversibility relation $Q(-x) = R Q(x)$ and \eqref{frev},
\begin{align*}
h_j(-x) &= \sum_{k=1}^{2m}\partial^2_{u_j u_k} f(Q(-x))\partial_x^k q(-x) + \partial_{u_{j-1}}f(Q(-x)) \\
&= \sum_{k=1}^{2m}\partial^2_{u_j u_k} f(RQ(x))\partial_x^k q(-x) + \partial_{u_{j-1}}f(RQ(x)) \\
&= \sum_{k=1}^{2m} (-1)^{j+k} \partial^2_{u_j u_k} f(Q(x)) (-1)^k \partial_x^k q(x) + (-1)^j \partial_{u_{j-1}}f(Q(x)) \\
&= (-1)^j \left( \sum_{k=1}^{2m} \partial^2_{u_j u_k} f(Q(x)) \partial_x^k q(x) + (-1)^j \partial_{u_{j-1}}f(Q(x)) \right) \\
&= (-1)^j h(x)
\end{align*} 
thus $h_j(x)$ is an odd function for $j$ odd and an even function for $j$ even. Since $f$ is smooth and $Q(x)$ is exponentially localized, the functions $h_j(x)$ are smooth and bounded.

The adjoint variational equation \eqref{adjvareq2} is the following equation written as a first order system
\begin{equation}\label{adj2}
[\partial_x H_c]^* w(x) = -H_c \partial_x w(x) = 0,
\end{equation}
Suppose \eqref{adj2} has a solution $w(x)$ which is an even function. Then by Lemma \ref{adjointlemma}, \eqref{adjvareq2} has a solution $W(x) = (w_1(x), \dots, w_{2m+1}(x))$ with $w_{2m+1}(x) = w(x)$. Comparing \eqref{Axform} from Lemma \ref{adjointlemma} to \eqref{DefAQ}, the other components of $W(x)$ are given by 
\begin{align}\label{adjWform}
w_r(x) &= (-1)^{2m+1-r} \partial_x^{2m+1-r}w(x) + \sum_{k=r}^{2m} (-1)^{k-r} \partial_x^{k-r}(h_{k+1}(x) w(x)) && r = 1, \dots 2m
\end{align}
Since we are assuming $w(x)$ is even, $w_r(x)$ is an even function for $r$ odd and an odd function for $r$ even, thus $W(-x) = R W(x)$. 

To find $\Psi(x)$, equation \ref{adj2} has an exponentially decaying solution $q(x)$, which is an even function. Thus we obtain $\Psi(x)$ by taking $w(x) = q(x)$ above. Every term in \eqref{adjWform} is a product of $q(x)$ or one of its derivatives with a smooth, bounded function. Since $q(x)$ is exponentially localized, $\Psi(x)$ is exponentially localized as well.

To find $\Psi^c(x)$, equation \eqref{adj2} also has the constant solution 1 as a solution, which is an even function. Thus we obtain $\Psi(x)$ by taking $w(x) = 1$ above. The individual entries are given by
\begin{align}\label{Psicrows}
[\Psi^c(x)]_r &= \sum_{k=r}^{2m} (-1)^{k-r} \partial_x^{k-r} h_{k+1}(x)
\end{align}
When we take $|x| \rightarrow \infty$, $h_{k+1}(x) \rightarrow \partial_{u_k}f(0)$, which is equal to $c_{k-1}$ for $k$ even and 0 for $k$ odd, and $c_0 = -c$. All higher derivatives of $h_{k+1}$ vanish. Thus we have
\begin{align*}
\lim_{|x|\rightarrow \infty}
[\Psi^c(x)]_r &= c_{r-1}
\end{align*}
Since $[\Psi^c(x)]_{2m+1} = 1$, we conclude that $\Psi^c(x) \rightarrow (-c, 0, c_2, \dots, c_{2m-2}, 0, 1)^T = W_0$. Alternatively, we can verify by direct substitution that $\Psi^c(x) = ( -Df(Q(x)), 1)$. Then as $|x| \rightarrow 0$, $\Psi^c(x) \rightarrow W_0$.

Using this, we have
\begin{align*}
\langle \Psi^c(x), Q'(x) \rangle 
&= \langle (-Df(Q(x)), 1), ((\partial_x q(x), \dots, \partial_x^{2m} q(x)), \partial_{2m+1}(x)) \rangle \\
&= -\langle Df(Q(x)), (\partial_x q(x), \dots, \partial_x^{2m} q(x)) \rangle + \partial_{2m+1}(x) \\
&= \partial_{2m+1}(x) - \partial_x f(q(x)) \\
&= 0
\end{align*}
since $q(x)$ is an equilibrum solution to the original PDE. Of course, we already knew that $\langle \Psi^c(x), Q'(x) \rangle = 0$ since the inner product is constant and $Q'(x)$ decays to 0. We also have (NEEDS BETTER NOTATION)
\begin{align*}
\langle \Psi^c(x), Q'(-x) \rangle 
&= \langle (-Df(Q(x)), 1), (Q'(-x), \partial_{2m+1}(-x)) \rangle \\
&= \langle (-Df(Q(x)), 1), (-RQ'(x), -\partial_{2m+1}(x)) \rangle \\
&= \langle Df(Q(x)), RQ'(x) \rangle - \partial_{2m+1}(x) \\
&= \langle R Df(R Q(x)), RQ'(x) \rangle - \partial_{2m+1}(x) \\
&= \frac{\partial}{\partial x} R f(R Q(x)) - \partial_{2m+1}(x) \\
&= \frac{\partial}{\partial x} f(Q(x)) - \partial_{2m+1}(x) \\
&= 0
\end{align*}

Finally, it follows from Lemma \ref{adjointlemma} that any bounded solution to \eqref{adjvareq2} must be perpendicular to $\R Q'(0) \oplus Y^+ \oplus Y^-$ at $x = 0$. In particular, this holds for $\Psi(0)$ and $\Psi^c(0)$. Since $S_1 = \R Q'(0) \oplus Y^+ \oplus Y^-$ is a $(2m-1)-$dimensional subspace of $\R^{2m+1}$ and $S_2 = \Psi(0) \oplus \Psi^c(0)$ is a $2-$dimensional subspace of $\R^{2m+1}$ which is perpendicular to $S_1$, it follows that there can be no other bounded, linearly independent solutions to \eqref{adjvareq2}. 

% proof of nu lambda lemma
\subsection{Proof of Lemma \ref{nulambdalemma}}

The characteristic polynomial of $A(0; \lambda)$ is
\begin{equation}\label{charpolyA0lambda}
p(\nu; \lambda) = p_2(\nu) + \lambda = -\nu^{2m+1} + c_{2m-2} \nu^{2m-1} + \dots + c_2 \nu^3 - c \nu + \lambda
\end{equation}
where $p_2(\nu)$ is the characteristic polynomial of $A(0)$ from \eqref{charpolyA0}

When $\lambda = 0$, $p(0; \nu)$ has a root at $\nu = 0$. From Lemma \ref{nondegenlemma}, $\nu = 0$ is simple root and is located a distance $\sqrt{\alpha_0^2 + \beta_0^2}$ from the next smallest roots. 

Since $p(0; 0) = 0$ and $\partial_\nu p(0; 0) = -c \neq 0$, we can use the IFT to can solve for $\nu$ in terms of $\lambda$ near $\lambda = 0$. In other words, there exists $\delta_0 > 0$ and a smooth function $\nu(\lambda)$ such that $\nu(0) = 0$ and for $|\lambda| < \delta$, $p(\nu(\lambda)$ is the unique solution to $p(\nu; \lambda) = 0$. Differentiating $\nu(\lambda)$ at $\lambda = 0$, we get
\begin{align*}
\nu'(0) &= -\frac{1}{\partial_\nu p_2(\nu(\lambda); \lambda) } \partial_\lambda p ( \nu(\lambda); \lambda ) \Big|_{\lambda = 0}\\
&= -\frac{1}{\partial_\nu p(\nu(0); 0) } 
&= = \frac{1}{c}
\end{align*}

Since $\nu(\lambda)$ is the root of a polynomial which is smooth in $\lambda$, we can expand $\nu(\lambda)$ in a Taylor series about $\lambda = 0$ to get
\begin{align*}
\nu(\lambda) = \frac{1}{c} \lambda + \mathcal{O}(|\lambda|^2)
\end{align*}
By reversibility, $p(\nu; \lambda)$ only involves odd powers of $\nu$, thus $p(\nu; \lambda) = 0$ implies $p(-\nu; -\lambda) = 0$. Uniqueness of the solution $\nu(\lambda)$ from the IFT implies that $\nu(-\lambda) = -\nu(\lambda)$, i.e. $\nu(\lambda)$ is an odd function of $\lambda$. Thus the Taylor expansion for $\nu(\lambda)$ involves only odd powers of $\lambda$, and we have]
\begin{align*}
\nu(\lambda) = \frac{1}{c} \lambda + \mathcal{O}(|\lambda|^3)
\end{align*}

Finally, we consider the case when $\lambda$ is pure imaginary. Take $\lambda = i \gamma$ and $\nu = i s$ in $p(\nu; \lambda)$ to get
\begin{align*}
p(i s; i \gamma) = (-1)^{m+1} i s^{2m+1} + c_{2m-2} (-1)^m i s^{2m-1} + \dots - c_2 i s^3 - c i s + i \gamma
\end{align*}
Since we want to solve $p(i s; i \gamma) = 0$, we can divide by $i$ to get the equilvent problem
\begin{align*}
\tilde{p}(s; \gamma) = (-1)^{m+1} s^{2m+1} + c_{2m-2} (-1)^m s^{2m-1} + \dots - c_2 s^3 - c s + \gamma = 0
\end{align*}
Since $\tilde{p}(0; 0) = 0$ and $\partial_s \tilde{p}(0; 0) = -c \neq 0$, we can use the IFT again to solve for $s$ in terms of $\gamma$. Thus we can find a smooth function $s(\gamma)$ such that $s(0) = 0$, $s(\gamma)$ is real, and $s(\gamma)$ is the unique solution to $\tilde{p}(s; \gamma) = 0$ for sufficiently small $\gamma$. Undoing these substitutions, this implies that $p(\lambda, i s(-i \lambda)) = 0$. By uniqueness of the IFT solution $\nu(\lambda)$, we conclude that $\nu(\lambda) = \nu(i \gamma) = i s(-i \lambda)$, which is pure imaginary.

\subsection{Solutions in Center Subspace}

Consider the equations
\begin{align}
V'(x) = A(Q(x); \lambda) V(x) \label{Veqlambda} \\
W'(x) = -A(Q(x); \lambda)^* W(x) \label{Weqlambda}
\end{align}\
where $Q(x)$ is the primary pulse solution and $A(Q(x), \lambda) = A(Q(x)) + \lambda B$. When $\lambda = 0$, these are the variational and adjoint variational equations.

By Lemma \ref{nulambdalemma}, the asymptotic matrix $A(\lambda)$ has a small eigenvalue $\nu(\lambda)$, with $\nu(\lambda) = \frac{1}{c} \lambda + \mathcal{O}(|\lambda|^3)$. Let $V_0(\lambda)$ and $W_0(\lambda)$ be the eigenvectors of $A(\lambda)$ and $-A(\lambda)^*$ corresponding to $\nu(\lambda)$. Since the eigenvalues of $A(\lambda)$ are smooth in $\lambda$, $V_0(0) = 0$, scale $V_0(\lambda)$ and $W_0(\lambda)$ so that $V_0(0) = V_0$, $W_0(\lambda) = 0$, and $\langle V_0(\lambda), W_0(\lambda) \rangle = 1$.

Using the Gap Lemma, we can find solutions $V^\pm(x; \lambda)$ to \eqref{Veqlambda} and $W^\pm(x; \lambda)$ to \eqref{Weqlambda} on $\R^\pm$ such that
\begin{align*}
V^\pm(x; \lambda) &= V_0(\lambda)e^{\nu(\lambda)x} + \tilde{V}^\pm(x; \lambda) \\
W^\pm(x; \lambda) &= W_0(\lambda)e^{\nu(\lambda)x} + \tilde{W}^\pm(x; \lambda)
\end{align*}
where $\tilde{V}^\pm(x; \lambda), \tilde{W}^\pm(x; \lambda) = \mathcal{O}(e^{-\tilde{\alpha}|x|})$. Furthermore, these solutions are analytic in $\lambda$. Scale these solutions so that $V^\pm(x; 0) = V^c(x)$ and $W^\pm(x; 0) = \Psi^c(x)$, where $V^c(x)$ is defined in Lemma \ref{varsolutions} and $\Psi^c(x)$ is defined in Lemma \ref{adjsolutions}.

Using a Melnikov-type argument, we should be able to show the following.

\begin{lemma}\label{Vpmlambdalemma}
\begin{align*}
\langle \Psi^c(0), V^+(0; \lambda) \rangle &= 1 + \mathcal{O}(|\lambda|) \\
\langle \Psi^c(0), V^-(0; \lambda) \rangle &= 1 + \mathcal{O}(|\lambda|) 
\end{align*}
and
\begin{align*}
\langle \Psi(0), V^+(0; \lambda) \rangle &= M_\Psi \lambda + \mathcal{O}(|\lambda|^2) \\
\langle \Psi(0), V^-(0; \lambda) \rangle &= -M_\Psi \lambda + \mathcal{O}(|\lambda|^2) 
\end{align*}
where $M_\Psi$ is a constant.
\end{lemma}

\subsection{Piecewise Formulation}

We are finally ready to prove the main stability results. Let $q_{np}(x)$ be a $n-$periodic pulse solution to \eqref{genODE} constructed according to Theorem \ref{perexist}, and let $Q_{np}(x) = (q_{np}(x), \partial_x q_{np}(x), \dots, \partial_x^{2m} q_{np}(x) )$. From Theorem \ref{perexist}, we can write $Q_{np}(x)$ piecewise as
\begin{equation}\label{Qnppiece}
\begin{aligned}
Q_i^-(x) &= Q^-(x; \beta_i^-) + \tilde{Q}_i^-(x) && x \in [-X_{i-1}, 0] \\
Q_i^+(x) &= Q^+(x; \beta_i^+) + \tilde{Q}_i^+(x) && x \in [0, X_i]
\end{aligned}
\end{equation}

Recall that we are looking for solutions to the eigenvalue problem 
\begin{equation}\label{PDEeig4}
V'(x) = A(Q_{np}(x))V(x) + \lambda B V(x)
\end{equation}
which comes from writing the PDE eigenvalue problem $\partial_x E''(q_{np}(x)) v = \lambda v$ as a first-order system. We can verify directly that $\partial_x E''(q_{np}(x)) q_{np}(x) = 0$. By a similar analysis to that preceding Hypothesis \ref{Melnikov2hyp}(BUT WITH DIFFERENT FUNCTION SPACES WHICH WE SHOULD SPECIFY), there exists a function $t_{np}(x)$ such that $\partial_x E''(q_{np}(x)) t_{np}(x) = q_{np}(x)$. Let $T_np(x) = t_{np}(x), \partial_x t_np(x), \dots, \partial_x^m t_np(x))$. Then we have the following relations.
and that we have
\begin{equation}\label{Arelations}
\begin{aligned}
(Q_{np}')' &= A(Q_{np}(x)) Q_{np}' \\
(T_{np})'' &= A(Q_{np}(x)) T_{np} + B Q_{np}' 
\end{aligned}
\end{equation}

Using the same intervals $\{X_0, \dots, X_{n-1}\}$, we can write $T_np(x)$ piecewise as $T_i^\pm(x)$, where
\begin{equation}\label{Tnppiece}
\begin{aligned}
T_i^-(x) &= T(x) + \tilde{T}_i^-(x) && x \in [-X_{i-1}, 0] \\
T_i^+(x) &= T(x) + \tilde{T}_i^+(x) && x \in [0, X_i]
\end{aligned}
\end{equation}
Bounds on the remainder terms $T_i^\pm(x)$ will be presented in Lemma \ref{stabestimates} below.

To exploit the relations \eqref{Arelations}, we take the following piecewise ansatz for the eigenfunction $V(x)$
\begin{equation}\label{Vpiecewise}
\begin{aligned}
V_i^-(x) &= d_i ((Q_i^-)'(x) + \lambda T_i^-(x)) + W_i^-(x) && x \in [-X_{i-1}, 0] \\
V_i^+(x) &= d_i ((Q_i^+)'(x) + \lambda T_i^+(x)) + W_i^+(x) && x \in [0, X_i] \\
\end{aligned}
\end{equation}
where $d_i \in \C$ are arbitrary constants. Substituting this ansatz into \eqref{PDEeig4}, we have
\begin{align*}
d_i [(Q_i^\pm)']'(x) &+ \lambda d_i (T_i^\pm)'(x) + (W_i^\pm)'(x) \\
&= d_i A(Q_i^\pm(x)) (Q_i^\pm)'(x) + \lambda d_i A(Q_i^\pm(x)) T_i^\pm(x) + A(Q_i^\pm(x)) W_i^\pm(x) \\
&+ \lambda d_i B (Q_i^\pm)'(x) + d_i \lambda^2 B T_i^\pm(x) + \lambda B W_i^\pm(x)
\end{align*}
Using \eqref{Arelations}, this simplifies to
\begin{align*}
(W_i^\pm)'(x) &= A(Q_i^\pm(x)) W_i^\pm(x) + \lambda B W_i^\pm(x) + d_i \lambda^2 B T_i^\pm(x) 
\end{align*}
Let $A_i^\pm(x; \lambda) = A(Q_i^\pm(x)) + \lambda B$ and let
\begin{align*}
\tilde{H}_i^\pm(x) &= B T_i^\pm(x) \\ 
H(x) &= B T(x)
\end{align*}
Then this becomes 
\begin{align*}
(W_i^\pm)'(x) &= A_i^\pm(x; \lambda) W_i^\pm(x) + d_i \lambda^2 \tilde{H}_i^\pm(x)
\end{align*}

In order to have a solution to \eqref{PDEeig4}, the eigenfunction $V(x)$ must be continuous. In other words, the $2n$ jumps between the pieces must be 0. Thus we need to solve the system of equations
\begin{align*}
(W_i^\pm)'(x) &= A_i^\pm(x; \lambda) W_i^\pm(x) + d_i \lambda^2 \tilde{H}_i^\pm(x) \\
W_i^+(X_i) - W_{i+1}^-(-X_i) &= D_i d && i = 0, \dots, n-1 \\
W_i^-(0) &= W_i^+(0) && i = 0, \dots, n-1  \\
\end{align*}
where
\begin{align}\label{defDid}
D_i d &= d_{i+1}[(Q_{i+1}^-)'(-X_i) + \lambda T_{i+1}^-(-X_i)]
- d_i [ (Q_i^+)'(X_i) + \lambda T_i^+(X_i) ] \\
\end{align}

It will turn out that we cannot find a unique solution to this sytem for all $\lambda$. Thus, we will instead consider the system
\begin{equation}\label{eigsystem}
\begin{aligned}
(W_i^\pm)'(x) &= A_i^\pm(x; \lambda) W_i^\pm(x) + d_i \lambda^2 \tilde{H}_i^\pm(x) \\
W_i^+(X_i) - W_{i+1}^-(-X_i) &= D_i d && i = 0, \dots, n-1 \\
W_i^\pm(0) &\in \R \Psi(0) \oplus \R \Psi^c(0) \oplus Y^+ \oplus Y^- && i = 0, \dots, n-1  \\
W_i^+(0) - W_i^-(0) &\in \R \Psi(0) \oplus \R \Psi^c(0) && i = 0, \dots, n-1 
\end{aligned}
\end{equation}

We do not need to include a component in $Q'(0)$ in the third equation since that is handled by the term $d_i (Q_i^\pm)'(x)$ in the ansatz \eqref{Vpiecewise}. A solution to \eqref{eigsystem} gives us an eigenfunction corresponding to eigenvalue $\lambda$ if and only if the $n$ jumps at $x = 0$ in the direction of $\Psi(0) \oplus \R \Psi^c(0)$ are 0, i.e. if and only if the $2n$ jump conditions are satisfied, i.e. 
\begin{align}
\xi_i &= \langle \Psi(0), W_i^+(0) - W_i^-(0) \rangle = 0 \label{jumpxi} \\
\xi_i^c &= \langle \Psi^c(0), W_i^+(0) - W_i^-(0) \rangle = 0 \label{jumpxic}
\end{align}
for $i = 0, \dots, n-1$.

We will need the following estimates, which we collect in the following lemma.
\begin{lemma}\label{stabestimates}
Let $\Delta H_i^\pm(x) = \tilde{H}_i^\pm(x) - H(x)$. Then we have the following estimates
\begin{enumerate}[(i)]
\item $|H(x)|, |\tilde{H}_i^\pm(x)| \leq C e^{-\alpha_0 |x|}$
\item $|\Delta H_i^-(x)| \leq C e^{-\alpha_0 X_{i-1}} e^{-\alpha_0(X_{i-1} + x) } + e^{-2 \alpha_0 X_i} e^{\alpha_0 x}$
\item $|\Delta H_i^+(x)| \leq C e^{-\alpha_0 X_i} e^{-\alpha_0(X_i - x) } + e^{-2 \alpha_0 X_{i-1}} e^{-\alpha_0 x}$
\item $||\Delta H_i^\pm|| \leq C(e^{-\alpha_0 X_i} + e^{-\alpha_0 X_{i-1}} )$ 
\item $D_i d = ( Q'(X_i) + Q'(-X_i) + Q^r(X_i))(d_{i+1} - d_i ) + \mathcal{O} ( e^{-\alpha X_i} |\lambda| )$
where
\[
|Q_R(X_i)| \leq C e^{-\alpha_0 X_i} e^{-\alpha_0 X_m}
\]
\end{enumerate}
\begin{proof}
By Hypothesis \ref{Melnikov2hyp}(i), $T(x)$ is exponentially localized, thus $|H(x)| \leq C e^{-\alpha_0 |x|}$. We can use Lin's method, as in the proof of Theorem \ref{perexist}, to solve for the remainder functions $\tilde{T}_i^\pm(x)$ in \eqref{Tnppiece}. Since the details are similar to that proof, they will be omitted. The jump conditions from Lin's method are automatically satisfied, since $T_np(x)$ is a continuous function. We conclude from Lin's method that, as in Lemma \ref{solvewithjumps}, we have bounds on the remainder functions
\begin{align*}
|\tilde{T}_i^-(x)| &\leq C e^{-\alpha_0 X_{i-1}} e^{-\alpha_0(X_{i-1} + x) } + e^{-2 \alpha_0 X_i} e^{\alpha_0 x} \\
|\tilde{T}_i^+(x)| &\leq C e^{-\alpha_0 X_i} e^{-\alpha_0(X_i - x) } + e^{-2 \alpha_0 X_{i-1}} e^{-\alpha_0 x}
\end{align*}
The bounds (i)-(iv) come from these estimates and the definitions of $H$ and $\tilde{H}_i^\pm$.

For estimate (v), we use \eqref{VQpm} in Lemma \ref{solvewithjumps} and \eqref{Vpiecewise} (with $\tilde{Q}$ in place of $V$) to get
\begin{align*}
Q_{i+1}^-(-X_i) &= Q^-(-X_i; \beta_{i+1}^-) + \tilde{Q}_{i+1}^-(-X_i) \\
&= Q^-(-X_i; \beta_{i+1} ^-) + Q^+(X_i; \beta_i^+) + \mathcal{O}(e^{-2 \alpha_0 X_i}) \\
&= Q(-X_i) + Q(X_i) 
+ \mathcal{O}(e^{-\alpha_0 X_i}(e^{-\alpha_0 X_{i-1}}+e^{-\alpha_0 X_i}+e^{-\alpha_0 X_{i+1}}))
\end{align*}
Since the bounds \ref{solvewithjumps} also apply to derivatives with respect to $x$, we have
\begin{align*}
(Q_{i+1}^-)'(-X_i) &= Q'(-X_i) + Q'(X_i) + \mathcal{O}(e^{-\alpha_0 X_i}e^{-\alpha_0 X_m})
\end{align*}
We write this as 
\begin{align*}
(Q_{i+1}^-)'(-X_i) &= Q'(-X_i) + Q'(X_i) + Q^R(X_i)
\end{align*}
where $Q^R(X_i)$ is a remainder term with
\[
|Q_R(X_i)| \leq C e^{-\alpha_0 X_i} e^{-\alpha_0 X_m}
\]
Similarly,
\begin{align*}
(Q_i^+)'(X_i)' &= Q'(-X_i) + Q'(X_i) + \mathcal{O}(e^{-\alpha_0 X_i}e^{-\alpha_0 X_m})
\end{align*}
As part of the construction of the periodic multipulse using Lin's method, we peformed a match at $\pm X_i$. Thus we have
$(Q_{i+1}^-)'(-X_i) = Q_i^+(X_i)$. In particular, this implies \begin{align*}
(Q_i^+)'(X_i)' &= Q'(-X_i) + Q'(X_i) + Q^R(X_i)
\end{align*}
To get the estimate (v), we substitute these into \eqref{Did2} and use the fact that $|\partial_c Q'(x)| \leq C e^{-\alpha |x|}$.
\end{proof}
\end{lemma}

\subsection{Conjugation}

To simplify the system, we would like to apply a change of coordinates so that the linear operator $A_i^\pm(x; \lambda)$ is transformed into a constant coefficient matrix. To do this we will use the Conjugation Lemma. For all $\lambda$, $A^\pm(x; \lambda)$ decays exponentially to the constant-coefficient matrix $A(\lambda)$. Specifically, 
\[
|A^\pm(x; \lambda) - A(\lambda)| \leq C e^{\alpha_0 |x|}
\]
where
\begin{equation}\label{Alambda}
A(\lambda) = \begin{pmatrix}
0 & 1 & \dots & 0 & 0 \\
0 & 0 & \dots & 0 & 0 \\
& & \ddots  \\
0 & 0 & \dots & 0 & 1 \\
\lambda & -c & \dots & c_{2m-2} & 0
\end{pmatrix}
\end{equation}

Before we continue, we define the following constants, which we will use throughout.
\begin{enumerate}
	\item Choose $\eta > 0$ sufficiently small so that $\alpha_0 - 4 \eta > 0$. Let
	\begin{align*}
	\alpha &= \alpha_0 - \eta \\
	\tilde{\alpha} &= \alpha - 3 \eta
	\end{align*}

	\item Choose $\delta < \delta_0$ sufficiently small so that for all $|\lambda| < \delta$
	\begin{enumerate}
		\item $|\nu(\lambda)| < \eta$, where $\nu(\lambda)$ is the small eigenvalue of $A(\lambda)$ defined in Lemma \ref{nulambdalemma}.
		\item The real part of any other eigenvalue of $A(\lambda)$ lies outside the interval $[-\alpha, \alpha]$. This is possible since the eigenvalues of $A(\lambda)$ are smooth functions of $\lambda$.
	\end{enumerate}

	\item Choose $X_m$ sufficiently large so that
	\begin{equation}
	e^{-\tilde{\alpha} X_m} < \delta
	\end{equation}
	where $X_m = \min\{X_0, \dots, X_{n-1}\}$.

	We can always choose a smaller $\delta$ later if needed.
\end{enumerate}

We can now use the conjugation lemma. We will apply it separately to each linear operator $A_i^\pm(x; \lambda)$. Define the Banach spaces
\begin{align*}
\Omega^- &= \C \times C([-X_{i-1}, 0], \R^{2m+1}) \\
\Omega^+ &= \C \times C([0, X_i], \R^{2m+1})
\end{align*}
$A_i^\pm(x; \lambda)$ is analytic in $\Lambda = (\lambda, Q_i^\pm(x))$. Using the conjugation lemma, let $P_i^\pm(x; \lambda, Q_i^\pm(x) )$ be the conjugation operator for $A_i^\pm(x; \lambda)$. For convenience, define 
\begin{equation}\label{defPipm}
P_i^\pm(x; \lambda) = P^\pm(x; \lambda, Q_i^\pm(x) )
\end{equation}
In addition, let $P^\pm(x)$ be the conjugation operators on $\R^\pm$ for $\Lambda = (0, Q(x))$ and $P^\pm(x; \lambda)$ be the conjugation operators on $\R^\pm$ for $\Lambda = (\lambda, Q(x))$. By the conjugation lemma, we can write these projections as
\begin{equation}\label{projTheta}
\begin{aligned}
P_i^\pm(x; \lambda) &= I + \Theta_i^\pm(x; \lambda) \\
P^\pm(x; \lambda) &= I + \Theta^\pm(x; \lambda)
P^\pm(x) &= I + \Theta^\pm(x)
\end{aligned}
\end{equation}
where we have the decay rates
\begin{equation}\label{Thetadecay}
\begin{aligned}
|\Theta_i^\pm(x; \lambda)| &\leq C e^{-\alpha |x|} \\
|\Theta^\pm(x; \lambda)| &\leq C e^{-\alpha |x|} \\
|\Theta^\pm(x)| &\leq C e^{-\alpha |x|}
\end{aligned}
\end{equation}
and $\alpha = \alpha_0 - \eta$. By the Conjugation Lemma, these decay rates also apply to derivatives with respect to $x$ and $\Lambda$.

We will want to be able to write $\Theta_i^\pm(x; \lambda)$ as a small perturbation of $\Theta^\pm(x)$. To do this, we expand $\Theta_i^\pm(x; \lambda)$ in a Taylor series about $(\lambda, U^\pm(x)) = (0, Q_i^\pm(x))$ to get
\begin{align*}
\Theta_i^\pm(x; \lambda) &= \Theta^\pm(x) + (D_\lambda \Theta^\pm(x))\lambda + (D_{U^\pm(x)}\Theta^\pm(x))( Q_i^\pm(x) - Q(x) ) + \mathcal{O}(|\lambda|^2 + ||Q_i^\pm - Q||^2 )
\end{align*}
Using the estimate from Lemma \ref{solvewithjumps}, this simplifies to
\begin{align}\label{ThetaTaylor}
\Theta_i^\pm(x; \lambda) &= \Theta^\pm(x) + e^{-\alpha |x|}( |\lambda| + e^{-2 \alpha X_m}) 
\end{align}
At $x = 0$, this gives us the estimate
\begin{equation}\label{PTaylor}
P_i^\pm(0; \lambda) = P^\pm(0) + \mathcal{O}(|\lambda| + e^{-2 \alpha X_m})
\end{equation}
If we only do not want to eliminate $\lambda$, these become
\begin{align}\label{PTaylorlambda}
\Theta_i^\pm(x; \lambda) &= \Theta^\pm(x; \lambda) + e^{-\alpha |x|}e^{-2 \alpha X_m} \\
P_i^\pm(0; \lambda) &= P^\pm(0; \lambda) + \mathcal{O}(e^{-2 \alpha X_m})
\end{align}

Using the projections $P_i^\pm(x; \lambda)$ from the conjugation lemma, we make the substitution $W_i^\pm = P_i^\pm(x; \lambda) Z_i^\pm$ in \eqref{eigsystem} to obtain the system
\begin{subequations}
\begin{align}
(Z_i^\pm(x))' &= A(\lambda) Z_i^\pm(x) + \lambda^2 d_i P_i^\pm(x; \lambda)^{-1} \tilde{H}_i^\pm(x) \label{systemZ} \\
P_i^+(X_i; \lambda) Z_i^+(X_i) &- P_{i+1}^-(-X_i; \lambda) Z_{i+1}^-(-X_i; \lambda) = D_i d \label{systemmiddle} \\
P_i^\pm(0; \lambda) Z_i^\pm(0) &\in Y^+ \oplus Y^- \oplus \C \Psi(0) \oplus \C \Psi^c(0) \label{systemcenter1} \\
P_i^+(0; \lambda) Z_i^+(0) - P_i^-(0; \lambda) Z_i^-(0) &\in \C \Psi(0) \oplus \C \Psi^c(0) \label{systemcenter2}
\end{align}
\end{subequations}
and the jump conditions become
\begin{equation}\label{jumpcondZ}
\begin{aligned}
\langle \Psi(0), P_i^+(0; \lambda) Z_i^+(0) - P_i^-(0; \lambda) Z_i^-(0) \rangle &= 0 \\
\langle \Psi^c(0), P_i^+(0; \lambda) Z_i^+(0) - P_i^-(0; \lambda) Z_i^-(0) \rangle &= 0
\end{aligned}
\end{equation}

\subsection{Evolution operator}

Since $A(\lambda)$ does not depend on $x$, we know exactly how solutions of the constant coefficient ODE $V' = A(\lambda)V$ will behave. Let $E^{u/s/c}(0)$ be the stable/unstable/center eigenspaces of $A(0)$. $E^s(0)$ and $E^u(0)$ are $m-$dimensional, and $E^c(0)$ is 1-dimensional. For $|\lambda| < \delta$, let $E^{u/s/c}(\lambda)$ be the corresponding eigenspaces of $A(\lambda)$. In particular, we note that $E^c(\lambda) = \text{span}\{ \nu(\lambda) \}$, where $\nu(\lambda)$ is the small eigenvalue of $A(\lambda)$ defined in Lemma \ref{nulambdalemma}. Although $E^c(\lambda)$ may not be a true center eigenspace, we retain this notation for convenience. 

Let $\Phi(x, y; \lambda) = e^{A(\lambda)(x-y)}$ be the evolution of the constant-coefficient ODE
\[
Z' = A(\lambda) Z
\]
and let $\Phi^{u/s/c}(x, y; \lambda) = \Phi(x, y; \lambda)P^{u/s/c}_0(\lambda)$ be the evolutions on the respective eigenspaces. For these evolutions, for $|\lambda| < \delta$ we have bounds
\begin{equation}\label{Zevolbounds}
\begin{aligned}
|\Phi^s(x, y; \lambda)| &\leq C e^{-\alpha(x - y)} \\
|\Phi^u(x, y; \lambda)| &\leq C e^{-\alpha(y - x)} \\
|\Phi^c(x, y; \lambda)| &\leq C e^{\eta|x - y|} 
\end{aligned}
\end{equation}
Since $E^c(\lambda)$ is one-dimensional, we have a formula for $\Phi^c(x, y; \lambda)$.
\begin{align}\label{centerevol}
\Phi^c(x, y; \lambda) v &= e^{\nu(\lambda)(x - y)} v && v \in E^c(\lambda)
\end{align}

Finally, we will look at the variational and adjoint variational e
equations for the linearization about the primary pulse $Q(x)$. Recall that these are given by 
\begin{align*}
V_i'(x) &= A(Q(x); 0) V_i(x) \\
W_i'(x) &= -A(Q(x); 0)^* W_i(x)
\end{align*}

WE MAY WANT TO RENAME THIS THETA

Let $\Theta(y, x)$ be the evolution operator for the variational equation. Then $\Theta(x, y)^*$ is the evolution operator for the adjoint variational equation. Recall that $P^\pm(x)$ conjugates $A(Q(x); 0)$. From the conjugation lemma, we have the following relationship between $\Theta(y, x)$ and $\Phi(y, x; 0)$.
\begin{align*}
\Theta(y, x) &= P^+(y) \Phi(y, x; 0) P^+(x)^{-1} && x, y \geq 0 \\
\Theta(y, x) &= P^-(y) \Phi(y, x; 0) P^-(x)^{-1} && x, y \leq 0
\end{align*}

\subsection{Inversion}

Define the spaces
\begin{align*}
V_a &= \bigoplus_{i=0}^{n-1} E^u(\lambda) \oplus E^s(\lambda) \\
V_b &= \bigoplus_{i=0}^{n-1} E^u(0) \oplus E^s(0) \\
V_c &= \bigoplus_{i=0}^{n-1} E^c(\lambda) \\
V_{\tilde{c}} &= \bigoplus_{i=0}^{n-1} E^c(\lambda) \\
V_\lambda &= B_\delta(0) \subset \C
\end{align*}
where the subscripts are all taken $\mod n$, since we are on a periodic domain. All the product spaces are endowed with the maximum norm, e.g. for $V_c$, 
\[
|c| = \max(|c_0^-|, \dots, |c_{n-1}^-|, |c_0^+|, \dots, |c_{n-1}^+|)
\]
In addition, we take the following notational convention. If we eliminate a subscript or superscript, we are taking the maximum over the eliminated subscript or superscript. For example,
\begin{enumerate}
	\item $|c_i| = \max(|c_i^+|, |c_i^-|)$ 
	\item $|c^+| = \max(|c_0^+|, \dots, |c_{n-1}^+|)$
\end{enumerate}

As in \cite{Sandstede1998}, we will solve the eigenvalue problem in a series of inversion steps. First, we look at equation \eqref{systemZ}
\[
(Z_i^\pm(x))' = A(\lambda) Z_i^\pm(x) + \lambda^2 d_i P_i^\pm(x; \lambda)^{-1} \tilde{H}_i^\pm(x) 
\]
Using the variation of constants formula and the eigenprojections for $A(\lambda)$, we can write \eqref{systemZ} as the set of fixed point equations
\begin{equation}\label{Zfpeq}
\begin{aligned}
Z_i^-(x) &= \Phi^s(x, -X_{i-1}; \lambda) a_{i-1}^- + \Phi^u(x, 0; \lambda) b_i^- + \Phi^c(x, -X_{i-1}; \lambda) c_{i-1} \\
&+ \lambda^2 d_i \int_0^x \Phi^u(x, y; \lambda) P_i^-(y; \lambda)^{-1} \tilde{H}_i^-(y)] dy \\
&+ \lambda^2 d_i \int_{-X_{i-1}}^x \Phi^s(x, y; \lambda) P_i^-(y; \lambda)^{-1} \tilde{H}_i^-(y) dy \\
&+ \lambda^2 d_i \int_{-X_{i-1}}^x \Phi^c(x, y; \lambda) P_i^-(y; \lambda)^{-1} \tilde{H}_i^-(y) dy  \\ 
Z_i^+(x) &= \Phi^u(x, X_i; \lambda) a_i^+ + \Phi^s(x, 0; \lambda) b_i^+ + \Phi^c(x, X_i; \lambda)(c_i + \tilde{c}_i) \\
&+ \lambda^2 d_i \int_0^x \Phi^s(x, y; \lambda) P_i^+(y; \lambda)^{-1} \tilde{H}_i^+(y) dy \\
&+ \lambda^2 d_i \int_{X_i}^x \Phi^u(x, y; \lambda) P_i^+(y; \lambda)^{-1} \tilde{H}_i^+(y) dy \\
&+ \lambda^2 d_i \int_{X_i}^x \Phi^c(x, y; \lambda) P_i^+(y; \lambda)^{-1} \tilde{H}_i^+(y) dy \\
\end{aligned}
\end{equation}
where $i = 0, \dots, n-1$. 

Because of the conjugation, $Z_i^\pm$ is only on the LHS of the fixed point equations \eqref{Zfpeq}, thus these formulas for $Z_i^\pm$ solve \eqref{systemZ}.

In the next lemma, we solve equation \eqref{systemmiddle}, which are the matching conditions at $\pm X_i$.
\begin{align*}
P_i^+(X_i; \lambda) Z_i^+(X_i) - P_{i+1}^-(-X_i; \lambda) Z_{i+1}^-(-X_i) &= D_i d && i = 0, \dots, n-1
\end{align*}

% first inversion lemma : match at \pm X_i
\begin{lemma}\label{Zinv1}
There exists an operator
\begin{align*}
A_1: V_\lambda \times V_b \times V_c \times V_d \rightarrow V_a \times V_{\tilde{c}} \\
\end{align*}
such that $(a, \tilde{c}) = A_1(\lambda)(b, c, d)$ solves \eqref{systemmiddle} for any $(b, c, d)$ and $\lambda$. This operator is analytic in $\lambda$ and linear in $(b, c, d)$. Piecewise bounds for $A_1$ are given by
\begin{align}\label{A1bound}
|A_1&(\lambda)_i(b, c, d)|
\leq C \Big( e^{-\alpha X_i} (|b_i^+| + |b_{i+1}^-| + |c_i| + |\lambda^2||d|) + |D_i||d| \Big)
\end{align} 
In addition, we can write $a_i^\pm$ and $\tilde{c}$ as 
\begin{align*}
a_i^+ &= P_i^+(X_i; \lambda) P_0^u(\lambda) D_i d + A_2(\lambda)_i^+(b, c, d) \\
a_i^- &= -P_i^-(-X_i; \lambda) P_0^s(\lambda) D_i d + A_2(\lambda)_i^-(b, c, d) \\
\tilde{c}_i &= P_0^c(\lambda) D_i d + A_2(\lambda)_i^c(b, c, d) )
\end{align*}
where $A_2$ is analytic in $\lambda$, linear in $(b, c, d)$, and has piecewise bounds
\begin{align}\label{A2bound}
|A_2&(\lambda)_i(b, c, d)|
\leq C e^{-\alpha X_i} \left( |b_i^+| + |b_{i+1}^-| + |c_i| + |\lambda|^2|d| + |D_i||d| \right)
\end{align}
Finally, we have the estimate
\begin{equation}\label{P0cDid}
|P_0^c(\lambda) D_i d| \leq C e^{-\alpha_0 X_i}(|\lambda| + e^{-\alpha_0 X_m})|d|
\end{equation}

\begin{proof}
At $\pm X_i$, the fixed point equations \eqref{Zfpeq} become
\begin{align*}
Z_{i+1}^-(-X_i) &= a_i^- + \Phi^u(-X_i, 0; \lambda) b_{i+1}^- + c_i 
+ \lambda^2 d_{i+1} \int_0^{-X_i} \Phi^u(-X_i, y; \lambda) P_{i+1}^-(y; \lambda)^{-1} \tilde{H}_i^-(y) dy \\
Z_i^+(X_i) &= a_i^+ + \Phi^s(X_i, 0; \lambda) b_i^+ + c_i + \tilde{c}_i 
+ \lambda^2 d_i \int_0^{X_i} \Phi^s(X_i, y; \lambda) P_i^+(y; \lambda)^{-1} \tilde{H}_i^+(y) dy
\end{align*}
To obtain these, we used the fact that, for example, $a_i^- \in E^s(\lambda)$ and $\Phi^s(-X_{i-1}, -X_{i-1}; \lambda)$ is the identity on $E^s(\lambda)$. Applying the appropriate projections, subtracting, and using equation \eqref{projTheta}, we obtain the equation 
\begin{align*}
D_i d &= (I + \Theta_i^+(X_i; \lambda))a_i^+ + (I + \Theta_i^+(X_i; \lambda))(c_i + \tilde{c}_i) + P_i^+(X_i; \lambda)\Phi^s(X_i, 0; \lambda) b_i^+ \\
&+ \lambda^2 d_i P_i^+(X_i; \lambda) \int_0^{X_i} \Phi^s(X_i, y; \lambda) P_i^+(y; \lambda)^{-1} \tilde{H}_i^+(y) dy \\
&- (I + \Theta_{i+1}^-(-X_i; \lambda))a_i^- - (I + \Theta_{i+1}^-(-X_i; \lambda))c_i - P_{i+1}^-(-X_i; \lambda)\Phi^u(-X_i, 0; \lambda) b_{i+1}^- \\ 
&- \lambda^2 d_{i+1} P_{i+1}^-(-X_i; \lambda) \int_0^{-X_i} \Phi^u(-X_i, y; \lambda) P_{i+1}^-(y; \lambda)^{-1} \tilde{H}_i^-(y) dy \\
\end{align*}
which simplifies to
\begin{align*}
D_i d &= (I + \Theta_i^+(X_i; \lambda))a_i^+ + \Theta_i^+(X_i; \lambda)c_i + (I + \Theta_i^+(X_i; \lambda))\tilde{c}_i + P_i^+(X_i; \lambda)\Phi^s(X_i, 0; \lambda) b_i^+ \\
&+ \lambda^2 d_i P_i^+(X_i; \lambda) \int_0^{X_i} \Phi^s(X_i, y; \lambda) P_i^+(y; \lambda)^{-1} \tilde{H}_i^+(y) dy \\
&- (I + \Theta_{i+1}^-(-X_i; \lambda))a_i^- - \Theta_{i+1}^-(-X_i; \lambda)c_i - P_{i+1}^-(-X_i; \lambda)\Phi^u(-X_i, 0; \lambda) b_{i+1}^- \\ 
&- \lambda^2 d_{i+1} P_{i+1}^-(-X_i; \lambda) \int_0^{-X_i} \Phi^u(-X_i, y; \lambda) P_{i+1}^-(y; \lambda)^{-1} \tilde{H}_i^-(y) dy \\
\end{align*}
This is of the form
\begin{align}\label{Dideq1}
D_i d &= a_i^+ - a_i^- + \tilde{c}_i + L_3(\lambda)_i(a, b, c, \tilde{c}, d)
\end{align}
where
\begin{align*}
L_3(\lambda)_i(a, b, c, \tilde{c}, d) &= \Theta_i^+(X_i; \lambda)a_i^+ + \Theta_i^+(X_i; \lambda)c_i + \Theta_i^+(X_i; \lambda)\tilde{c}_i + P_i^+(X_i; \lambda)\Phi^s(X_i, 0; \lambda) b_i^+ \\
&+ \lambda^2 d_i P_i^+(X_i; \lambda) \int_0^{X_i} \Phi^s(X_i, y; \lambda) P_i^+(y; \lambda)^{-1} \tilde{H}_i^+(y) dy \\
&- \Theta_{i+1}^-(-X_i; \lambda)a_i^- - \Theta_{i+1}^-(-X_i; \lambda)c_i - P_{i+1}^-(-X_i; \lambda)\Phi^u(-X_i, 0; \lambda) b_{i+1}^- \\ &- \lambda^2 d_{i+1} P_{i+1}^-(-X_i; \lambda) \int_0^{-X_i} \Phi^u(-X_i, y; \lambda) P_{i+1}^-(y; \lambda)^{-1} \tilde{H}_i^-(y) dy \\
\end{align*}
To get a bound on $L_3$, we will bound the individual terms. 
\begin{enumerate}
\item For the terms involving $a_i^\pm$, we use \eqref{Thetadecay} to get
\[
|\Theta_i^+(X_i; \lambda)a_i^+ - \Theta_{i+1}^-(-X_i; \lambda)a_i^-| \leq C e^{-\alpha X_i}(|a_i^+| + |a_i^-|)
\]
\item For the terms involving $c_i$, and $\tilde{c}_i$, e use \eqref{ThetaTaylor} and \eqref{Thetadecay} to get
\[
|\Theta_i^+(X_i; \lambda)c_i + \Theta_i^+(X_i; \lambda)\tilde{c}_i - \Theta_{i+1}^-(-X_i; \lambda)c_i| \leq 
C e^{-\alpha X_i} (|c_i| + |\tilde{c}_i|)
\]

\item For the terms involving $b$, we use \eqref{Zevolbounds} to get
\[
| P_i^+(X_i; \lambda)\Phi^s(X_i, 0; \lambda) b_i^+ - P_i^-(-X_i; \lambda) \Phi^u(-X_i, 0; \lambda) b_{i+1}^-| \leq C e^{-\alpha X_i} (|b_i^+| + |b_{i+1}^-|)
\]

\item For the integral terms, we use \eqref{Zevolbounds} and the estimates from Lemma \ref{stabestimates} to get
\begin{align*}
&\left|
P^+(X_i; \beta_i^+, \lambda) \int_0^{X_i} \Phi^s(X_i, y; \lambda) P^+(X_i; \beta_i^+, \lambda)^{-1} \tilde{H}_i^+(y) dy \right| \\
&\leq C \int_0^{X_i} e^{-\alpha(X_i - y)}e^{-\alpha_0 y} dy \\
&\leq C e^{-\alpha X_i} \int_0^{X_i} e^{-(\alpha_0 - \alpha)y} dy \\
&= C e^{-\alpha X_i} \int_0^{X_i} e^{-\eta y} dy \\ 
&= C e^{-\alpha X_i}
\end{align*}
The other integral is similar.

\end{enumerate}
Putting these all together, we have the following bound for $L_3$.
\begin{equation}\label{L3bound}
|L_3(\lambda)_i(a, b, c, \tilde{c}, d)| \leq C e^{-\alpha X_i} \left( |a_i| + |b_i^+| + |b_{i+1}^-| + |c_i| + |\tilde{c}_i| + |\lambda^2| |d| \right)
\end{equation}
Since $e^{-\alpha X_m} < \delta$, this becomes
\begin{align*}
|L_3(\lambda)_i(a, b, c, \tilde{c}, d)| \leq C \delta ( |a_i| + |\tilde{c}| ) + C e^{-\alpha X_i} \left( |b_i^+| + |b_{i+1}^-| + |c_i| + |\lambda^2| |d| \right)
\end{align*}
Let 
\[
J_1: \bigoplus_{j=1}^n (E^s(\lambda) \times E^u(
\lambda) \times E^c(\lambda) ) \rightarrow \bigoplus_{j=1}^n \rightarrow \C^{2m+1}
\]
be defined by $(J_1)_i(a_i^+, a_i^-, \tilde{c}_i) = (a_i^+ - a_i^-, \tilde{c}_i)$. The map $J_i$ is a linear isomorphism since $E^s(\lambda) \oplus E^u(\lambda) \oplus E^c(\lambda) = \C^{2m+1}$. Consider the map
\[
S_1(a, \tilde{c}) = J_1 (a, \tilde{c}) + L_3(\lambda)(a, 0, c, 0, 0) = J_1( I + J_1^{-1} L_3(\lambda)(a, 0, c, 0, 0))
\]
For sufficiently small $\delta$, $||J_1^{-1} L_3(\lambda)(a, 0, \tilde{c}, 0, 0)|| < 1$, thus the operator $S_1(a, \tilde{c})$ is invertible. We can then solve for $(a, \tilde{c})$ to get
\[
(a, \tilde{c}) = A_1(\lambda)(b, c, d) = S_i^{-1}(-D d + L_3(\lambda)(0, b, 0, c, d)
\]
Using the bound on $L_3$ and noting which pieces are involved, $A_1$ has piecewise bounds
\begin{align*}
|A_1&(\lambda)_i(b, c, d)|
\leq C \Big( e^{-\alpha X_i} (|b_i^+| + |b_{i+1}^-| + |c_i| + |\lambda^2||d|) + |D_i||d| \Big)
\end{align*} 

Next, we apply the projections $P_0^{s/u}(\lambda)$ on the eigenspaces $E^{s/u}(\lambda)$ to \eqref{Dideq1} to obtain the expressions
\begin{align*}
a_i^+ &= P_0^u(\lambda) D_i d + A_2(\lambda)_i^+(b, c^-, d) \\
a_i^- &= -P_0^s(\lambda) D_i d + A_2(\lambda)_i^-(b, c^-, d) \\
\end{align*}
The bound on the remainder term $A_2(\lambda)_i(b, c^-, d)$ is found by substituting the bound for $A_1$ into the bound for $L_3$ and simplifying. 
\begin{align*}
|A_2&(\lambda)_i(b, c^-, d)|
\leq C e^{-\alpha X_i} \left( |b_i^+| + |b_{i+1}^-| + |c_i| + |\lambda|^2|d| + |D_i||d| \right)
\end{align*} 
Anticipating what we will need at the end, we will derive expressions for $a_i^+$ and $a_i^-$ which involve the conjugation projections. Using the conjugation operator $P_i^+(X_i; \lambda)$, we write $a_i^+$ as
\begin{align*}
a_i^+ 
&= P_i^+(X_i; \lambda)a_i^+ + (I - P_i^+(X_i; \lambda))a_i^+ \\
&= P_i^+(X_i; \lambda)a_i^+ - \Theta_i^+(X_i; \lambda))a_i^+
\end{align*}
Rearranging this and substituting the expression above for $a_i^+$, we get
\begin{align*}
P_i^+(X_i; &\lambda) a_i^+ = P_0^u(\lambda) D_i d + A_2(\lambda)_i^+(b, c^-, d) + \Theta_i^+(X_i; \lambda))a_i^+ \\
&= P_0^u(\lambda) D_i d + A_2(\lambda)_i^+(b, c^-, d) + \mathcal{O}\Big( e^{-\alpha X_i} ( e^{-\alpha X_i} (|b_i^+| + |b_{i+1}^-|) + |c_i| + e^{-\alpha X_i} |\lambda^2||d| + |D_i||d| )\Big)
\end{align*}
where we used the bound $A_1$ and the estimate \eqref{Thetadecay}. The last term on the RHS is the same (or higher) order as $A_2$, so we incorporate that into the bound on $A_2(\lambda)_i^+(b, c^-, d)$ to get
\begin{align*}
P_i^+(X_i; \lambda)a_i^+ &= P_0^u(\lambda) D_i d + A_2(\lambda)_i^+(b, c^-, d)
\end{align*}
Finally, apply the operator $P_i^+(X_i; \lambda)^{-1}$ on the left on both sides to solve for $a_i^+$. Since $P_i^+(X_i; \lambda)^{-1}$ a bounded operator, we will also incorporate this into $A_2(\lambda)_i^+(b, c^-, d)$, which will leave the bound unchanged. Thus we have
\begin{align*}
a_i^+ &= P_i^+(X_i; \lambda) P_0^u(\lambda) D_i d + A_2(\lambda)_i^+(b, c^-, d)
\end{align*}
We do the same thing for $a_i^-$, which gives us
\begin{align*}
a_i^- &= -P_i^-(-X_i; \lambda) P_0^s(\lambda) D_i d + A_2(\lambda)_i^-(b, c^-, d)
\end{align*}

Finally, apply the projection $P_0^c(\lambda)$ on the eigenspace $E^c(\lambda)$ to \eqref{Dideq1}. To do this, we will rewrite use \eqref{projTheta} to rewrite $L_3$ as
\begin{align*}
L_3(\lambda)_i(a, b, c, \tilde{c}, d) &= \Theta_i^+(X_i; \lambda)a_i^+ + \Theta_i^+(X_i; \lambda)c_i + \Theta_i^+(X_i; \lambda)\tilde{c}_i + (I + \Theta_i^+(X_i; \lambda))\Phi^s(X_i, 0; \lambda) b_i^+ \\
&+ \lambda^2 d_i (I + \Theta_i^+(X_i; \lambda)) \int_0^{X_i} \Phi^s(X_i, y; \lambda) P_i^+(y; \lambda)^{-1} \tilde{H}_i^+(y) dy \\
&- \Theta_{i+1}^-(-X_i; \lambda)a_i^- - \Theta_{i+1}^-(-X_i; \lambda)c_i - (I + \Theta_{i+1}^-(-X_i; \lambda)) \Phi^u(-X_i, 0; \lambda) b_{i+1}^- \\ &- \lambda^2 d_{i+1} (I + \Theta_{i+1}^-(-X_i; \lambda)) \int_0^{-X_i} \Phi^u(-X_i, y; \lambda) P_{i+1}^-(y; \lambda)^{-1} \tilde{H}_i^-(y) dy \\
\end{align*}
Since the ranges of the evolution operators $\Phi^s(x, y; \lambda)$ and $\Phi^u(x, y; \lambda)$ are $E^s(\lambda)$ and $E^u(\lambda)$ (respectively), applying the projection $P_0^c(\lambda)$ eliminates any terms involving these. Thus we have the estimate....

We would like to obtain an estimate for $P_0^c(\lambda) D_i d$. Recall from Lemma \ref{stabestimates} that
\begin{equation}\label{Did3}
D_i d = ( Q'(X_i) + Q'(-X_i))(d_{i+1} - d_i ) + \mathcal{O} \left( e^{-\alpha_0 X_i} \left( |\lambda| +  e^{-\alpha_0 X_m}  \right) |d| \right) 
\end{equation}
Using the fact that the eigenprojection $P_0^c(\lambda)$ is a smooth function of $\lambda$, we have for $P_0^c(Q'(X_i) + Q'(-X_i))$,
\begin{align*}
P_0^c(\lambda)(Q'(X_i) + Q'(-X_i))
&= P_0^c(0)(Q'(X_i) + Q'(-X_i)) + \mathcal{O}(|\lambda|e^{-\alpha_0 X_i}) \\
&= \mathcal{O}(e^{-\alpha_0 X_i}(|\lambda| + e^{-\alpha_0 X_i})) \\
\end{align*}
since $Q'(-X_i)$ decays exponentially towards $E^u$ and $Q'(X_i)$ decays exponentially towards $E^s$. Combining this with the bound on the higher order terms of \eqref{Did3}, we obtain the bound
\[
|P_0^c(\lambda) D_i d| \leq C e^{-\alpha_0 X_i}(|\lambda| + e^{-\alpha_0 X_m})|d|
\]
\end{proof}
\end{lemma}

In the next lemma, we solve for the conditions at $x = 0$.
\begin{equation}\label{centercond}
\begin{aligned}
P_i^\pm(0; \lambda) Z_i^\pm(0) &\in \oplus Y^+ \oplus Y^- \oplus \C \Psi(0) \oplus \C \Psi^c(0) \\
P_i^+(0; \lambda) Z_i^+(0) - P_i^-(0; \lambda) Z_i^-(0) &\in \C \Psi(0) \oplus \C \Psi^c(0)
\end{aligned}
\end{equation}
Recall that we have the decomposition
\begin{equation}\label{DSdecomp}
\C^{2m+1} = \C Q'(0) \oplus Y^+ \oplus Y^- \oplus \C \Psi(0) \oplus \C \Psi^c(0)
\end{equation}
Thus \eqref{centercond} is equivalent to the three projections
\begin{equation}\label{centercond2}
\begin{aligned}
P(\C Q'(0) ) P_i^-(0; \lambda) Z_i^-(0) &= 0 \\
P(\C Q'(0) ) P_i^+(0; \lambda) Z_i^+(0) &= 0 \\
P(Y_i^+ \oplus Y_i^-) ( P_i^+(0; \lambda) Z_i^+(0) - P_i^-(0; \lambda) Z_i^-(0) ) &= 0
\end{aligned}
\end{equation}
where the kernel of each projection is the remaining spaces in the direct sum decomposition \eqref{DSdecomp}. We do not need to include $\C Q'(0)$ in the third equation of \eqref{centercond2} since we eliminated any component in $\C Q'(0)$ in the first two equations.

% second inversion lemma
\begin{lemma}\label{Zinv2}
There exist operators
\begin{align*}
B_1: &V_\lambda \times V_c \times V_d \rightarrow V_b \\
A_3: &V_\lambda \times V_c \times V_d \rightarrow V_a \times V_{\tilde{c}} 
\end{align*}
such that $( (a, \tilde{c}) , b ) = ( A_3(\lambda)(c, d), B_1(\lambda)(c, d) )$ solves \eqref{systemmiddle}, \eqref{systemcenter1}, and \eqref{systemcenter2} for any $(c, d)$. These operators are analytic in $\lambda$ and linear in $(c,d)$. Piecewise bounds for $B_1$ and $A_3$ are given by
\begin{align}
|B_1(\lambda)_i(c, d)| &\leq C \Big( (|\lambda| + e^{-\alpha X_m})(|e^{\nu(\lambda)X_{i-1}} c_{i-1}| + |e^{-\nu(\lambda)X_i} c_i|) + (|\lambda| + e^{-\alpha X_m})^2 |d| \Big) \label{B1bound} \\
|A_3(\lambda)_i(c, d)|
&\leq C \Big(  
e^{-\alpha X_i} (|\lambda| + e^{-\alpha X_m})(|e^{\nu(\lambda)X_{i-1}} c_{i-1}| + |e^{-\nu(\lambda)X_{i+1}}c_{i+1}|) \nonumber \\
&+ e^{-(\alpha - \eta)X_i}|c_i^-| + e^{-\alpha X_i} |\lambda^2||d| + |D_i||d| \Big) \label{A3bound}
\end{align} 
In addition, we can write
\begin{align*}
a_i^+ &= P_i^+(X_i; \lambda) P_0^u(\lambda) D_i d + A_4(\lambda)_i^+(c, d) \\
a_i^- &= -P_i^-(-X_i; \lambda) P_0^s(\lambda) D_i d + A_4(\lambda)_i^-(c, d) \\
\tilde{c}_i &= P_0^c(\lambda) D_i d + A_4(\lambda)_i^c(c, d) )
\end{align*}

where $A_4(\lambda)(c, d)$ is analytic in $\lambda$, linear in $(c^-, d)$, and has piecewise bounds
\begin{align}
|A_4&(\lambda)_i(c, d)|
\leq C \Big(  
e^{-\alpha X_i} (|\lambda| + e^{-\alpha X_m})(|e^{\nu(\lambda)X_{i-1}} c_{i-1}| + |e^{-\nu(\lambda)X_{i+1}}c_{i+1}|) \nonumber \\
&+ e^{-(\alpha - \eta)X_i}|c_i^-| + e^{-\alpha X_i} |\lambda^2||d| + e^{-\alpha X_i}|D||d| \Big)\label{A4bound}
\end{align}

\begin{proof}
Recall that at $Q(0)$, the tangent spaces to the stable and unstable manifold are given by
\begin{align*}
T_{Q(0)} W^u(0) &= \R Q'(0) \oplus Y^- \\
T_{Q(0)} W^s(0) &= \R Q'(0) \oplus Y^+
\end{align*}
Thus we have
\begin{align*}
P^-(0)^{-1} Q'(0) &= V^- \in E^u(0) \\
P^+(0)^{-1} Q'(0) &= V^+ \in E^s(0)
\end{align*}
Let
\begin{align*}
E^u(0) &= \C V^- \oplus E^- \\
E^s(0) &= \C V^+ \oplus E^+ \\
\end{align*}
Then we have
\begin{align*}
P^-(0)^{-1} Y^- = E^- \\
P^+(0)^{-1} Y^+ = E^+ \\
\end{align*}
We decompose $b_i^\pm$ uniquely as $b_i^\pm = x_i^\pm + y_i^\pm$, where $x_i^\pm \in \C V^\pm$ and $y_i^\pm \in E^\pm$.

At $x = 0$, the fixed point equations \eqref{Zfpeq} become
\begin{align*}
Z_i^-(0) &= \Phi^s(0, -X_{i-1}; \lambda) a_{i-1}^- + \Phi^u(0, 0; \lambda) b_i^- + \Phi^c(0, -X_{i-1}; \lambda) c_{i-1} \\
&+ \lambda^2 d_i \int_{-X_{i-1}}^0 \Phi^s(0, y; \lambda) P_i^-(y; \lambda)^{-1} \tilde{H}_i^-(y) dy \\
&+ \lambda^2 d_i \int_{-X_{i-1}}^0 \Phi^c(0, y; \lambda) P_i^-(y; \lambda)^{-1} \tilde{H}_i^-(y) dy  \\ 
Z_i^+(0) &= \Phi^u(0, X_i; \lambda) a_i^+ + \Phi^s(0, 0; \lambda) b_i^+ + \Phi^c(0, X_i; \lambda) c_i + \Phi^c(0, X_i; \lambda) \tilde{c}_i\\
&+ \lambda^2 d_i \int_{X_i}^0 \Phi^u(0, y; \lambda) P_i^+(y; \lambda)^{-1} \tilde{H}_i^+(y) dy \\
&+ \lambda^2 d_i \int_{X_i}^0 \Phi^c(0, y; \lambda) P_i^+(y; \lambda)^{-1} \tilde{H}_i^+(y) dy \\
\end{align*}
Noting that $\Phi^u(0, 0; \lambda) = P_0^u(\lambda)$, adding and subtracting $P_0^u(0)$ and $P_0^s(0)$, and using equation \eqref{centerevol} for the evolution $\Phi^c$ on $E^c(\lambda)$, these become
\begin{align*}
Z_i^-(0) &= \Phi^s(0, -X_{i-1}; \lambda) a_{i-1}^- + x_i^- + y_i^- + (P_0^u(\lambda) - P_0^u(0))b_i^- + e^{\nu(\lambda) X_{i-1}} c_{i-1} \\
&+ \lambda^2 d_i \int_{-X_{i-1}}^0 \Phi^s(0, y; \lambda) P_i^-(y; \lambda)^{-1} \tilde{H}_i^-(y) dy \\
&+ \lambda^2 d_i \int_{-X_{i-1}}^0 \Phi^c(0, y; \lambda) P_i^-(y; \lambda)^{-1} \tilde{H}_i^-(y) dy  \\ 
Z_i^+(0) &= \Phi^u(0, X_i; \lambda) a_i^+ + x_i^+ + y_i^+ + (P_0^s(\lambda) - P_0^s(0)) b_i^+ + e^{-\nu(\lambda)X_i} c_i +  e^{-\nu(\lambda)X_i} \tilde{c}_i \\
&+ \lambda^2 d_i \int_{X_i}^0 \Phi^u(0, y; \lambda) P_i^+(y; \lambda)^{-1} \tilde{H}_i^+(y) dy \\
&+ \lambda^2 d_i \int_{X_i}^0 \Phi^c(0, y; \lambda) P_i^+(y; \lambda)^{-1} \tilde{H}_i^+(y) dy 
\end{align*}
Since $c_i$, $c_{i-1}$, and $\tilde{c}_i$ are in the eigenspaces $E^c(\lambda)$, we do some further manipulation to separate out a component in $E^c(0)$.
\begin{align*}
Z_i^-(0) &= \Phi^s(0, -X_{i-1}; \lambda) a_{i-1}^- + x_i^- + y_i^- + (P_0^u(\lambda) - P_0^u(0))b_i^- \\
&+ P_0^c(0) e^{\nu(\lambda) X_{i-1}} c_{i-1} + (P_0^c(\lambda) - P_0^c(0)) e^{\nu(\lambda) X_{i-1}} c_{i-1} \\
&+ \lambda^2 d_i \int_{-X_{i-1}}^0 \Phi^s(0, y; \lambda) P_i^-(y; \lambda)^{-1} \tilde{H}_i^-(y) dy \\
&+ \lambda^2 d_i \int_{-X_{i-1}}^0 \Phi^c(0, y; \lambda) P_i^-(y; \lambda)^{-1} \tilde{H}_i^-(y) dy  \\ 
Z_i^+(0) &= \Phi^u(0, X_i; \lambda) a_i^+ + x_i^+ + y_i^+ + (P_0^s(\lambda) - P_0^s(0)) b_i^+ \\
&+ P_0^c(0) e^{-\nu(\lambda)X_i} c_i + (P_0^c(\lambda) - P_0^c(0)) e^{-\nu(\lambda)X_i} c_i \\
&+ P_0^c(0) e^{-\nu(\lambda)X_i} \tilde{c}_i + (P_0^c(\lambda) - P_0^c(0)) e^{-\nu(\lambda)X_i} \tilde{c}_i \\
&+ \lambda^2 d_i \int_{X_i}^0 \Phi^u(0, y; \lambda) P_i^+(y; \lambda)^{-1} \tilde{H}_i^+(y) dy \\
&+ \lambda^2 d_i \int_{X_i}^0 \Phi^c(0, y; \lambda) P_i^+(y; \lambda)^{-1} \tilde{H}_i^+(y) dy 
\end{align*}
Finally, we apply the conjugation projections $P_i^\pm(0; \lambda)$. For the $c$ and $b$ terms, we write these as projections as
\[
P_i^\pm(0; \lambda) = P^\pm(0) + (P_i^\pm(0; \lambda) - P^\pm(0))
\]
After all of this, we wind up with the equations
\begin{align*}
P_i^-(0; \lambda) Z_i^-(0) &= P^-(0)( x_i^- + y_i^- + P_0^c(0) e^{\nu(\lambda) X_{i-1}} c_{i-1} ) \\
&+ P_i^-(0; \lambda) \Phi^s(0, -X_{i-1}; \lambda) a_{i-1}^- + (P_i^-(0; \lambda) - P^-(0))b_i^- + P_i^-(0; \lambda)(P_0^u(\lambda) - P_0^u(0))b_i^- \\
&+ (P_i^-(0; \lambda) - P^-(0)) P_0^c(0) e^{\nu(\lambda) X_{i-1}} c_{i-1} + P_i^-(0; \lambda) (P_0^c(\lambda) - P_0^c(0)) e^{\nu(\lambda) X_{i-1}} c_{i-1} \\
&+ \lambda^2 d_i P_i^-(0; \lambda) \int_{-X_{i-1}}^0 \Phi^s(0, y; \lambda) P_i^-(y; \lambda)^{-1} \tilde{H}_i^-(y) dy \\
&+ \lambda^2 d_i P_i^-(0; \lambda) \int_{-X_{i-1}}^0 \Phi^c(0, y; \lambda) P_i^-(y; \lambda)^{-1} \tilde{H}_i^-(y) dy  \\ 
P_i^+(0; \lambda) Z_i^+(0) &=  P^+(0)( x_i^+ + y_i^+ + P_0^c(0) e^{-\nu(\lambda)X_i} c_i + + P_0^c(0) e^{-\nu(\lambda)X_i} \tilde{c}_i )\\
&+ P_i^+(0; \lambda) \Phi^u(0, X_i; \lambda) a_i^+ + (P_i^+(0; \lambda) - P^+(0)) b_i^+ + P_i^+(0; \lambda) (P_0^s(\lambda) - P_0^s(0)) b_i^+ \\
&+ (P_i^+(0; \lambda) - P^+(0))P_0^c(0) e^{-\nu(\lambda)X_i} c_i + P_i^+(0; \lambda) (P_0^c(\lambda) - P_0^c(0)) e^{-\nu(\lambda)X_i} c_i \\
&+ (P_i^+(0; \lambda) - P^+(0))P_0^c(0) e^{-\nu(\lambda)X_i} \tilde{c}_i + P_i^+(0; \lambda) (P_0^c(\lambda) - P_0^c(0)) e^{-\nu(\lambda)X_i} \tilde{c}_i \\
&+ \lambda^2 d_i P_i^+(0; \lambda) \int_{X_i}^0 \Phi^u(0, y; \lambda) P_i^+(y; \lambda)^{-1} \tilde{H}_i^+(y) dy \\
&+ \lambda^2 d_i P_i^+(0; \lambda) \int_{X_i}^0 \Phi^c(0, y; \lambda) P_i^+(y; \lambda)^{-1} \tilde{H}_i^+(y) dy \\
\end{align*}

With this setup, the projections on $Q'(0)$ and $Y^+ \oplus Y^-$ will either eliminate or act as the identity on the terms in the first lines of $P_i^-(0; \lambda) Z_i^-(0)$ and $P_i^+(0; \lambda) Z_i^+(0)$. Thus, applying the projections in \eqref{centercond2}, we obtain an expression of the form
\begin{equation}\label{projxy}
\begin{pmatrix}x_i^- \\ x_i^+ \\ 
y_i^+ - y_i^- \end{pmatrix} + L_4(\lambda)_i(b, c^-, d) = 0
\end{equation}
To get a bound on $L_4$, we will bound the individual terms involved. 

\begin{enumerate}
\item For the $a_i$ terms, we substitute $A_1$ from Lemma \ref{Zinv1} and use the bound \eqref{A1bound} to get
\begin{align*}
|P_i^-(0; \lambda) \Phi^s(0, -X_{i-1}; \lambda) a_{i-1}^-| 
&\leq C \left( e^{-2 \alpha X_{i-1}} (|b_{i-1}^+| + |b_i^-| + |c_{i-1}| + |\lambda^2||d|) + e^{- \alpha X_{i-1}} |D_{i-1}||d| \right)\\
|P_i^+(0; \lambda) \Phi^u(0, X_i; \lambda) a_i^+| 
&\leq C \left( e^{-2 \alpha X_i} (|b_i^+| + |b_{i+1}^-| + |c_i| + |\lambda|^2|d|) + e^{-\alpha X_i}|D_i||d| \right)
\end{align*}

\item For the $b_i$ terms, we use \eqref{PTaylor} to get
\begin{align*}
|(P_i^-(0; \lambda) &- P^-(0))b_i^- + P_i^-(0; \lambda)(P_0^u(\lambda) - P_0^u(0))b_i^-| \\
&\leq C ( e^{-\alpha X_m} + |\lambda|)|b_i^-|
\end{align*}
The other term is similar.

\item For the $c_i$ terms, we use \eqref{PTaylor} to get
\begin{align*}
|(P_i^-(0; \lambda) &- P^-(0)) P_0^c(0) e^{\nu(\lambda) X_{i-1}} c_{i-1} + P_i^-(0; \lambda) (P_0^c(\lambda) - P_0^c(0)) e^{\nu(\lambda) X_{i-1}} c_{i-1} | \\
&\leq C (e^{-\alpha X_m} + |\lambda|)|e^{\nu(\lambda) X_{i-1}} c_{i-1}|
\end{align*}
and
\begin{align*}
|(P_i^+(0; \lambda) &- P^+(0))P_0^c(0) e^{-\nu(\lambda)X_i} c_i + P_i^+(0; \lambda) (P_0^c(\lambda) - P_0^c(0)) e^{-\nu(\lambda)X_i} c_i| \\
&\leq C (e^{-\alpha X_m} + |\lambda|)|e^{-\nu(\lambda)X_i} c_i|
\end{align*}

\item For the $\tilde{c}_i$ terms, we use \eqref{PTaylor}, the expression $\tilde{c}_i = P_0^c(\lambda) D_i d + A_2(\lambda)_i^c(b, c^-, d) )$ from Lemma \ref{Zinv1}, and the bounds on $P_0^c(\lambda) D_i d$ and $A_2$ from that lemma to get
\begin{align*}
|(P_i^+(0; \lambda) &- P^+(0))P_0^c(0) e^{-\nu(\lambda)X_i} \tilde{c}_i + P_i^+(0; \lambda) (P_0^c(\lambda) - P_0^c(0)) e^{-\nu(\lambda)X_i} \tilde{c}_i| \\
&\leq C (e^{-\alpha X_m} + |\lambda|)|P_0^c(\lambda) D_i d + A_2(\lambda)_i^c(b, c^-, d) | \\
&\leq C (e^{-\alpha X_m} + |\lambda|) e^{-|\nu(\lambda)|X_i} \Big( e^{-\alpha_0 X_i}(|\lambda| + e^{-\alpha_0 X_m})|d| \\
&+ e^{-\alpha X_i}(|b_i^+| + |b_{i+1}^-| + |c_i| + |\lambda|^2|d| + |D_i||d| ) \Big) \\
&\leq C e^{-(\alpha - \eta)X_i} (e^{-\alpha X_m} + |\lambda|) (|b_i^+| + |b_{i+1}^-| + |c_i| + |\lambda|^2|d| + |D_i||d| )
\end{align*}
where we used the fact that $D_i = \mathcal{O}(e^{-\alpha_0 X_i}$ is lower order than $e^{-\alpha X_i}(|\lambda| + e^{-\alpha_0 X_m})$.

\item The bound on the integral terms is determined by the integrals involving the center subspace, since there is potential growth in that subspace.
\begin{align*}
\left| \lambda^2 d_i P_i^-(0; \lambda) \int_{-X_{i-1}}^0 \Phi^c(0, y; \lambda) P_i^-(y; \lambda)^{-1} \tilde{H}_i^-(y) dy \right| &\leq C |\lambda|^2 |d| \int_{-X_{i-1}}^0 e^{-\eta y} e^{\alpha_0 y} dy \\
&\leq C |\lambda|^2 |d|
\end{align*}
The other term is similar.
\end{enumerate}

Putting all of these individual bounds together, using the fact that $D_i = \mathcal{O}(e^{-\alpha X_m})$, and simplifying, we obtain the bound for $L_4(\lambda)_i(b, c, d)$.
\begin{align*}
L_4(\lambda)_i(b, c, d) &\leq 
C\Big( (|\lambda| + e^{-\alpha X_m})|b|  
+ (|\lambda| + e^{-\alpha X_m})(|e^{\nu(\lambda)X_{i-1}} c_{i-1}| + |e^{-\nu(\lambda)X_i} c_i|) + (|\lambda| + e^{-\alpha X_m})^2 |d|  \Big) 
\end{align*}

Since $|\lambda|, e^{-\alpha X_m} < \delta$, we have
\begin{align*}
L_4(\lambda)_i(b, c, d) &\leq C \delta |b| 
+ C \Big( (|\lambda| + e^{-\alpha X_m})(|e^{\nu(\lambda)X_{i-1}} c_{i-1}| + |e^{-\nu(\lambda)X_i} c_i|) + (|\lambda| + e^{-\alpha X_m})^2 |d| \Big) 
\end{align*}
which is uniform in $|b|$. Define the map
\[
J_2: \left( \bigoplus_{j=1}^n \C V^+ \oplus \C V^- \right) \oplus
\left( \bigoplus_{j=1}^n E^+ \oplus E^- \right) 
\rightarrow \bigoplus_{j=1}^n \C V^+ \oplus \C V^- \oplus (E^+ \oplus E^-)
\]
by 
\[
J_2( (x_i^+, x_i^-),(y_i^+, y_i^-))_i = ( x_i^+, x_i^-, y_i^+ - y_i^- )
\]
Since $\C^{2m} = E^s(0) \oplus E^u(0) = \C V^+ \oplus \C V^- \oplus (E^+ \oplus E^-)$, $J_2$ is an isomorphism. Using this as the fact that $b_i = (x_i^- + y_i^-, x_i^+ + y_i^+)$, we can write \eqref{projxy} as
\begin{equation}\label{projxy2}
J_2( (x_i^+, x_i^-),(y_i^+, y_i^-))_i 
+ L_4(\lambda)_i(b_i, 0, 0) + L_4(\lambda)_i(0, c, d) = 0
\end{equation}
Consider the map
\begin{align*}
S_2(b)_i &= J_2( (x_i^+, x_i^-),(y_i^+, y_i^-))_i 
+ L_4(\lambda)_i(b_i, 0, 0) 
\end{align*}
Substituting this in \eqref{projxy2}, we have
\begin{align*}
S_2(b) &= -L_4(\lambda)(0, c, d)
\end{align*}

For sufficiently small $\delta$, the operator $S_2(b)$ is invertible. Thus we can solve for $b$ to get
\begin{align}
b = B_1(\lambda)(c,d) 
= -S_2^{-1} L_4(\lambda)(0, c, d)
\end{align}
The bound on $B_1$ is given by the bound on $L_4$, where we note which piece is involved.
\begin{align*}
|B_1(\lambda)_i(c, d)| &\leq C \Big( (|\lambda| + e^{-\alpha X_m})(|e^{\nu(\lambda)X_{i-1}} c_{i-1}| + |e^{-\nu(\lambda)X_i} c_i|) + (|\lambda| + e^{-\alpha X_m})^2 |d| \Big)
\end{align*}

We can plug $B_1$ into the bound \eqref{A1bound} for $A_1$ to get $A_3$ with bound
\begin{align*}
|A_3&(\lambda)_i(c, d)|
\leq C \Big(  
e^{-\alpha X_i} (|\lambda| + e^{-\alpha X_m})(|e^{\nu(\lambda)X_{i-1}} c_{i-1}| + |e^{-\nu(\lambda)X_{i+1}}c_{i+1}|) \\
&+ e^{-(\alpha - \eta)X_i}|c_i^-| + e^{-\alpha X_i} |\lambda^2||d| + |D_i||d| \Big)
\end{align*} 
We can also plug $B_1$ into the bound \eqref{A2bound} for $A_2$ to get $A_4$ with bound
\begin{align*}
|A_4&(\lambda)_i(c, d)|
\leq C \Big(  
e^{-\alpha X_i} (|\lambda| + e^{-\alpha X_m})(|e^{\nu(\lambda)X_{i-1}} c_{i-1}| + |e^{-\nu(\lambda)X_{i+1}}c_{i+1}|) \\
&+ e^{-(\alpha - \eta)X_i}|c_i| + e^{-\alpha X_i} |\lambda^2||d| + e^{-\alpha X_i}|D||d| \Big)
\end{align*} 

\end{proof}
\end{lemma}

\subsection{Jump Conditions}
Up to this point, we have solved uniquely for $a$, $b$, and $c^+$. We will not be able to obtain a unique solution for $c_i^-$ and $d$. We will instead compute the jump conditions in the direction of $\Psi(0)$ and $\Psi^c(0)$. Obtaining a nontrivial solution for $c_i^-$ and $d$ will be equivalent to these jump conditions being 0.

First, we compute the jump in the direction of $\Psi^c(0)$. Before we do that, we prove the following lemma regarding inner products with $\Psi^c(0)$ and $\Psi(0)$.

% lemma : inner products with Psi and Psi^c
\begin{lemma}\label{PsiIP}
We have the following expressions involving the inner product with $\Psi^c(0)$.
\begin{enumerate}[(i)]
	\item $\langle \Phi^c(0), P^\pm(0) V \rangle = V$ for all $V \in E^c(0)$.
	\item $\langle \Phi(0), P^\pm(0) V \rangle = 0$ for all $V \in E^c(0)$.
	\item $\langle \Phi^c(0), P^-(0) V \rangle = 0$ and $\langle \Phi(0), P^-(0) V \rangle = 0$ for all $V \in E^u(0)$.
	\item $\langle \Phi^c(0), P^+(0) V \rangle = 0$ and $\langle \Phi(0), P^+(0) V \rangle = 0$ for all $V \in E^s(0)$.
\end{enumerate}
\begin{proof}
For (i), recall that $E^c(0) = \text{span }\{ V_0 \}$, where $V_0$ is the eigenvector of $A(0)$ corresponding to the eigenvalue 0. Furthermore, the constant function $Z(x) = V_0$ solves $Z' = A(0) Z$ with initial condition $V_0$. Let $W^-(x) = P^-(x) Z(x) = P^-(x) V_0$. By the conjugation lemma, we can write
\[
W^-(x) = V_0 + \mathcal{O}({e^{-\tilde{\alpha}|x|}})
\]
Since the inner product $\langle \Phi^c(x), W^-(x) \rangle$ is constant in $x$, sending $x \rightarrow -\infty$, we conclude by the continuity of the inner product that
\[
\langle \Phi^c(0), W^-(0) \rangle = \langle W_0, V_0 \rangle = 1 
\]
Thus $\langle \Phi^c(0), P^-(0) V \rangle = V$ for all $V \in E^c(0)$. The same holds for $\langle \Phi^c(0), P^+(0) V \rangle$.

For (ii), we use the same argument as in (i), except we look at the inner product $\langle \Phi(x), W^-(x) \rangle$. Since this is constant in $x$, we send $x \rightarrow \infty$. This time, $W^-(x)$ remains bounded, but $\Phi(x)$ decays to 0, thus by the continuity of the inner product, we conclude that $\langle \Phi(0), W^-(0) \rangle = 0$, from which (ii) follows.

For (iii) and (iv), we note that $P^-(0)E^u = \C Q'(0) \oplus Y^-$ and $P^+(0)E^u = \C Q'(0) \oplus Y^+$. The result follows since $\Psi^c(0), \Psi(0) \perp \C Q'(0) \oplus Y^+ \oplus Y^-$.
\end{proof}
\end{lemma}

We also have the following lemma.

\begin{lemma}\label{Ecconj}
For $c \in E^c(\lambda)$, 
\begin{align*}
P_i^-(0; \lambda)c &= c V^-(0; \lambda) + \mathcal{O}(e^{-2\alpha X_m}) \\
P_i^+(0; \lambda)c &= c V^+(0; \lambda) + \mathcal{O}(e^{-2\alpha X_m})
\end{align*}
\begin{proof}
From \eqref{PTaylorlambda} and the conjugation, we have
\begin{align*}
P_i^-(0; \lambda)c &= P^-(0; \lambda)c + \mathcal{O}(e^{-2\alpha X_m}) \\
&= c V^-(0; \lambda) + \mathcal{O}(e^{-2\alpha X_m})
\end{align*}
\end{proof}
\end{lemma}

We can now compute the jump in the direction of $\Psi^c(0)$.

% jump lemma : center adjoint
\begin{lemma}\label{jumpcenteradj}
The jumps in the direction of $\Psi^c(0)$ are given by
\begin{equation}\label{xic}
\begin{aligned}
\xi^c_i = \langle \Psi^c(X_i), &Q'(-X_i) \rangle (d_{i+1} - d_i ) + \langle \Psi^c(-X_{i-1}), Q'(X_{i-1}) \rangle (d_i - d_{i-1} ) \\
&+ e^{-\nu(\lambda) X_i} c_i^- - e^{\nu(\lambda) X_{i-1}} c_{i-1}^- +  R^c(\lambda)_i(c, d)
\end{aligned}
\end{equation}
The remainder term $R^c_i(c, d)$ is analytic in $\lambda$, linear in $(c, d)$, and has bound
\begin{align*}
||R^c&(c, d)_i|| \leq C \Big(
(|\lambda| + e^{-\alpha X_m})(|e^{\nu(\lambda)X_{i-1}}c_{i-1}| + |e^{-\nu(\lambda)X_i}c_i|) \\
&+ (|\lambda| + e^{-\alpha X_m})( e^{-(\alpha - \eta) X_{i-1}} |e^{\nu(\lambda)X_{i-2}}c_{i-2}| + e^{-(\alpha - \eta) X_i} |e^{-\nu(\lambda)X_{i+1}}c_{i+1}|)  \\
&+ (|\lambda| + e^{-\alpha X_m})^2 |d|
\Big)
\end{align*}
The jump conditions can be written as the matrix equation
\begin{equation}\label{matrixjumpc}
(K(\lambda) + C_1 K(\lambda) + K_1(\lambda)) c + (\tilde{A} + D_1) d = 0
\end{equation}
where
\begin{align*}
K(\lambda) =  
\begin{pmatrix}
e^{-\nu(\lambda)X_1} & & & & & -e^{\nu(\lambda)X_0} \\
-e^{\nu(\lambda)X_1} & e^{-\nu(\lambda)X_2} \\
& -e^{\nu(\lambda)X_2} & e^{-\nu(\lambda)X_3} \\
\vdots & & \vdots & &&  \vdots \\
& & & & -e^{\nu(\lambda)X_{n-1}} & e^{-\nu(\lambda)X_0} 
\end{pmatrix}
\end{align*}
and
\begin{align*}
\tilde{A} &= \begin{pmatrix}
-\tilde{a}_0 - \tilde{a}_1 & \tilde{a}_0 + \tilde{a}_1 \\
\tilde{a}_0 + \tilde{a}_1 & -\tilde{a}_0 - \tilde{a}_1
\end{pmatrix} && n = 2 \\
\tilde{A} &= \begin{pmatrix}
-\tilde{a}_{n-1} - \tilde{a}_0 & \tilde{a}_0 & & & \dots & \tilde{a}_{n-1}\\
\tilde{a}_0 & -\tilde{a}_0 - \tilde{a}_1 & \tilde{a}_1 \\
& \tilde{a}_1 & -\tilde{a}_1 - \tilde{a}_2 & \tilde{a}_2 \\
& & \vdots & & \vdots \\
\tilde{a}_{n-1} & & & & \tilde{a}_{n-2} & -\tilde{a}_{n-2} - \tilde{a}_{n-1} \\
\end{pmatrix} && n > 2
\end{align*}
where
\[
\tilde{a}_i = \langle \Psi^c(X_i), Q^R(X_i) \rangle 
\]
The matrices have uniform bounds
\begin{align*}
\tilde{A} &= \mathcal{O}(e^{-\alpha X_m}) \\
C_1 &= \mathcal{O}(e^{-(\alpha - \eta) X_m}(|\lambda| + e^{-\alpha X_m})) \\
D_1 &= \mathcal{O}((|\lambda| + e^{-\alpha X_m})^2)
\end{align*}
$K_1(\lambda)$ has the same form as $K(\lambda)$, but each term in $K(\lambda)$ has been multiplied by a factor of order $\mathcal{O}(|\lambda| + e^{-\alpha X_m})$. The specific forms of $K_1(\lambda)$ and $C_1$ are given in the proof.

\begin{proof}
From Lemma \ref{Zinv2}, $P_i^\pm(0; \lambda) Z_i^\pm(0)$ are given by
\begin{align*}
P_i^-(0; \lambda) Z_i^-(0) &= P^-(0) b_i^- + P_i^-(0; \lambda) e^{\nu(\lambda) X_{i-1}} c_{i-1} \\
&+ P_i^-(0; \lambda) \Phi^s(0, -X_{i-1}; \lambda) a_{i-1}^- + (P_i^-(0; \lambda) - P^-(0))b_i^- + P_i^-(0; \lambda)(P_0^u(\lambda) - P_0^u(0))b_i^- \\
&+ \lambda^2 d_i P_i^-(0; \lambda) \int_{-X_{i-1}}^0 \Phi^s(0, y; \lambda) P_i^-(y; \lambda)^{-1} \tilde{H}_i^-(y) dy \\
&+ \lambda^2 d_i P_i^-(0; \lambda) \int_{-X_{i-1}}^0 \Phi^c(0, y; \lambda) P_i^-(y; \lambda)^{-1} \tilde{H}_i^-(y) dy  \\ 
P_i^+(0; \lambda) Z_i^+(0) &=  P^+(0) b_i^+ + P_i^+(0; \lambda) e^{-\nu(\lambda)X_i} c_i \\
&+ P_i^+(0; \lambda) \Phi^u(0, X_i; \lambda) a_i^+ + (P_i^+(0; \lambda) - P^+(0)) b_i^+ + P_i^+(0; \lambda) (P_0^s(\lambda) - P_0^s(0)) b_i^+ \\
&+ P_i^+(0; \lambda) e^{-\nu(\lambda)X_i} \tilde{c}_i \\
&+ \lambda^2 d_i P_i^+(0; \lambda) \int_{X_i}^0 \Phi^u(0, y; \lambda) P_i^+(y; \lambda)^{-1} \tilde{H}_i^+(y) dy \\
&+ \lambda^2 d_i P_i^+(0; \lambda) \int_{X_i}^0 \Phi^c(0, y; \lambda) P_i^+(y; \lambda)^{-1} \tilde{H}_i^+(y) dy
\end{align*}

The terms involving $c$ and $a$ are the leading order terms in the jump condition. 

\begin{enumerate}
\item For the terms involving $c$, using Lemmas \ref{Vpmlambdalemma} and \ref{Ecconj}, we have
\begin{align*}
\langle \Psi^c(0), P_i^-(0; \lambda) e^{\nu(\lambda) X_{i-1}} c_{i-1} \rangle &= \langle \Psi^c(0), V^+(0; \lambda) e^{\nu(\lambda) X_{i-1}} c_{i-1} \rangle + \mathcal{O}(e^{-2 \alpha X_m}|e^{\nu(\lambda) X_{i-1}} c_{i-1}|) \\
&= e^{\nu(\lambda) X_{i-1}} c_{i-1} + \mathcal{O}((|\lambda| + e^{-2 \alpha X_m})|e^{\nu(\lambda) X_{i-1}} c_{i-1}|)
\end{align*}
Similarly,
\begin{align*}
\langle \Psi^c(0), P_i^+(0; \lambda) e^{-\nu(\lambda) X_i} c_i \rangle &= e^{-\nu(\lambda) X_i} c_i + \mathcal{O}((|\lambda| + e^{-2 \alpha X_m})|e^{-\nu(\lambda) X_i} c_i|)
\end{align*}

\item For the terms involving $a$, we use the expressions from Lemma \ref{Zinv2}.
\begin{align*}
\langle &\Psi^c(0), P_i^-(0; \lambda) \Phi^s(0, -X_{i-1}; \lambda) a_{i-1}^- \rangle \\
&= -\langle \Psi^c(0), P_i^-(0; \lambda) \Phi^s(0, -X_{i-1}; \lambda) P_i^-(-X_{i-1}; \lambda)^{-1} P_0^s(\lambda) D_{i-1} d \rangle \\
&+ \langle \Psi^c(0), P_i^-(0; \lambda) \Phi^s(0, -X_{i-1}; \lambda) A_4(\lambda)_{i-1}^-(c, d) \rangle \\
&= -\langle \Psi^c(0), \Theta_i^s(0, -X_{i-1}; \lambda) P_0^s(0) D_{i-1} d \rangle + \langle \Psi^c(0), P_i^-(0; \lambda) \Phi^s(0, -X_{i-1}; \lambda) A_4(\lambda)_{i-1}^-(c, d) \rangle
\end{align*}
For the leading order term,
\begin{align*}
\langle \Psi^c(0), &\Theta_i^s(0, -X_{i-1}; \lambda) P_0^s(0) D_{i-1} d \rangle
= \langle \Psi^c(0), \Theta^s(0, -X_{i-1}; 0) P_0^s(0) D_{i-1} d \rangle + \mathcal{O}(e^{-2\alpha X_{i-1}}(|\lambda| + e^{-2 \alpha X_m})|d|) \\
&= \langle \Psi^c(0), \Theta(0, -X_{i-1}; 0) P^s_-(-X_{i-1}) P_0^s(0) D_{i-1} d \rangle + \mathcal{O}(e^{-2\alpha X_{i-1}}(|\lambda| + e^{-2 \alpha X_m})|d|) \\
&= \langle \Psi^c(-X_{i-1}), P^s_-(-X_{i-1}) P_0^s(0) D_{i-1} d \rangle + \mathcal{O}(e^{-2\alpha X_{i-1}}(|\lambda| + e^{-2 \alpha X_m})|d|)
\end{align*}
Using the expression for $D_{i-1}d$ from Lemma \ref{stabestimates},
\begin{align*}
P^s_-(-X_{i-1}) &P_0^s(0) D_{i-1} d = P^s_-(-X_{i-1}) P_0^s(0) ( Q'(X_{i-1}) + Q'(-X_{i-1})(d_i - d_{i-1}) \\
&+ \mathcal{O} \left( e^{-\alpha X_m} \left( |\lambda| +  e^{-\alpha X_m}  \right) |d| \right)
\end{align*}
For the first two terms on the RHS, we have
\begin{align*}
P^s_-(-X_{i-1}) P_0^s(0) Q'(X_{i-1})
&= P_0^s(0) Q'(X_{i-1}) + \mathcal{O}(e^{-2 \alpha X_{i-1}}) \\
&= P^s_+(X_{i-1}) Q'(X_{i-1}) + \mathcal{O}(e^{-2 \alpha X_{i-1}}) \\
&= Q'(X_{i-1}) + \mathcal{O}(e^{-2 \alpha X_{i-1}}) \\
\end{align*}
and
\begin{align*}
P^s_-(-X_{i-1}) P_0^s(0) Q'(-X_{i-1})
&= P^s_-(-X_{i-1}) Q'(-X_{i-1}) + \mathcal{O}(e^{-2 \alpha X_{i-1}}) \\
&= Q'(-X_{i-1}) + \mathcal{O}(e^{-2 \alpha X_{i-1}}) \\
\end{align*}
Thus the leading order term is
\begin{align*}
\langle \Psi^c(0), &\Theta_i^s(0, -X_{i-1}; \lambda) P_0^s(0) D_{i-1} d \rangle \\
&= \langle \Psi^c(-X_{i-1}), Q'(-X_{i-1}) + Q'(X_{i-1}) \rangle (d_i - d_{i-1} ) + \mathcal{O}(e^{-\alpha X_m}(|\lambda| + e^{-\alpha X_m})|d|) \\
&= \langle \Psi^c(-X_{i-1}), Q'(X_{i-1}) \rangle (d_i - d_{i-1} ) + \mathcal{O}(e^{-\alpha X_m}(|\lambda| + e^{-\alpha X_m})|d|)
\end{align*}
since $\langle \Psi^c(-X_{i-1}), Q'(-X_{i-1}) \rangle = 0$. For the higher order term, we use the bound for $A_4$ from Lemma \ref{Zinv2} to get
\begin{align*}
\langle \Psi^c(0), &P_i^-(0; \lambda) \Phi^s(0, -X_{i-1}; \lambda) A_4(\lambda)_{i-1}^-(c, d) \rangle \\
&\leq C e^{-\alpha X_{i-1}} \Big(  
e^{-\alpha X_{i-1}} (|\lambda| + e^{-\alpha X_m})(|e^{\nu(\lambda)X_{i-2}} c_{i-2}| + |e^{-\nu(\lambda)X_i}c_i|) \\
&+ e^{-(\alpha - \eta)X_{i-1}}|c_{i-1}| + e^{-\alpha X_m} |\lambda^2||d| + e^{-\alpha X_m}|D||d| \Big)
\end{align*}
Combining all of these, we have for the $a_{i-1}^-$ term,
\begin{align*}
\langle &\Psi^c(0), P_i^-(0; \lambda) \Phi^s(0, -X_{i-1}; \lambda) a_{i-1}^- \rangle = -\langle \Psi^c(-X_{i-1}), Q'(X_{i-1}) \rangle (d_i - d_{i-1} ) \\
&+ \mathcal{O}\Big(  
e^{-2 \alpha X_{i-1}} (|\lambda| + e^{-\alpha X_m})(|e^{\nu(\lambda)X_{i-2}} c_{i-2}| + |e^{-\nu(\lambda)X_i}c_i|) + e^{-(2 \alpha - \eta)X_{i-1}}|c_{i-1}| \\
&+ e^{-\alpha X_m} 
(|\lambda| + e^{-\alpha X_m})^2 |d| )
\end{align*}
Similarly, for the $a_i^+$ term, we have
\begin{align*}
\langle &\Psi^c(0), P_i^+(0; \lambda) \Phi^u(0, X_i; \lambda) a_i^+ \rangle = \langle \Psi^c(X_i), Q'(-X_i) \rangle (d_{i+1} - d_i ) \\
&+ \mathcal{O}\Big(  
e^{-2 \alpha X_i} (|\lambda| + e^{-\alpha X_m})(|e^{\nu(\lambda)X_{i-1}} c_{i-1}| + |e^{-\nu(\lambda)X_{i+1}}c_{i+1}|) + e^{-(2 \alpha - \eta)X_i}|c_i| \\
&+ e^{-\alpha X_m} 
(|\lambda| + e^{-\alpha X_m})^2 |d| )
\end{align*}
\end{enumerate}

The rest of the terms are higher order.
\begin{enumerate}

\item For the term involving $\tilde{c}$, we use $\tilde{c}_i = P_0^c(\lambda) D_i d + A_4(\lambda)_i^c(c, d) )$, the bound \eqref{P0cDid} from Lemma \ref{Zinv1}, and the bound $A_4$ from Lemma \ref{Zinv2} to get
\begin{align*}
|P_i^+&(0; \lambda) e^{-\nu(\lambda)X_i} \tilde{c}_i|
\leq C e^{-|\nu(\lambda)|X_i}|P_0^c(\lambda) D_i d + A_4(\lambda)_i^c(c, d) | \\
&\leq C e^{-|\nu(\lambda)|X_i}\Big( e^{-\alpha_0 X_i}(|\lambda| + e^{-\alpha_0 X_m})|d| +  
e^{-\alpha X_i} (|\lambda| + e^{-\alpha X_m})(|e^{\nu(\lambda)X_{i-1}} c_{i-1}| + |e^{-\nu(\lambda)X_{i+1}}c_{i+1}|) \\
&+ e^{-(\alpha - \eta)X_i}|c_i| + e^{-\alpha X_i} |\lambda^2||d| + e^{-\alpha X_i}|D||d| \Big) \\
&\leq C \Big( e^{-(\alpha - 2\eta)X_i}|c_i|  + e^{-(\alpha-\eta) X_i} (|\lambda| + e^{-\alpha X_m})(|e^{\nu(\lambda)X_{i-1}} c_{i-1}| + |e^{-\nu(\lambda)X_{i+1}}c_{i+1}|) \\
&+ (e^{-\alpha X_m} + |\lambda|)^2|d|  \Big)
\end{align*}

\item For the integral terms involving the center subspace, we use the estimate from Lemma \ref{stabestimates} to get 
\begin{align*}
&\left| \langle \Psi^c(0), P_i^-(0; \lambda) \lambda^2 d_i \int_{-X_{i-1}}^0 \Phi^c(0, y; \lambda) P_i^-(y; \lambda) \tilde{H}_i^-(y) dy \rangle \right| \\
&\leq C |\lambda|^2 |d_i| \int_{-X_{i-1}}^0 e^{-|\nu(\lambda)|y}e^{\alpha y} dy \\
&\leq C |\lambda|^2 |d| \int_{-X_{i-1}}^0 e^{\tilde{\alpha} y} dy \\
&\leq C |\lambda|^2 |d|
\end{align*}
The other one is similar.

\item For the non-center integral terms, we have the bound
\begin{align*}
&\left| P_i^-(0; \lambda) \lambda^2 d_i
\int_{-X_{i-1}}^0 \Phi^s(0, y; \lambda) P^-(y; \beta_i^-, \lambda)^{-1} \tilde{H}_i^-(y) dy \right| \\
&\leq C |\lambda|^2 |d_i| \int_{-X_{i-1}}^0 e^{\alpha_0 y} e^{\alpha_0 y} dy \\
&\leq C |\lambda|^2 |d|
\end{align*}
the other one is similar.

\item For the terms involving $b$, note that by Lemma \ref{PsiIP}, the terms $P^-(0) b_i^-$ and $P^+(0)b_i^+$ are eliminated outright when we take the inner product with $\Psi^c(0)$. For the other terms, we use the estimate for $B_1$ from Lemma \ref{Zinv2}.
\begin{align*}
&|(P_i^-(0; \lambda) - P^-(0))b_i^- + P_i^-(0; \lambda)(P_0^u(\lambda) - P_0^u(0))b_i^-| \\
&\leq C(|\lambda| + e^{-\alpha X_m}) |B_1(c, d)| \\
&\leq C(|\lambda| + e^{-\alpha X_m}) \Big( (|\lambda| + e^{-\alpha X_m})(|e^{\nu(\lambda)X_{i-1}}c_{i-1}| + |e^{-\nu(\lambda)X_i}c_i|)+ (|\lambda| + e^{-\alpha X_m})^2 |d| \Big)
\end{align*}
\end{enumerate}

Putting all of this together, we obtain the center jump expressions
\begin{align*}
\xi^c_i = \langle \Psi^c(X_i), &Q'(-X_i) \rangle (d_{i+1} - d_i ) + \langle \Psi^c(-X_{i-1}), Q'(X_{i-1}) \rangle (d_i - d_{i-1} ) \\
&+ e^{-\nu(\lambda) X_i} c_i^- - e^{\nu(\lambda) X_{i-1}} c_{i-1}^- +  R^c(\lambda)_i(c^-, d)
\end{align*}
The remainder term $R^c_i(c, d)$ has bound
\begin{align*}
||R^c&(c, d)_i|| \leq C \Big(
(|\lambda| + e^{-\alpha X_m})(|e^{\nu(\lambda)X_{i-1}}c_{i-1}| + |e^{-\nu(\lambda)X_i}c_i|) \\
&+ (|\lambda| + e^{-\alpha X_m})( e^{-(\alpha - \eta) X_{i-1}} |e^{\nu(\lambda)X_{i-2}}c_{i-2}| + e^{-(\alpha - \eta) X_i} |e^{-\nu(\lambda)X_{i+1}}c_{i+1}|)  \\
&+ (|\lambda| + e^{-\alpha X_m})^2 |d|
\Big)
\end{align*}

To write this in matrix form, let
\begin{align*}
K(\lambda) =  
\begin{pmatrix}
e^{-\nu(\lambda)X_1} & & & & & -e^{\nu(\lambda)X_0} \\
-e^{\nu(\lambda)X_1} & e^{-\nu(\lambda)X_2} \\
& -e^{\nu(\lambda)X_2} & e^{-\nu(\lambda)X_3} \\
\vdots & & \vdots & &&  \vdots \\
& & & & -e^{\nu(\lambda)X_{n-1}} & e^{-\nu(\lambda)X_0} 
\end{pmatrix}
\end{align*}
The leading order terms in the center jump expression involving $c$ are given by $K(\lambda)c$, where $c = (c_1, \dots, c_{n-1}, c_0)^T$. For the remainder terms, let $G_i$ be the sum of the remainder terms of the form $e^{\pm \nu(\lambda) X_i} c_i$. Then 
\[
G_i = \gamma_{i,i-1} e^{\nu(\lambda)X_{i-1}}c_{i-1} + \gamma_{i,i} e^{-\nu(\lambda)X_i}c_i + \gamma_{i,i-2} e^{\nu(\lambda)X_{i-2}}c_{i-2} + \gamma_{i,i+1} e^{-\nu(\lambda)X_{i+1}}c_{i+1}
\] 
where from the remainder bound we have for the coefficients $\gamma_{i, j}$
\begin{align*}
\gamma_{i,i-1}, \gamma_{i,i} &= \mathcal{O}(|\lambda| + e^{-\alpha X_m}) \\
\gamma_{i,i-2} &= \mathcal{O}(e^{-(\alpha - \eta) X_{i-1}}(|\lambda| + e^{-\alpha X_m})) \\
\gamma_{i,i+1} &= \mathcal{O}(e^{-(\alpha - \eta) X_i}(|\lambda| + e^{-\alpha X_m}))
\end{align*}
Adding and subtracting $e^{\nu(\lambda)X_i}c_i$ and $e^{-\nu(\lambda)X_{i-1}}c_{i-1}$, this becomes
\begin{align*}
G_i &= \gamma_{i,i-1} e^{\nu(\lambda)X_{i-1}}c_{i-1} + \gamma_{i,i} e^{-\nu(\lambda)X_i}c_i + \gamma_{i,i-2} ( e^{\nu(\lambda)X_{i-2}}c_{i-2} - e^{-\nu(\lambda)X_{i-1}}c_{i-1}) \\
&+ \gamma_{i,i-2} e^{-\nu(\lambda)X_{i-1}}c_{i-1} + \gamma_{i,i+1} (e^{-\nu(\lambda)X_{i+1}}c_{i+1} - e^{\nu(\lambda)X_i}c_i) + \gamma_{i,i+1} e^{\nu(\lambda)X_i}c_i
\end{align*}
Next, we note that
\begin{align*}
\gamma_{i,i-2} e^{-\nu(\lambda)X_{i-1}}c_{i-1} &= \mathcal{O}(e^{-(\alpha - 3 \eta) X_{i-1}}(|\lambda| + e^{-\alpha X_m})e^{\nu(\lambda)X_{i-1}}c_{i-1} \\
\gamma_{i,i+1} e^{\nu(\lambda)X_i}c_i &= \mathcal{O}(e^{-(\alpha - 3 \eta) X_i}(|\lambda| + e^{-\alpha X_m})e^{-\nu(\lambda)X_i}c_i
\end{align*}
Both of these coefficients are higher order than $\gamma_{i,i-1}$ and $\gamma_{i,i}$. Substituting these into $G_i$, collecting terms, and keeping the notation $\gamma_{i,j}$ for the resulting coefficients, we have
\begin{align*}
G_i(&\lambda) = \gamma_{i,i-1} e^{\nu(\lambda)X_{i-1}}c_{i-1} + \gamma_{i,i} e^{-\nu(\lambda)X_i}c_i \\
&+ \gamma_{i,i-2} ( e^{\nu(\lambda)X_{i-2}}c_{i-2} - e^{-\nu(\lambda)X_{i-1}}c_{i-1}) + \gamma_{i,i+1} (e^{-\nu(\lambda)X_{i+1}}c_{i+1} - e^{\nu(\lambda)X_i}c_i)
\end{align*}
where we have the same bounds on the coefficients $\gamma_{i,j}$. Using these, we can write the terms involving $c$ in matrix form as
\[
(K(\lambda) + C_1 K(\lambda) + K_1(\lambda)) c
\]
where $K_1(\lambda)$ is ``$\gamma-$perturbation'' of $K(\lambda)$ given by
\begin{align*}
K_1(\lambda) =  
\begin{pmatrix}
e^{-\nu(\lambda)X_1} \gamma_{1,1} & & & & & e^{\nu(\lambda)X_0}\gamma_{1,0} \\
e^{\nu(\lambda)X_1}\gamma_{2,1} & e^{-\nu(\lambda)X_2}\gamma_{2,2} \\
& e^{\nu(\lambda)X_2}\gamma_{3,2} & e^{-\nu(\lambda)X_3}\gamma_{3,3} \\
\vdots & & \vdots & &&  \vdots \\
& & & & e^{\nu(\lambda)X_{n-1}}\gamma_{0,n-1} & e^{-\nu(\lambda)X_0}\gamma_{0,0} 
\end{pmatrix}
\end{align*}
with 
\[
\gamma_{i,i-1}, \gamma_{i,i} = \mathcal{O}(|\lambda| + e^{-\alpha X_m})
\] 
and $C_1$ is the periodic, banded matrix
\begin{align*}
C_1 &= \begin{pmatrix}
0 & \gamma_{1,2} & 0 & 0 & \dots & 0 & -\gamma_{n-1,0} & 0 \\
0 & 0 & \gamma_{2,3} & 0 & \dots & 0 & 0 & -\gamma_{2,1} \\
-\gamma_{3,1} & 0 & 0 & \gamma_{3,4} & \dots & 0 & 0 & 0 \\
&  & & \ddots  \\
0 & 0 & 0 & 0 & \dots & 0 & 0 & \gamma_{n-1,0} \\
\gamma_{0,1} & 0 & 0 & 0 & \dots & -\gamma_{0, n-2} & 0 & 0 
\end{pmatrix}
\end{align*}
which has uniform bound
\begin{align*}
||C_1|| &= 
\mathcal{O}(e^{-(\alpha - \eta) X_m}(|\lambda| + e^{-\alpha X_m}))
\end{align*}

For the terms involving $D$, let
\[
\tilde{a}_i = \langle \Psi^c(X_i), Q'(-X_i) \rangle 
\]
By reversibility, $Q(-x) = R Q(x)$ implies $Q'(-x) = -R Q'(x)$. From Lemma \ref{varadjsolutions}, $\Psi^c(x) = R \Psi(-x)$. Thus, since $R^2 = I$ and $R$ is self-adjoint,
\begin{align*}
\langle \Psi^c(-X_i), Q'(X_i) \rangle &= \langle \Psi^c(-X_i), R^2 Q'(X_i) \rangle \\
&= -\langle R \Psi^c(-X_i), Q'(-X_i) \rangle \\
&= -\langle \Psi^c(X_i), Q'(-X_i) \rangle \\
&= -a_i
\end{align*}
Define the matrix $\tilde{A}$ by
\begin{align*}
\tilde{A} &= \begin{pmatrix}
-\tilde{a}_0 - \tilde{a}_1 & \tilde{a}_0 + \tilde{a}_1 \\
\tilde{a}_0 + \tilde{a}_1 & -\tilde{a}_0 - \tilde{a}_1
\end{pmatrix} && n = 2 \\
\tilde{A} &= \begin{pmatrix}
-\tilde{a}_{n-1} - \tilde{a}_0 & \tilde{a}_0 & & & \dots & \tilde{a}_{n-1}\\
\tilde{a}_0 & -\tilde{a}_0 - \tilde{a}_1 & \tilde{a}_1 \\
& \tilde{a}_1 & -\tilde{a}_1 - \tilde{a}_2 & \tilde{a}_2 \\
& & \vdots & & \vdots \\
\tilde{a}_{n-1} & & & & \tilde{a}_{n-2} & -\tilde{a}_{n-2} - \tilde{a}_{n-1} \\
\end{pmatrix} && n > 2
\end{align*}
We note that we have the bound
\[
\tilde{A} = \mathcal{O}(e^{-\alpha X_m})
\]

Let $D_1$ be the matrix we get from the terms involving $d$ in $R^c(\lambda)_i(c, d)$. Then $D_1$ has uniform bound
\begin{align*}
D_1 &= \mathcal{O}((|\lambda| + e^{-\alpha X_m})^2)
\end{align*}
We can write the terms involving $d$ in the center jump equation as
\[
(\tilde{A} + D_1) d
\]
Combining everything, the center jump condition can be written in matrix form as
\[
(K(\lambda) + C_1 K(\lambda) + K_1(\lambda)) c + (\tilde{A} + D_1) d = 0
\]

\end{proof}
\end{lemma}

Finally, we compute the jump in the direction of $\Psi(0)$.
% lemma : jump in decaying adjoint direction
\begin{lemma}\label{jumpadj}
The jumps in the direction of $\Psi(0)$ are given by
\begin{equation}
\begin{aligned}
\xi_i = 
\langle \Psi(X_i), Q'(-X_i) \rangle (d_{i+1} - d_i ) + \langle \Psi(-X_{i-1}), Q'(X_{i-1}) \rangle (d_i - d_{i-1} ) \\
+ M_\Psi \lambda( e^{-\nu(\lambda)X_i}c_i + e^{\nu(\lambda)X_{i-1}}c_{i-1})
- \lambda_2 d_i M + R_i(\lambda)(c, d)
\end{aligned}
\end{equation}
$M$ is the higher order Melnikov integral
\begin{equation}\label{M}
M = \int_{-\infty}^\infty \langle \Psi(y), H(y) \rangle dy
\end{equation}
The remainder term $R(\lambda)(c^-, d)$ is analytic in $\lambda$, linear in $(c^-, d)$, and has piecewise bound
\begin{align*}
|R(\lambda)_i&(c_i^-, d)| \leq C \Big( (|\lambda| + e^{-\alpha X_m})^2(|e^{\nu(\lambda)X_{i-1}}c_{i-1}| + |e^{-\nu(\lambda)X_i}c_i|) \\
&+ (|\lambda| + e^{-\alpha X_m})^2(e^{-(\alpha - \eta) X_{i-1}} |e^{\nu(\lambda)X_{i-2}}c_{i-2}| + e^{-(\alpha - \eta) X_i} |e^{-\nu(\lambda)X_{i+1}}c_{i+1}|) \\
&+ (|\lambda| + e^{-\alpha X_m})^3 |d| \Big)
\end{align*}
We can write these conditions in matrix form as
\begin{equation}
(\lambda M_\Psi \tilde{K}(\lambda) + C_2 K(\lambda) + K_2(\lambda))c + (A - \lambda^2 M I + D_2)d = 0
\end{equation}
The matrix $A$ is given by
\begin{align*}
A &= \begin{pmatrix}
-a_0 -a_1 & a_0 + a_1 \\
-a_0 + a_1 & -a_0 - a_1
\end{pmatrix} && n = 2 \\
A &= \begin{pmatrix}
-a_{n-1} - a_0 & a_0 & & & \dots & a_{n-1}\\
a_0 & -a_0 - a_1 &  a_1 \\
& a_1 & -a_1 - a_2 &  a_2 \\
& & \vdots & & \vdots \\
a_{n-1} & & & & a_{n-2} & -a_{n-2} - a_{n-1} \\
\end{pmatrix} && n > 2
\end{align*}
with
\begin{align*}
a_i &= \langle \Psi(X_i), Q'(-X_i) \rangle \\
\end{align*}
The matrix $K(\lambda)$ is defined in Lemma \ref{jumpcenteradj}. The matrix $\tilde{K}(\lambda)$ is given by
\begin{align*}
\tilde{K}(\lambda) =  
\begin{pmatrix}
e^{-\nu(\lambda)X_1} & & & & & e^{\nu(\lambda)X_0} \\
e^{\nu(\lambda)X_1} & e^{-\nu(\lambda)X_2} \\
& e^{\nu(\lambda)X_2} & e^{-\nu(\lambda)X_3} \\
\vdots & & \vdots & &&  \vdots \\
& & & & e^{\nu(\lambda)X_{n-1}} & e^{-\nu(\lambda)X_0} 
\end{pmatrix}
\end{align*}
The remainder matrices have uniform bounds
\begin{align*}
C_2 &= \mathcal{O}(e^{-(\alpha - \eta) X_m}(|\lambda| + e^{-\alpha X_m})^2) \\
D_2 &= \mathcal{O}((|\lambda| + e^{-\alpha X_m})^3)
\end{align*}
$K_2(\lambda)$ has the same form as $K(\lambda)$, but each term in $K(\lambda)$ has been multiplied by a factor of order $\mathcal{O}(|\lambda| + e^{-\alpha X_m})^2$. The specific forms of $K_2(\lambda)$ and $C_2$ are given in the proof.

\begin{proof}
Recall that the terms $P_i^\pm(0; \lambda) Z_i^\pm(0)$ are given by
\begin{align*}
P_i^-(0; \lambda) Z_i^-(0) &= P^-(0) b_i^- + P_i^-(0; \lambda) e^{\nu(\lambda) X_{i-1}} c_{i-1} \\
&+ P_i^-(0; \lambda) \Phi^s(0, -X_{i-1}; \lambda) a_{i-1}^- + (P_i^-(0; \lambda) - P^-(0))b_i^- + P_i^-(0; \lambda)(P_0^u(\lambda) - P_0^u(0))b_i^- \\
&+ \lambda^2 d_i P_i^-(0; \lambda) \int_{-X_{i-1}}^0 \Phi^s(0, y; \lambda) P_i^-(y; \lambda)^{-1} \tilde{H}_i^-(y) dy \\
&+ \lambda^2 d_i P_i^-(0; \lambda) \int_{-X_{i-1}}^0 \Phi^c(0, y; \lambda) P_i^-(y; \lambda)^{-1} \tilde{H}_i^-(y) dy  \\ 
P_i^+(0; \lambda) Z_i^+(0) &= P^+(0) b_i^+ + P_i^+(0; \lambda) e^{-\nu(\lambda)X_i} c_i \\
&+ P_i^+(0; \lambda) \Phi^u(0, X_i; \lambda) a_i^+ + (P_i^+(0; \lambda) - P^+(0)) b_i^+ + P_i^+(0; \lambda) (P_0^s(\lambda) - P_0^s(0)) b_i^+ \\
&+ P_i^+(0; \lambda) e^{-\nu(\lambda)X_i} \tilde{c}_i \\
&+ \lambda^2 d_i P_i^+(0; \lambda) \int_{X_i}^0 \Phi^u(0, y; \lambda) P_i^+(y; \lambda)^{-1} \tilde{H}_i^+(y) dy \\
&+ \lambda^2 d_i P_i^+(0; \lambda) \int_{X_i}^0 \Phi^c(0, y; \lambda) P_i^+(y; \lambda)^{-1} \tilde{H}_i^+(y) dy
\end{align*}

As in Lemma \ref{jumpcenteradj}, we begin by computing the leading order terms.

\begin{enumerate}
\item The non-center integral will give us the higher order Melnikov integral. For the ``minus'' piece, we use the uniform estimate $\tilde{H}_i^-(y) = H(y) + \mathcal{O}(e^{-\alpha_0 X_m})$ from Lemma \ref{stabestimates} to get
\begin{align*}
&\langle \Psi(0), P_i^-(0; \lambda) \int_{-X_{i-1}}^0 \Phi^s(0, y; \lambda) P_i^-(y; \lambda)^{-1} \tilde{H}_i^-(y) dy \rangle \\
&= \int_{-X_{i-1}}^0 \langle \Psi(0), P_i^-(0; \lambda), \Phi^s(0, y; \lambda) P_i^-(y; \lambda)^{-1} \tilde{H}_i^-(y) \rangle dy \\
&= \int_{-X_{i-1}}^0 \langle \Psi(0), P_i^-(0; \lambda), \Phi^s(0, y; \lambda) P_i^-(y; \lambda)^{-1} H(y) \rangle dy + \mathcal{O}({e^{-\alpha_0 X_m}})\\
&= \int_{-X_{i-1}}^0 \langle \Psi(0), \Theta(0, y) H(y) \rangle dy + \mathcal{O}(|\lambda| + {e^{-\alpha X_m}})\\
&= \int_{-X_{i-1}}^0 \langle \Theta(y, 0)^* \Psi_i(0), H(y) \rangle dy + \mathcal{O}(|\lambda| + {e^{-\alpha X_m}})\\
&= \int_{-X_{i-1}}^0 \langle \Psi(y), H(y) \rangle dy + \mathcal{O}(|\lambda| + {e^{-\alpha X_m}})\\
&= \int_{-\infty}^0 \langle \Psi(y), H(y) \rangle dy + \mathcal{O}(|\lambda| + {e^{-\alpha X_m}})
\end{align*}
The ``positive'' piece is similar, and gives us the other half of the Melnikov integral.

\item For the terms involving $a$, we use the expressions from Lemma \ref{Zinv2}. This is similar to what we did in Lemma \ref{jumpcenteradj}. For the term involving $a_{i-1}^-$, we have 
\begin{align*}
\langle &\Psi(0), P_i^-(0; \lambda) \Phi^s(0, -X_{i-1}; \lambda) a_{i-1}^- \rangle \\
&= -\langle \Psi(0), \Theta_i^s(0, -X_{i-1}; \lambda) P_0^s(0) D_{i-1} d \rangle + \langle \Psi(0), P_i^-(0; \lambda) \Phi^s(0, -X_{i-1}; \lambda) A_4(\lambda)_{i-1}^-(c, d) \rangle
\end{align*}
For the leading order term, similar to Lemma \ref{jumpcenteradj}, we have
\begin{align*}
\langle \Psi(0), &\Theta_i^s(0, -X_{i-1}; \lambda) P_0^s(0) D_{i-1} d \rangle \\
&= \langle \Psi(-X_{i-1}), P^s_-(-X_{i-1}) P_0^s(0) D_{i-1} d \rangle + \mathcal{O}(e^{-2 \alpha X_{i-1}}(|\lambda| + e^{-2 \alpha X_m})|d|)
\end{align*}
Substituting the expression for $D_{i-1}d$ from Lemma \ref{stabestimates} and proceeding as in Lemma \ref{jumpcenteradj}, the leading order term becomes
\begin{align*}
\langle \Psi(0), &\Theta_i^s(0, -X_{i-1}; \lambda) P_0^s(0) D_{i-1} d \rangle \\
&= \langle \Psi(-X_{i-1}), Q'(-X_{i-1}) + Q'(X_{i-1}) \rangle (d_i - d_{i-1} ) + \mathcal{O}(e^{-2 \alpha X_m}(|\lambda| + e^{-\alpha X_m})|d|) \\
&= \langle \Psi(-X_{i-1}), Q'(X_{i-1}) \rangle (d_i - d_{i-1} ) + \mathcal{O}(e^{-2 \alpha X_m}(|\lambda| + e^{-\alpha X_m})|d|)
\end{align*}
since $\langle \Psi(-X_{i-1}), Q'(-X_{i-1}) \rangle = 0$. For the higher order term, we use the bound for $A_4$ from Lemma \ref{Zinv2} to get
\begin{align*}
\langle \Psi(0), &P_i^-(0; \lambda) \Phi^s(0, -X_{i-1}; \lambda) A_4(\lambda)_{i-1}^-(c, d) \rangle \\
&\leq C e^{-\alpha X_{i-1}} \Big(  
e^{-\alpha X_{i-1}} (|\lambda| + e^{-\alpha X_m})(|e^{\nu(\lambda)X_{i-2}} c_{i-2}| + |e^{-\nu(\lambda)X_i}c_i|) \\
&+ e^{-(\alpha - \eta)X_{i-1}}|c_{i-1}| + e^{-\alpha X_m} |\lambda^2||d| + e^{-\alpha X_m}|D||d| \Big)
\end{align*}
Combining all of these, we have for the $a_{i-1}^-$ term,
\begin{align*}
\langle &\Psi(0), P_i^-(0; \lambda) \Phi^s(0, -X_{i-1}; \lambda) a_{i-1}^- \rangle = -\langle \Psi(-X_{i-1}), Q'(X_{i-1}) \rangle (d_i - d_{i-1} ) \\
&+ \mathcal{O}\Big(  
e^{-2 \alpha X_{i-1}} (|\lambda| + e^{-\alpha X_m})(|e^{\nu(\lambda)X_{i-2}} c_{i-2}| + |e^{-\nu(\lambda)X_i}c_i|) + e^{-(2 \alpha - \eta)X_{i-1}}|c_{i-1}| \\
&+ (|\lambda| + e^{-\alpha X_m})^3 |d| )
\end{align*}
Similarly, for the $a_i^+$ term, we have
\begin{align*}
\langle &\Psi(0), P_i^+(0; \lambda) \Phi^u(0, X_i; \lambda) a_i^+ \rangle = \langle \Psi(X_i), Q'(-X_i) \rangle (d_{i+1} - d_i ) \\
&+ \mathcal{O}\Big(  
e^{-2 \alpha X_i} (|\lambda| + e^{-\alpha X_m})(|e^{\nu(\lambda)X_{i-1}} c_{i-1}| + |e^{-\nu(\lambda)X_{i+1}}c_{i+1}|) + e^{-(2 \alpha - \eta)X_i}|c_i| \\
&+ e^{-\alpha X_m} 
(|\lambda| + e^{-\alpha X_m})^3 |d| )
\end{align*}
\end{enumerate}

The remaining terms will be higher order. Doing these in turn, we have
\begin{enumerate}
\item For the terms involving $b$, we first note that by Lemma \ref{PsiIP}, the terms $P^-(0) b_i^-$ and $P^+(0)b_i^+$ will vanish when we take the inner product with $\Psi(0)$. For the remaining terms, we substitute the estimate for $B_1$ from Lemma \ref{Zinv2}.
\begin{align*}
&|\langle \Psi(0), (P_i^-(0; \lambda) - P^-(0))b_i^- + P_i^-(0; \lambda)(P_0^u(\lambda) - P_0^u(0))b_i^-| \\
&\leq C (|\lambda| + e^{-\alpha X_m})\Big( 
(|\lambda| + e^{-\alpha X_m})( |e^{\nu(\lambda)X_{i-1}} c_{i-1}| + |e^{-\nu(\lambda)X_i} c_i|)+ (|\lambda| + e^{-\alpha X_m})^2|d| \Big)
\end{align*}

\item For the terms involving $c$, we use Lemmas \ref{Vpmlambdalemma} and \ref{Ecconj} to get
\begin{align*}
\langle \Psi(0), P_i^-(0; \lambda) e^{\nu(\lambda) X_{i-1}} c_{i-1} \rangle &= \langle \Psi^c(0), V^-(0; \lambda) e^{\nu(\lambda) X_{i-1}} c_{i-1} \rangle + \mathcal{O}(e^{-2 \alpha X_m}|e^{\nu(\lambda) X_{i-1}} c_{i-1}|) \\
&= -M_\phi \lambda e^{\nu(\lambda) X_{i-1}} c_{i-1} + \mathcal{O}((|\lambda|^2 + e^{-2 \alpha X_m})|e^{\nu(\lambda) X_{i-1}} c_{i-1}|)
\end{align*}
Similarly,
\begin{align*}
\langle \Psi(0), P_i^+(0; \lambda) e^{-\nu(\lambda) X_i} c_i \rangle 
&= M_\phi \lambda e^{-\nu(\lambda) X_i} c_{i-1} + \mathcal{O}((|\lambda|^2 + e^{-2 \alpha X_m})|e^{-\nu(\lambda) X_i} c_i|)
\end{align*}

\item For the center integral term, we have
\begin{align*}
&\langle \Psi(0), P_i^-(0; \lambda)
\int_{-X_{i-1}}^0 \Phi^c(0, y; \lambda) P_i^-(y; \lambda)^{-1} \tilde{H}_i^-(y) dy \rangle \\
&= \int_{-X_{i-1}}^0 \langle \Psi(0), P_i^-(0; \lambda) \Phi^c(0, y; \lambda) P_i^-(y; \lambda)^{-1} \tilde{H}_i^-(y) \rangle dy \\
&= \int_{-X_{i-1}}^0 \langle \Psi(0), P^-(0) \Phi^c(0, y; 0) P^-(y)^{-1} \tilde{H}_i^-(y) \rangle dy + \mathcal{O}(|\lambda| + e^{-\alpha X_m}) \\
&= \mathcal{O}(|\lambda| + e^{-\alpha X_m})
\end{align*}
where the integral vanishes by Lemma \ref{PsiIP} since $\Phi^c(0, y; 0) P^-(y)^{-1} \tilde{H}_i^-(y) \in E^c(0)$.

\item For the term involving $\tilde{c}$, we first use Lemmas \ref{Vpmlambdalemma} and \ref{Ecconj} to get
\begin{align*}
\langle \Psi(0), P_i^+(0; \lambda) e^{-\nu(\lambda) X_i}\tilde{c}_i \rangle \mathcal{O}((|\lambda| + e^{-2 \alpha X_m})|e^{-\nu(\lambda) X_i}\tilde{c}_i|)
\end{align*}
We obtained a bound for $|e^{-\nu(\lambda) X_i}\tilde{c}_i|$ in the previous lemma. Using that here, we have
\begin{align*}
|\langle &\Psi(0), P_i^+(0; \lambda) e^{-\nu(\lambda) X_i}\tilde{c}_i \rangle| \leq C (|\lambda| + e^{-2 \alpha X_m})\Big( e^{-(\alpha - 2\eta)X_i}|c_i| \\
&+ e^{-(\alpha-\eta) X_i} (|\lambda| + e^{-\alpha X_m})(|e^{\nu(\lambda)X_{i-1}} c_{i-1}| + |e^{-\nu(\lambda)X_{i+1}}c_{i+1}|) + (e^{-\alpha X_m} + |\lambda|)^2|d|  \Big)
\end{align*}

\end{enumerate}

Putting this all together, we have the jump expressions
\begin{align*}
\xi_i = \langle \Psi(X_i), Q'(-X_i) \rangle (d_{i+1} - d_i ) + \langle \Psi(-X_{i-1}), Q'(X_{i-1}) \rangle (d_i - d_{i-1} ) \\
+ M_\Psi \lambda( e^{-\nu(\lambda)X_i}c_i + e^{\nu(\lambda)X_{i-1}}c_{i-1})
- \lambda_2 d_i M + R_i(\lambda)(c, d)
\end{align*}
where $M$ is the higher order Melnikov integral
\[
M = \int_{-\infty}^\infty \langle \Psi(y), H(y) \rangle dy
\]
and the remainder term has piecewise bound
\begin{align*}
|R(\lambda)_i&(c_i^-, d)| \leq C \Big( (|\lambda| + e^{-\alpha X_m})^2(|e^{\nu(\lambda)X_{i-1}}c_{i-1}| + |e^{-\nu(\lambda)X_i}c_i|) \\
&+ (|\lambda| + e^{-\alpha X_m})^2(e^{-(\alpha - \eta) X_{i-1}} |e^{\nu(\lambda)X_{i-2}}c_{i-2}| + e^{-(\alpha - \eta) X_i} |e^{-\nu(\lambda)X_{i+1}}c_{i+1}|) \\
&+ (|\lambda| + e^{-\alpha X_m})^3 |d| \Big)
\end{align*}

As in Lemma \ref{jumpcenteradj}, we will write these jump expressions in matrix form. The terms involving the $c$ terms work out similarly to those in Lemma \ref{jumpcenteradj}. They are given by
\[
(\lambda M_\Psi \tilde{K}(\lambda) + C_2 K(\lambda) + K_2(\lambda))c
\]
$K(\lambda)$ is the same matrix as in Lemma \ref{jumpcenteradj}. $\tilde{K}(\lambda)$ is the same matrix as $K(\lambda)$, except all terms are positive.
\begin{align*}
\tilde{K}(\lambda) =  
\begin{pmatrix}
e^{-\nu(\lambda)X_1} & & & & & e^{\nu(\lambda)X_0} \\
e^{\nu(\lambda)X_1} & e^{-\nu(\lambda)X_2} \\
& e^{\nu(\lambda)X_2} & e^{-\nu(\lambda)X_3} \\
\vdots & & \vdots & &&  \vdots \\
& & & & e^{\nu(\lambda)X_{n-1}} & e^{-\nu(\lambda)X_0} 
\end{pmatrix}
\end{align*}
$K_2(\lambda)$ is the ``$\tilde{\gamma}-$perturbation'' of $K(\lambda)$ given by
\begin{align*}
K_2(\lambda) =  
\begin{pmatrix}
e^{-\nu(\lambda)X_1} \tilde{\gamma}_{1,1} & & & & & e^{\nu(\lambda)X_0}\tilde{\gamma}_{1,0} \\
e^{\nu(\lambda)X_1}\tilde{\gamma}_{2,1} & e^{-\nu(\lambda)X_2}\tilde{\gamma}_{2,2} \\
& e^{\nu(\lambda)X_2}\tilde{\gamma}_{3,2} & e^{-\nu(\lambda)X_3}\tilde{\gamma}_{3,3} \\
\vdots & & \vdots & &&  \vdots \\
& & & & e^{\nu(\lambda)X_{n-1}}\tilde{\gamma}_{0,n-1} & e^{-\nu(\lambda)X_0}\tilde{\gamma}_{0,0} 
\end{pmatrix}
\end{align*}
where 
\begin{align*}
\tilde{\gamma}_{i,i-1}, \tilde{\gamma}_{i,i} &= \mathcal{O}(|\lambda| + e^{-\alpha X_m})^2
\end{align*}
$C_2$ is the periodic, banded matrix
\begin{align*}
C_2 &= \begin{pmatrix}
0 & \tilde{\gamma}_{1,2} & 0 & 0 & \dots & 0 & -\tilde{\gamma}_{n-1,0} & 0 \\
0 & 0 & \gamma_{2,3} & 0 & \dots & 0 & 0 & -\tilde{\gamma}_{2,1} \\
-\tilde{\gamma}_{3,1} & 0 & 0 & \tilde{\gamma}_{3,4} & \dots & 0 & 0 & 0 \\
&  & & \ddots  \\
0 & 0 & 0 & 0 & \dots & 0 & 0 & \tilde{\gamma}_{n-1,0} \\
\tilde{\gamma}_{0,1} & 0 & 0 & 0 & \dots & -\tilde{\gamma}_{0, n-2} & 0 & 0 
\end{pmatrix}
\end{align*}
where
\begin{align*}
\tilde{\gamma}_{i,i-2}, \tilde{\gamma}_{i,i+1} &= \mathcal{O}(e^{-(\alpha - \eta) X_m}(|\lambda| + e^{-\alpha X_m})^2) 
\end{align*}
The matrix $C_2$ has uniform bound
\begin{align*}
C_2 &= \mathcal{O}(e^{-(\alpha - \eta) X_m}(|\lambda| + e^{-\alpha X_m})^2)
\end{align*}

For the terms involving $d$, let
\[
a_i = \langle \Psi(X_i), Q'(-X_i) \rangle 
\]
By reversibility, $Q(-x) = R Q(x)$ implies $Q'(-x) = -R Q'(x)$. From Lemma \ref{varadjsolutions}, $\Psi(x) = R \Psi(-x)$. Thus, since $R^2 = I$ and $R$ is self-adjoint, we have as in Lemma \ref{jumpcenteradj},
\begin{align*}
\langle \Psi(-X_i), Q'(X_i) \rangle &= \langle \Psi(-X_i), R^2 Q'(X_i) \rangle \\
&= -\langle R \Psi(-X_i), Q'(-X_i) \rangle \\
&= -\langle \Psi(X_i), Q'(-X_i) \rangle \\
&= -a_i
\end{align*}
Thus the terms involving $d$ are given in matrix form by
\[
(A - \lambda^2 M I + D_2)d
\]
The matrix $A$ is given by
\begin{align*}
A &= \begin{pmatrix}
-a_0 -a_1 & a_0 + a_1 \\
-a_0 + a_1 & -a_0 - a_1
\end{pmatrix} && n = 2 \\
A &= \begin{pmatrix}
-a_{n-1} - a_0 & a_0 & & & \dots & a_{n-1}\\
a_0 & -a_0 - a_1 &  a_1 \\
& a_1 & -a_1 - a_2 &  a_2 \\
& & \vdots & & \vdots \\
a_{n-1} & & & & a_{n-2} & -a_{n-2} - a_{n-1} \\
\end{pmatrix} && n > 2
\end{align*}
$M$ is the higher order Melnikov integral
\[
M = \int_{-\infty}^\infty \langle \Psi(y), H(y) \rangle dy
\]
The remainder terms involving $D$ are collected into the matrix $D_2$, which has uniform bound
\begin{align*}
D_2 &= \mathcal{O}((|\lambda| + e^{-\alpha X_m})^3)
\end{align*}
\end{proof}
\end{lemma}

\subsection{Proof of Theorem \ref{blockmatrixtheorem}}

Theorem \ref{blockmatrixtheorem} combines the jump matrix formulas from Lemma \ref{jumpcenteradj} and Lemma \ref{jumpadj} into a single block matrix. 

\section{Proof of eigenvalue location theorems}

In Theorem \ref{locateeigtheorem}, we solve the block matrix equation from Theorem \ref{blockmatrixtheorem} to locate the PDE eigenvalues $\lambda$ for $|\lambda| < \delta$, where $\delta$ is given in Theorem \ref{blockmatrixtheorem}. We prove the theorem below in a series of lemmas. 

\subsection{Rescaling}

First, we adopt the same scaling and parameterization as in the existence problem. Recall from Theorem \ref{perexist} that a periodic $n-$pulse $Q_np(x)$ is specified by a scaling parameter $r \in \mathcal{R}$ and $n$ length parameters $b_j$, which depend on $r$ as well as a phase parameter $\theta$ and a baseline length parameter $b_j^0$.

In the next lemma, we rewrite the terms in the block matrix equation \eqref{blockeq} from Theorem \ref{blockmatrixtheorem} using this parameterization.

% block matrix reparameterization
\begin{lemma}\label{reparam}
Using the scaling and parameterization above, the block matrix equation from Theorem \ref{blockmatrixtheorem} takes the form
\begin{equation}\label{blockeq2}
\begin{pmatrix}
K(\lambda) + C_1 K(\lambda) + K_1(\lambda) & D_1 \\
C_2 K(\lambda) + K_2(\lambda) & r \tilde{A} - \lambda^2 MI + D_2
\end{pmatrix}
\begin{pmatrix}c \\ d \end{pmatrix} 
= 0
\end{equation}
$\tilde{A}$ has the same form as the matrix $A$ from Theorem \ref{blockmatrixtheorem}, except the entries $a_j$ replaced by $\tilde{a}_j$, where
\begin{align}\label{tildea}
\tilde{a}_j 
&= (-1)^{-\rho \log r / \pi} s_0 e^{\alpha \phi/\beta} \left( \beta b_j \cos\left( -\rho \log b_j \right) - \alpha b_j \sin \left( -\rho \log b_j  \right) \right) + \mathcal{O}(r^{\gamma/2\alpha})
\end{align}
The eigenvalues of $A$ are given by $\{0, \tilde{\mu_1}, \dots, \tilde{\mu}_{n-1} \}$, where $r \tilde{\mu}_j = \mu_j$ and $\mu_j$ is an eigenvalue of $A$.

The remainder terms have bounds
\begin{align*}
C_1 &= \mathcal{O}\left(r^{\tilde{\gamma}/2}(|\lambda| + r^{1/2})\right) \\
D_1 &= \mathcal{O}\left((|\lambda| + r^{1/2})^2\right) \\
C_2 &= \mathcal{O}\left(r^{\tilde{\gamma}/2}(|\lambda| + r^{1/2})^2\right) \\
D_2 &= \mathcal{O}\left((|\lambda| + r^{1/2})^3\right)
\end{align*}
where $0 < \tilde{\gamma} < 1$. The matrices $K_1(\lambda)$ and $K_2(\lambda)$ are perturbations of the nonzero entries of $K(\lambda)$ by $\mathcal{O}(|\lambda| + r^{1/2})$.

Finally the domain half-length $X$ is given by
\begin{align}\label{Xscaled}
X &= \frac{1}{2\alpha} (n |\log r| + |\log (b_0\cdots b_{n-1})| ) - \frac{n \phi}{2 \beta}
\end{align}

\begin{proof}
From the discussion following Lemma \ref{IPform}, 
\begin{align*}
r &= e^{-\alpha(2 X^* + \phi/\beta)} \\
b_j &= e^{-2 \alpha(X_j - X^*)} && j = 0, \dots, n-1
\end{align*}
where $X^* \leq X_m$. In terms of $b_j$ and $r$,
\begin{align*}
X^* &= -\frac{1}{2\alpha}\log r - \alpha \frac{\phi}{\beta} \\
X_j &= -\frac{1}{2\alpha}\log(b_j r) - \frac{\phi}{2 \beta} 
\end{align*}
In particular,
\begin{align*}
e^{-2 \alpha X_m} &\leq e^{-2 \alpha X^*} \leq C r \\
e^{-2 \alpha X_j} &= e^{\phi \alpha/\beta} _j r \\
\end{align*}

First, we rewrite the matrix $A$ using this scaling. From Lemma 6.1 in \cite{Sandstede1998},
\begin{align*}\label{IPpsiQprime}
\langle \Psi(-x), Q'(x) \rangle
&= s_0 e^{-2 \alpha x}\left( \beta \cos(2 \beta x + \phi) - \alpha \sin(2 \beta x + \phi)\right) + \mathcal{O}(e^{-(2 \alpha + \gamma) x}
\end{align*}
where $s_0 > 0$ and $\gamma$ are the same as in Lemma \ref{IPform}. Thus for the coefficients $a_j$ of $A$, we have
\begin{align*}
a_j &= \langle \Psi(-X_j), Q'(X_j) \rangle \\
&= s_0 e^{-2 \alpha X_j}\left( \beta \cos(2 \beta X_j + \phi) - \alpha \sin(2 \beta X_j + \phi)\right) + \mathcal{O}(e^{-(2 \alpha + \gamma) X_j}) \\
&= s_0 e^{\alpha \phi/\beta} r b_j \left( \beta \cos\left( -\frac{\beta}{\alpha} \log(b_j r) \right) - \alpha \sin \left( -\frac{\beta}{\alpha} \log(b_j r) \right) \right) + \mathcal{O}(r^{1+\gamma/2\alpha} b_j^{1 + \gamma/2\alpha}) \\
&= s_0 e^{\alpha \phi/\beta} r b_j \left( \beta \cos\left( -\rho \log(b_j r) \right) - \alpha \sin \left( -\rho \log(b_j r) \right) \right) + \mathcal{O}(r^{1+\gamma/2\alpha})
\end{align*}
since $b_j \in (0, 1]$. Since $r \in \mathcal{R}$, $r = \exp\left(-\frac{m \pi}{\rho}\right)$ for some nonnegative integer $m$, this becomes 
\begin{align*}
a_j &= s_0 e^{\alpha \phi/\beta} r b_j \left( \beta \cos\left( m \pi -\rho \log b_j \right) - \alpha \sin \left( m \pi -\rho \log b_j \right) \right) + \mathcal{O}(r^{1+\gamma/2\alpha}) \\
&= (-1)^m s_0 e^{\alpha \phi/\beta} r b_j \left( \beta \cos\left(-\rho \log b_j \right) - \alpha \sin \left(-\rho \log b_j \right) \right) + \mathcal{O}(r^{1+\gamma/2\alpha})
\end{align*}
To scale $r$ out of this, let $a_j = r \tilde{a}_j$, where
\begin{align*}
\tilde{a}_j 
&= (-1)^m s_0 e^{\alpha \phi/\beta} \left( \beta b_j \cos\left( -\rho \log b_j \right) - \alpha b_j \sin \left( -\rho \log b_j  \right) \right) + \mathcal{O}(r^{\gamma/2\alpha})
\end{align*}
Finally, we solve for $m$ in terms of $r$ to get $(-1)^{-\rho \log r / \pi}$. Substituting this in, we obtain
\begin{align*}
\tilde{a}_j 
&= (-1)^{-\rho \log r / \pi} s_0 e^{\alpha \phi/\beta} \left( \beta b_j \cos\left( -\rho \log b_j \right) - \alpha b_j \sin \left( -\rho \log b_j  \right) \right) + \mathcal{O}(r^{\gamma/2\alpha})
\end{align*}
Thus we have $A = r \tilde{A}$, where $\tilde{A}$ is the same matrix as $A$ with $a_j$ replaced with $\tilde{a}_j$. The matrices $\tilde{A}$ and $A$ are both symmetric, thus the eigenvalues of $\tilde{A}$ are real, and $(1,1,\dots,1)^T$ is an eigenvector of $\tilde{A}$ with eigenvalue 0. Furthermore, $\tilde{\mu}$ is an eigenvalue of $\tilde{A}$ if and only if $\mu = r \tilde{\mu}$ is an eigenvalue of $A$.

Since
\begin{align*}
e^{-\alpha X_m} &= C r^{1/2} \\
e^{-(\alpha - \eta) X_m} &= C r^{\tilde{\gamma}/2}
\end{align*}
where $0 < \tilde{\gamma} < 1$, the remainder terms in Theorem \ref{blockmatrixtheorem} have the bounds given above. In addition, $K_1(\lambda)$ and $K_2(\lambda)$ are perturbations of the nonzero entries of $K(\lambda)$ by $\mathcal{O}(|\lambda| + r^{1/2})$.

Finally, the domain half-length $X$ is given by
\begin{align*}
X &= \sum_{j=0}^{n-1} X_j \\
&= -\sum_{j=0}^{n-1} \frac{1}{2\alpha}\log(b_j r) - \frac{n \phi}{2 \beta}\\
&= -\frac{1}{2\alpha} \log\left( r^n \prod_{j=0}^{n-1} b_j \right) - \frac{n \phi}{2 \beta} \\
&= \frac{1}{2\alpha} (n |\log r| + |\log (b_0\cdots b_{n-1})| ) - \frac{n \phi}{2 \beta}
\end{align*}
\end{proof}
\end{lemma}

\subsection{Characterization of \texorpdfstring{$K(\lambda)$}{K} }

We will first find the interaction eigenvalues, which we expect to occur near where $A - \lambda^2 M I$ is singular. In order to do this, we will need to invert $K(\lambda)$ away from the points where $K(\lambda)$ is singular.

In this section, we characterize the matrix $K(\lambda)$. First, we prove the following general result about the determinant of a periodic, bi-diagonal matrix.

% bidiagonal determinant
\begin{lemma}\label{bidiag}
Let $A$ be the periodic bi-diagonal matrix
\begin{equation}
A = \begin{pmatrix}
a_1 & & & & & & b_n \\
b_1 & a_2 \\
& b_2 & a_3 \\
\vdots & & & \vdots & &&  \vdots \\
& & & & b_{n-2} & a_{n-1} \\
& & & & & b_{n-1} & a_n
\end{pmatrix}
\end{equation}
Then 
\begin{equation}
\det{A} = \prod_{k = 1}^n a_k + (-1)^n \prod_{k = 1}^{n-1} b_k
\end{equation}

\begin{proof}
Expanding by minors using the last column, we have
\begin{align*}
\det A &= a_n \det
\begin{pmatrix}
a_1 \\
b_1 & a_2 \\
& b_2 & a_3 \\
\vdots & & & & \vdots \\
& & & & b_{n-2} & a_{n-1}
\end{pmatrix}
+ (-1)^{n-1} \det
\begin{pmatrix}
b_1 & a_2 \\
& b_2 & a_3 \\
\vdots & & & & \vdots \\
& & & & & b_{n-2} & a_{n-1} \\
& & & & & & b_{n-1}
\end{pmatrix} \\
&= \prod_{k = 1}^n a_k + (-1)^{n-1} \prod_{k = 1}^n b_k
\end{align*}
since both of the matrices on the RHS are triangular.
\end{proof}
\end{lemma}

As a corollary, we can compute the determinant of $K(\lambda)$.
% determinant of K(\lambda)
\begin{corollary}\label{detKcorr}
For the matrix $K(\lambda)$ defined in Theorem \ref{blockmatrixtheorem}, we have 
\begin{equation}\label{detK}
\det K = e^{-\nu(\lambda)X} - e^{\nu(\lambda)X} = -2 \sinh (\nu(\lambda) X)
\end{equation}
where $X = X_0 + X_1 + \dots + X_{n-1}$ is half the length of the periodic domain. $\det K(\lambda) = 0$ if and only if $\nu(\lambda) = i n \pi/X$ for $n \in Z$. 
\begin{proof}
Since $K(\lambda)$ is a periodic, bi-diagonal matrix, by Lemma \ref{bidiag} we have
\begin{align*}
\det K(\lambda) &= \prod_{k = 0}^{n-1} e^{-\nu(\lambda)X_k} + (-1)^{n-1} \prod_{k = 1}^n (-e^{\nu(\lambda)X_k}) \\
&= e^{-\nu(\lambda)(X_0 + X_1 + \dots X_{n-1})} + (-1)^{n-1} (-1)^n e^{\nu(\lambda)(X_0 + X_1 + \dots X_{n-1})} \\
&= e^{-\nu(\lambda)X} - e^{\nu(\lambda)X} \\
&= -2 \sinh (\nu(\lambda)X)
\end{align*}
It follows that $\det K(\lambda) = 0$ if and only if $\nu(\lambda) = i n \pi/X$ for $n \in Z$.
\end{proof}
\end{corollary}

Corollary \ref{detKcorr} tells us that that $K(\lambda)$ is singular if and only if $\nu(\lambda) = i n \pi/X$ for $n \in Z$. In the next lemma, we determine values of $\lambda$ for which $K(\lambda)$ is singular. 

% lambda for which K(lambda) is singular
\begin{lemma}\label{Ksingularlemma}
For sufficiently small $n/X$, $K(\lambda)$ is singular at $\lambda = \pm \lambda^K(X,n)$, where
\begin{equation}\label{lambdaK}
\lambda^K(X,n)
= c \frac{n \pi i }{X} + \mathcal{O}\left( \frac{n}{X} \right)^3
\end{equation} 
$\lambda^K(X,n)$ is purely imaginary, $\lambda^K(X, 0) = 0$, and $\lambda^K(X, -n) = -\lambda^K(X, n)$.
\begin{proof}
To find the values of $\lambda$ where $K(\lambda)$ is singular, we need to solve $\nu(\lambda) = n \pi i/X$ for sufficiently small $n \in \Z$. We know $\nu(0) = 0$, so we only need to do this for nonzero $n$.

Let $G(\lambda, r) = \nu(\lambda) - r$. Then $G(0, 0) = 0$ and $D_\lambda G(0, 0) = \nu'(0) = 1/c$, which is nonzero by Hypothesis \ref{c0nonzero}. Using the IFT, we can solve for $\lambda$ in terms of $r$ for $r$ near 0. In other words, there exists a function $\lambda(r)$ such that $\lambda(0) = 0$ and $G(\lambda(r), r) = 0$ for sufficiently small $r$. Thus, for sufficiently small $r$, $\nu(\lambda(r)) = r$. By Lemma \ref{nulambdalemma}, $\nu(-\lambda(r)) = -\nu(\lambda(r)) = -r$, thus by uniqueness of the IFT solution, $\lambda(-r) = -\lambda(r)$, i.e. $\lambda(r)$ is an odd function. We also have from the IFT 

\begin{align*}
\lambda'(r) &= -\frac{1}{\partial_\lambda G(\lambda(r), r) } \partial_r G(\lambda(r), r) \\
&= \frac{1}{\partial_\lambda \nu(\lambda) } 
\end{align*}
which, at $\lambda = 0$, is $\lambda'(r) = c$. Expanding $\lambda(r)$ in a Taylor series about $r = 0$ and using the fact that $\lambda(r)$ is an odd function, we have
\begin{align*}
\lambda(r) &= \lambda(0) + \lambda'(0) r + \mathcal{O}(|r|^2) \\
&= c r + \mathcal{O}(|r|^3)
\end{align*}

For $n/X$ sufficiently small, take $r = n \pi i / X$. Let 
\[
\lambda^K(X, n) = \lambda\left( \frac{n \pi i}{X} \right)
\]
Then $\nu(\lambda^K(X, n)) = n \pi i / X$, which implies $\det K(\lambda^K(X, n)) = 0$. We have the expansion
\begin{align*}
\lambda^K(X,n)
&= c \frac{n \pi i }{X} + \mathcal{O}(n/X)^3 \\
\end{align*} 

Since $\lambda(r)$ is an odd function, $\lambda^K(X,-n) = -\lambda^K(X,n)$. Finally, since for $\lambda$ pure imaginary, $\nu(\lambda)$ is also pure imaginary, $\lambda(X,n)$ is pure imaginary.
\end{proof}
\end{lemma}

Before we continue, we note the following estimate for the norm of $K(\lambda)$.
\begin{equation}\label{Klambdanorm}
||K(\lambda)|| \leq n ||K(\lambda)||_{\text{max}} = C e^{|\text{Re }\nu(\lambda)|X_{n-1}}
\end{equation}

In the next lemma, we derive an expression for the inverse of $K(\lambda)$ (when it is nonsingular).

% lemma : inverse of K(lambda)
\begin{lemma}\label{Kinvlemma}
When $\det K(\lambda) \neq 0$,
\begin{equation}\label{Klambdainv}
K(\lambda)^{-1} = \frac{1}{\det K(\lambda)} \tilde{K}(\lambda)
\end{equation}
where
\begin{align}\label{tildeK}
\tilde{K}&(\lambda) = \\
&\begin{pmatrix}
e^{-\nu(\lambda)(X_2+\dots+X_{n-1}+X_0)} & e^{-\nu(\lambda)(-X_2-\dots-X_{n-1}-X_0)} &
e^{-\nu(\lambda)(X_2-\dots-X_{n-1}-X_0)} & \dots & e^{-\nu(\lambda)(X_2+\dots+X_{n-1}-X_0)}  \\ 
e^{-\nu(\lambda)(X_3+\dots+X_0-X_1)} & e^{-\nu(\lambda)(X_3+\dots+X_0+X_1)} &
e^{-\nu(\lambda)(-X_3-\dots-X_0-X_1)} & \dots & e^{-\nu(\lambda)(X_3+\dots-X_0-X_1)} \\ 
& \ddots & \ddots \\
e^{-\nu(\lambda)(-X_1-X_2 -\dots-X_{n-1})} & e^{-\nu(\lambda)(X_1-X_2 -\dots-X_{n-1})} &
e^{-\nu(\lambda)(X_1+X_2 -\dots-X_{n-1})} & \dots & e^{-\nu(\lambda)(X_1+X_2+\dots+X_{n-1})}  \nonumber 
\end{pmatrix}
\end{align}
and we have the bound
\begin{equation}\label{Klambdainvnorm}
||K(\lambda)^{-1}|| \leq C \frac{e^{|\text{Re }\nu(\lambda)|X }}{| \det K(\lambda) |}
\end{equation}

\begin{proof}
This can be verified directly. Note that each row is essentially a cyclic permutation of the previous row. Everything is shifted one place to the right, but a different index omitted in each row; row $k$ omits index $k$ (which is taken $\mod n$, so row $n$ omits the index 0). For row $j$, we can verify by matrix multiplication that
\begin{align*}
[K(\lambda)\tilde{K}(\lambda)]_{jj} &= e^{-\nu(\lambda)(X_0 + \dots + X_{n-1})} - e^{\nu(\lambda)(X_0 + \dots + X_{n-1})} = \det K(\lambda) \\
[K(\lambda)\tilde{K}(\lambda)]_{jk} &= 0 && j \neq k
\end{align*}
where $\det(K(\lambda))$ is given in Corollary \ref{detKcorr}. The same holds for the product $\tilde{K}(\lambda)K(\lambda)$.
\end{proof}
\end{lemma}

Before we continue, we recall that we need to make sure the roots of $\det A - \lambda^2 M I$ and $\det K(\lambda)$ do not get too close. To ensure that, we will from now on assume that the $\epsilon-$ball condition holds, which is given in Definition \ref{epsilonballs}. Note that the factor of $1/4$ in the $\epsilon-$ball condition ensures that the $\epsilon-$balls do not overlap. Lemma \ref{epsilonballlemma} which guarantees that the $\epsilon-$ball condition is always satisfied for sufficiently small $r$. We prove that lemma in the next section

\subsection{Proof of Lemma \ref{epsilonballlemma}}

The nonzero roots of $\det A - \lambda^2 M I$ are of the form $\pm r^{1/2} \sqrt{\tilde{\mu}_j/M}$, where $\tilde{\mu}_j$ is one of the nonzero eigenvalues of $\tilde{A}$. These roots come in pairs which are symmetric about the origin. Although we only have to consider the case when these roots are purely imaginary, we do not know in advance which ones these will be, so we consider the worst case scenario where all the roots are purely imaginary. Let $a = \max_j\{| \sqrt{\tilde{\mu}_j/M}|\}$. We will show that for sufficiently small $r$, 
\[
|\lambda^K(X,1)| - a r^{1/2} > \frac{\epsilon}{X}
\]
Since $X > 0$, this is equivalent to
\[
(|\lambda^K(X,1)| - a r^{1/2})X  > \epsilon
\]
This implies that any purely imaginary interaction eigenvalues will be located between 0 and $\pm \lambda^K(X,1)$. Recall from Lemma \ref{Ksingularlemma} that
\[
\lambda^K(X,1)
= c \frac{\pi i }{X} + \mathcal{O}(1/X)^3 
\]
Thus we have
\begin{align*}
(|\lambda^K(X,1)| - a r^{1/2})X &= \left( |c| \frac{\pi}{X} + \mathcal{O}(1/X)^3 - a r^{1/2} \right)X \\
&= |c| \pi + \mathcal{O}(1/X)^2 - a r^{1/2} X \\
&= |c| \pi \left(1 + \mathcal{O}(1/X)^2 - \tilde{a} r^{1/2} X\right)
\end{align*}
where $\tilde{a} = a / |c| \pi$. From Lemma \ref{reparam}, $X$ is given in terms of $r$ and $b_j$ by
\[
\frac{1}{2\alpha} (n |\log r| + |\log (b_0 \cdots b_{n-1})| ) + \tilde{C}
\]
Substituting this in, we have
\begin{align*}
(|&\lambda^K(X,1)| - a r^{1/2})X \\
&= C \left(1 - \tilde{a} r^{1/2}(n |\log r| + |\log (b_0 \cdots b_{n-1})| + \tilde{C}) + \mathcal{O}\left( \frac{1}{n |\log r| + |\log (b_0 \cdots b_{n-1})| + \tilde{C}} \right) \right)
\end{align*}
where $C$ is a positive constant. As $r \rightarrow 0$, $b_j \rightarrow b_j^0$, which are constants. Thus as $r \rightarrow 0$, the quantity in parentheses on the RHS approaches 1. 

Thus there exists $r(b^0) > 0$ such that for $r \in \mathcal{R}$ with $r \leq r(b^0)$, $(|\lambda^K(X,1)| - a r^{1/2})X > r^{1/4}$. Since $\epsilon = r^{1/4}/X$, the result follows upon multiplication by $X$.

\subsection{Bounds on \texorpdfstring{$K(\lambda)^{-1}$}{inverse of K} }

In this section, we obtain a bound for $K(\lambda)^{-1}$. This bound hold as long as we are sufficiently far from the points where $K(\lambda)$ is singular, which we guarantee by taking the $\epsilon-$ball condition. First, we obtain lower bounds for $\det K(\lambda)$. 

% bounds for Det K
\begin{lemma}\label{detKlemma}
We have the following lower bounds for $\det K(\lambda)$.
\begin{enumerate}[(i)]
\item If $|\text{Re }\nu(\lambda)| \geq 1/X$, 
\begin{equation}\label{detKbound1}
|\det K(\lambda)| \geq \sqrt{2} e^{|\text{Re }\nu(\lambda)X|}
\end{equation}
\item If the $\epsilon-$ball condition holds and $|\lambda| \geq C r^{1/2}$,
\begin{equation}\label{detKbound2}
|\det K(\lambda)|\geq C \min\{ X r^{1/2}, r^{1/4} \}
\end{equation}
\end{enumerate}

\begin{proof}
First, we take the case (i) where $|\text{Re }\nu(\lambda)| \geq 1/X$. For convenience, let $\nu(\lambda)X = a + bi$, so $|a| \geq 1$. Expanding $|\sinh(a + b i)|^2$ gives us
\begin{align*}
|\sinh(a + b i)|^2 
&= |\sinh a \cosh b i + \cosh a \sin b i|^2 \\
&= |\sinh a \cos b + i \cosh a \sin b |^2 \\
&= \sinh^2 a \cos^2 b + \cosh^2 a \sin^2 b \\
&= \sinh^2 a \cos^2 b + \sinh^2 a \sin^2 b 
+ \cosh^2 a \sin^2 b - \sinh^2 a \sin^2 b \\
&= \sinh^2 a (\cos^2 b + \sin^2 b) 
+ \sin^2 b( \cosh^2 a - \sinh^2 a) \\
&= \sinh^2 a + \sin^2 b
\end{align*}
In this case,
\begin{align*}
|\sinh(\nu(\lambda) X)|^2 &= |\sinh(a + b i)|^2 \\
&\geq \sinh^2 a \\
&= \frac{1}{4}\left( e^{2a} + e^{-2a} - 2 \right) \\
&\geq \frac{1}{4}\left( e^{2|a|} - 2 \right)
\end{align*}
For $|a| \geq 1$, we have
\begin{align*}
e^{2|a|} - 2 - \frac{e^{2|a|}}{2} 
&= \frac{e^{2|a|}}{2} - 2 \\
&= \frac{e^2}{2} - 2 > 0
\end{align*}
Thus $e^{2|a|} - 2 \geq \frac{e^{2|a|}}{2}$ and we conclude
\begin{align*}
|\sinh(\nu(\lambda) X)|^2 \geq \frac{e^{2|a|}}{2} \\
|\sinh(\nu(\lambda) X)| \geq \frac{e^{|a|}}{\sqrt{2}}
\end{align*}
from which it follows that if $|\text{Re } \nu(\lambda)| \geq 1/X$,
\begin{align*}
|\det K(\lambda)| &= 2 |\sinh(\nu(\lambda) X)|
\geq \sqrt{2} e^{|\text{Re }\nu(\lambda)X|}
\end{align*}

Next, we consider the case (ii). Assume the $\epsilon-$ball condition holds. There are two cases to consider. First, suppose $|\lambda| \leq \lambda^K(X,1)/2$. Since $\epsilon \leq 1/4X$, the $\epsilon$ criterion is automatically satisfied. Expanding $\sinh( \nu(\lambda) X)$ in a Taylor series about $\lambda = 0$, and recalling that $\nu(0) = 0$, we get
\begin{align*}
\sinh(\nu(\lambda + \xi) X) &= \frac{1}{c}X\xi + \mathcal{O}((X \xi)^3)
\end{align*}
Since $|\lambda| \geq C r^{1/2}$, we have $C_1 r^{1/2} < \xi < |c| \pi C_2/X$, thus for this case,
\begin{align*}
|\det K(\lambda)| &= 2 |\sinh(\nu(\lambda) X)| \\
& \geq C r^{1/2}X
\end{align*}

For the second case suppose $|\lambda| \geq \lambda^K(X,1)/2$. Since the points $\lambda^K(X,k)$ are spaced apart on the imaginary axis by approximtely $k \pi/X$, and $\epsilon \leq 1/4X$, there is enough room between the points for us to work. We want this bound to hold for when $|\text{Re } \nu(\lambda)| < 1/X$. 

Once again, let $\nu(\lambda) X = a + b i$. Then, using the same expansion for $|\sinh(a + b i)|^2$ as above, we have
\begin{align*}
|\sinh(\nu(\lambda) X)|^2 
&= \sinh^2 a + \sin^2 b \\
&\geq \sin^2 b \\
&= \sin^2 (\text{Im }\nu(\lambda)X)
\end{align*}
from which it follows that
\begin{align*}
|\det K(\lambda)| = 2 |\sinh(\nu(\lambda) X)| \geq 2|\sin(\text{Im }\nu(\lambda)X)|
\end{align*}

For $k/X$ suffienctly small, $\lambda^K(X, k)$ is defined by Lemma \ref{Ksingularlemma}. Since $\nu(\lambda)$ is smooth in $\lambda$, from Lemma \ref{nulambdalemma} we have the Taylor series expansion for $\nu'(\lambda)$
\[
\nu'(\lambda) = \frac{1}{c} + \mathcal{O}(|\lambda|^2)
\]
Expand $\nu(\lambda)$ in a Taylor series about $\lambda^K(X, k)$ to get
\begin{align*}
\nu\left( \lambda^K(X, k) + \xi \right) &= \nu( \lambda(X, k) ) + \nu'(\lambda(X, k)\xi
+ \mathcal{O}(\xi^2) \\
&= \frac{k \pi i}{X} + \left( \frac{1}{c} + \mathcal{O}(|\lambda(X, k)|^2) \right) \xi
+ \mathcal{O}(\xi^2) \\
&= \frac{k \pi i}{X} \frac{1}{c}\xi \left( 1 + \mathcal{O} \left(\frac{k}{X}\right)^2 \right) + \mathcal{O}(\xi^2)
\end{align*}
Since the $\epsilon-$ball condition holds,
\[
\text{Im } \xi \geq \epsilon = \frac{r^{1/4}}{X}
\]
Thus as long as $\xi$ is not so large that we are closer to a different $\lambda^K(X, k)$,
\begin{align*}
|\text{Im } \nu\left( \lambda^K(X, k) + \xi \right)X - k \pi| \geq C r^{1/4}
\end{align*}
from which it follows that
\[
|\det K(\lambda)| \geq C \min\{ X r^{1/2}, r^{1/4} \}
\]
\end{proof}
\end{lemma}

In the final lemma of this section, we obtain bounds on $K(\lambda)^{-1}$ as well as the products $K_1(\lambda)K(\lambda)^{-1}$ and $K_2(\lambda)K(\lambda)^{-1}$. Note that the lemma requires us to take $|\lambda| \geq C r^{1/2}$. This will not turn out to be a problem, since we will use this lemma to locate a finite number of interaction eigenvalues, all of which are of order $\mathcal{O}(r^{1/2})$.

% lemma : bounds on K
\begin{lemma}\label{Kinvboundslemma}
Choose $\lambda \in \C$ such that $|\lambda| \geq C r^{1/2}$ and the $\epsilon-$ball condition holds. Then we have the following bounds.
\begin{enumerate}[(i)]
\item 
\begin{equation}\label{Kinvbound}
||K(\lambda)^{-1}|| \leq C \max \left\{ r^{-1/4}, \frac{r^{-1/2}}{X} \right\} \\
\end{equation}
\item 
\begin{align}
||K_1(\lambda)K(\lambda)^{-1}|| &\leq C (|\lambda| + r^{1/2}) \max \left\{ r^{-1/4}, \frac{r^{-1/2}}{X} \right\} \label{K1Kinvbound} \\
||K_2(\lambda)K(\lambda)^{-1}|| &\leq C (|\lambda| + r^{1/2}) \max \left\{ r^{-1/4}, \frac{r^{-1/2}}{X} \right\} \label{K2Kinvbound}
\end{align}
\end{enumerate}

\begin{proof}
For the bound on $|K(\lambda)^{-1}|$, if $\text{Re }\nu(\lambda)|X \geq 1/X$, we have from Lemma \ref{detKlemma}
\begin{align*}
||K(\lambda)^{-1}|| &\leq C \frac{e^{|\text{Re }\nu(\lambda)|X }}{| \det K(\lambda) |} \leq C \frac{e^{|\text{Re }\nu(\lambda)|X }}{e^{|\text{Re }\nu(\lambda)X|}} = C 
\end{align*}

If $\text{Re }\nu(\lambda)|X \leq 1/X$, then we have from Lemma \ref{detKlemma}
\begin{align*}
||K(\lambda)^{-1}|| &\leq C \frac{e^{|\text{Re }\nu(\lambda)|X }}{| \det K(\lambda) |} \\
& \leq C \frac{1}{\min \{ r^{1/4}, X r^{1/2} \}} \\
& \leq C \max \left\{ r^{-1/4}, \frac{r^{-1/2}}{X} \right\}
\end{align*}
This gives us the overall bound
\begin{align*}
||K(\lambda)^{-1}|| &\leq C \max \left\{ r^{-1/4}, \frac{r^{-1/2}}{X} \right\}
\end{align*}

For $K_1(\lambda)K(\lambda)^{-1}$, using the equation for $K(\lambda)^{-1}$ from Lemma \ref{Kinvlemma} and the form of $K_1(\lambda)$ from the proof of Lemma \ref{jumpcenteradj},
\begin{align*}
||K_1(\lambda)K(\lambda)^{-1}|| &\leq 
C \frac{e^{|\text{Re }\nu(\lambda)|X}}{|\det K(\lambda)|} \max {|\gamma|_{ij}} \\
&\leq C \max \left\{ r^{-1/4}, \frac{r^{-1/2}}{X} \right\} \max {|\gamma|_{ij}}
\end{align*}
where we used the bounds we obtained from the first part of this lemma for the two cases where $\text{Re }\nu(\lambda)|X \geq 1/X$ and $\text{Re }\nu(\lambda)|X \leq 1/X$. The constants $\gamma_{ij}$ are the coefficients of $K_1(\lambda)$ from the proof of Lemma \ref{jumpcenteradj}. Since $\gamma_{ij} = \mathcal{O}(|\lambda| + r^{\tilde{\gamma}/2})$, we conclude
\begin{align*}
||K_1(\lambda)K(\lambda)^{-1}|| &\leq C (|\lambda| + r^{\tilde{\gamma}/2}) \max \left\{ r^{-1/4}, \frac{r^{-1/2}}{X} \right\}
\end{align*}
Since $K_2(\lambda)$ is a similar perturbation of $K(\lambda)$, we have a similar bound for $||K_2(\lambda)K(\lambda)^{-1}||$.
\end{proof}
\end{lemma}

\subsection{Interaction Eigenvalues}

In this section, we find the interaction eigenvalues, which come from the second line of the block matrix equation from Theorem \ref{blockmatrixtheorem}. To do this, we solve the first line of the block matrix equation \eqref{blockeq} for $c$ and plug it into the second line of the block matrix. We expect the interaction eigenvalues to be close to the points where $A - \lambda^2 M I$ is singular. From Lemma \ref{reparam}, $A = r \tilde{A}$, thus it makes sense to take the scaling
\[
\lambda = r^{1/2}\tilde{\lambda}
\]

In the following lemma, we derive an equation we can solve to find the interaction eigenvalues.

% equation for d
\begin{lemma}\label{deqlemma}
For sufficiently small $r$, the interaction eigenvalues are given by $\lambda = r^{1/2} \tilde{\lambda}$, where the $\tilde{\lambda}$ are the values for which the equation
\begin{equation}\label{eqford}
(\tilde{A} - \tilde{\lambda}^2 MI + \tilde{D}_3)d = 0
\end{equation}
has a nontrivial solution. The remainder term $\tilde{D}_3$ has bound
\begin{equation}\label{tildeD3bound}
||\tilde{D}_3|| \leq C r^{1/2}
\end{equation}

\begin{proof}
First, we solve the top line of the block matrix equation \eqref{blockeq} for $c$. For $\lambda \neq \lambda^K(X, n)$, $K(\lambda)$ is invertible, and we can write the top line of \eqref{blockeq} as
\begin{align*}
(I + C_1 + K_1(\lambda)K(\lambda)^{-1}) K(\lambda) c = -D_1 d
\end{align*}
Using the scaling $\lambda = \mathcal{O}(r^{1/2})$ and the bounds from Lemma \ref{Kinvboundslemma}
\begin{align*}
||K_1(\lambda)K(\lambda)^{-1}|| &\leq C (|\lambda| + r^{1/2})\max \left\{ r^{-1/4}, \frac{r^{-1/2}}{X} \right\} \\
&\leq C \max \left\{ r^{1/4}, \frac{1}{X} \right\} \\
&\leq C \max \left\{ r^{1/4}, \frac{1}{n |\log r| + |\log (b_0 \cdots b_{n-1})| ) } \right\}
\end{align*}
where we use the expression for $X$ from Lemma \ref{reparam}. Combining this with the bound for $C_1$ (which is stronger, so is subsumed by the bound on $||K_1(\lambda)K(\lambda)^{-1}||$), we have the bound
\[
||C_1 + K_1(\lambda)K(\lambda)^{-1}|| \leq C \max \left\{ r^{1/4}, \frac{1}{n |\log r| + |\log (b_0 \cdots b_{n-1})| } \right\}
\]
Since the RHS goes to 0 as $r \rightarrow 0$, $I + C_1 + K_1(\lambda)K(\lambda)^{-1}$ is invertible for sufficiently small $r$. Let $C_3 = (I + C_1 + K_1(\lambda)K(\lambda)^{-1})^{-1}$. Then we can solve for $c$ to get
\[
c = -K(\lambda)^{-1} C_3 D_1 d
\]

Plugging this into the second line of \eqref{blockeq}, we get the folowing equation for $d$.
\begin{align*}
(C_2 K(\lambda) + K_2(\lambda))c + (r\tilde{A} - r \tilde{\lambda}^2 MI + D_2)d &= 0 \\
(r\tilde{A} - r \tilde{\lambda}^2 MI + D_2)d - (C_2 K(\lambda) + K_2(\lambda))K(\lambda)^{-1} C_3 D_1 &= 0 \\
(r\tilde{A} - r \tilde{\lambda}^2 MI + D_2)d - (C_2 + K_2(\lambda)K(\lambda)^{-1}) C_3 D_1 &= 0 \\
\end{align*}
Let 
\[
D_3 = D_2 - C_2 C_3 D_1 - K_2(\lambda) K(\lambda)^{-1} C_3 D_1
\]
be the remainder term, so that the equation for $d$ becomes
\[
(r\tilde{A} - r \tilde{\lambda}^2 MI + D_3)d = 0
\]
Using bounds from Lemma \ref{reparam} and Lemma \ref{Kinvboundslemma} together with the scaling $\lambda = \mathcal{O}(r^{1/2})$, we have the following bound for $D_3$
\begin{align*}
||D_3|| &\leq C \left( (|\lambda| + r^{1/2})^3 + r^{\tilde{\gamma}/2}(|\lambda| + r^{1/2})^4 + (|\lambda| + r^{1/2})^3 \right) \\
&\leq C r^{3/2}
\end{align*}
Dividing by $r$, we get the equation 
\[
(\tilde{A} - \tilde{\lambda}^2 MI + \tilde{D}_3)d = 0
\]
where 
\[
||\tilde{D}_3|| \leq C r^{1/2}
\]
\end{proof}
\end{lemma}

Because of our scaling, the parameter $r$ only occurs in the remainder term. If we assume Hypothesis \ref{Adistincteigs}, we can solve \eqref{eqford} for the interaction eigenvalues.

% solve for int eigs
\begin{lemma}\label{inteigslemma}
Assume Hypothesis \ref{Adistincteigs}, and let $0, \tilde{\mu}_1, \dots, \tilde{\mu}_{n-1}$ be the eigenvalues of $\tilde{A}$. Then there exists $r_1 \leq r_0$ such that for $r \in \mathcal{R}$ with $r \leq r_1$, we have $2n - 2$ pairs of interaction eigenvalues
\begin{align*}
\lambda &= \pm \lambda^{\text{int}}_j(r) && j = 0, \dots, n-2
\end{align*}
where
\begin{align}\label{inteigsformula}
\lambda^{\text{int}}_j(r) = r^{1/2} \sqrt{\tilde{\mu}_j / M} + \mathcal{O}(r^{3/4})
\end{align}
These interaction eigenvalue pairs are either real or purely imaginary, and the remainder term cannot move them off of the real or imaginary axis.

\begin{proof}
Let $\{0, \mu_1, \dots, \mu_{n-1}\}$ be the eigenvalues of $A$, which we are distinct by Hypothesis \ref{Adistincteigs}. Since $A = r \tilde{A}$, the eigenvalues of $\tilde{A}$ are $\{0, \tilde{\mu}_1, \dots, \tilde{\mu}_{n-1}\}$, which are also distinct, and $\mu_j = r \tilde{\mu}_j$.\\

Equation \eqref{eqford} has a nontrivial solution if and only if
\[
\tilde{E}(\tilde{\lambda}, r) = \det
\left( \tilde{A} - \tilde{\lambda}^2 MI + \mathcal{O}(r^{1/2}) \right) = 0
\]
When $r = 0$, $\tilde{E}(\tilde{\lambda}, 0) = \det(\tilde{A} - \tilde{\lambda}^2 MI)$, which we can evaluate by taking $\mu = \tilde{\lambda}^2 M$ in the characteristic polynomial of $\tilde{A}$.
\begin{equation}\label{tildeE1}
\tilde{E}(\tilde{\lambda}, 0) = \tilde{\lambda}^2
\left( \tilde{\lambda} - \sqrt{\tilde{\mu}_1 / M} \right)
\left( \tilde{\lambda} + \sqrt{\tilde{\mu}_1 / M} \right) \dots
\left( \tilde{\lambda} - \sqrt{\tilde{\mu}_{n-1} / M} \right)
\left( \tilde{\lambda} + \sqrt{\tilde{\mu}_{n-1} / M} \right)
\end{equation}

For $j = 1, \dots, n-1$, $\tilde{E}(\pm \sqrt{\tilde{\mu}_j / M}, 0) = 0$. Since the eigenvalues of $A_0$ are distinct, $\partial_{\tilde{\lambda}} \tilde{E}(\pm \sqrt{\tilde{\mu}_j / M}, 0) \neq 0$. Thus there exists $r_1 \leq r_0$ so that for $r \leq r_1$, we can use the IFT to solve for $\tilde{\lambda}$ in terms of $r$ near the $2n-2$ roots $\pm \sqrt{\tilde{\mu}_j / M}$ of \eqref{tildeE1}. In other words, for $r \leq r_1$, there are unique smooth functions $\tilde{\lambda}_j^\pm(r)$ such that $\tilde{\lambda}_j^\pm(0) = \pm \sqrt{\tilde{\mu}_j / M}$ and $\tilde{E}(\tilde{\lambda}_j^\pm(r); r) = 0$. We also have the estimate
\[
\tilde{\lambda}_j^\pm(r) = \pm \sqrt{\tilde{\mu}_j/ M} + \mathcal{O}(r^{1/4})
\]

Undoing the scaling, let
\[
\lambda_j^\pm(r) = r^{1/2} \tilde{\lambda}_j^\pm(r)
\]
These are the interaction eigenvalues we seek. By Hamiltonian symmetry, eigenvalues must come in quartets $\pm a \pm b i$. Since  the eigenvalues $\tilde{\mu}$ of $\tilde{A}$ are distinct, the only way we can satisfy Hamiltonian symmetry is if $\lambda_j^+(r) = \lambda_j^-(r)$, in which case the eigenvalue pairs must be real or purely imaginary. Thus the interaction eigenvalues are given by $\lambda = \pm \lambda^{\text{int}}_j(r)$, where
\begin{align*}
\lambda^{\text{int}}_j(r) = r^{1/2} \sqrt{\tilde{\mu}_j / M} + \mathcal{O}(r^{3/4})
\end{align*}
By Hamiltonian symmetry, the remainder term cannot move these off of the real or imaginary axis.
\end{proof}
\end{lemma}

\subsection{Characterization of \texorpdfstring{$A - \lambda^2 MI$}{Matrix A} }

We can now find the ``essential spectrum'' eigenvalues, which we expect to occur near the points where $K(\lambda)$ is singular. In order to do this, we will need to invert $A - \lambda^2 MI$ away from the points where it is singular.

As in the previous section, we will assume that the $\epsilon-$ball condition holds. In addition, since we expect that the smallest nonzero ``essential spectrum'' eigenvalues will occur at approximately $\lambda = \pm c \pi i / X$, we will restrict ourselves to $\lambda$ with $|\lambda| \geq C/X$.

We first prove a lower bound the determinant of $(A - \lambda^2 MI)$.

% lemma : bound on det (A - \lambda^2 MI)
\begin{lemma}\label{detAboundlemma}
We have the following lower bounds for $\det(A - \lambda^2 M I)$.
\begin{enumerate}[(i)]
\item If $|\lambda| \geq C/X$ and the $\epsilon-$ball condition holds,
\begin{equation}\label{detAbound1}
|\det(A - \lambda^2 M I)|
\geq C \frac{1}{X^2} \left( \frac{r^{1/4}}{X} \right)^{n-1} \left( |\lambda|^2 + r \right)^{(n-1)/2}
\end{equation}

\item If $|\lambda| \geq 2 r^{1/2} \sqrt{\tilde{\mu}_M/M}$, where $\tilde{\mu}_M = \max\{\tilde{\mu}_1, \dots, \tilde{\mu}_{n-1} \}$, then
\begin{equation}\label{detAbound2}
|\det(A - \lambda^2 M I)|
\geq C |\lambda|^{n+1} \left( |\lambda|^2 + r \right)^{(n-1)/2}
\end{equation}

\end{enumerate}
\begin{proof}
$\det(A - \lambda^2 MI)$ is the characteristic polynomial $p(t)$ of $A$ with $t = \lambda^2 / M$. Since the eigenvalues of $A$ are $\{0, r \tilde{\mu}_1, \dots, r\tilde{\mu}_{n-1}\}$, the roots of $\det(A - \lambda^2 MI)$ are 
\[
\{0, \pm r^{1/2} \sqrt{\tilde{\mu}_1/M}, \dots, \pm r^{1/2} \sqrt{\tilde{\mu}_{n-1}/M}\}
\]
where the root at 0 has algebraic multiplicty 2. Thus we have
\begin{align}\label{detAlambda}
\det(A &- \lambda^2 M I) \\ 
&= C \lambda^2 (\lambda - r^{1/2} \sqrt{\tilde{\mu_1}/M} )(\lambda + r^{1/2} \sqrt{\tilde{\mu}_1/M} )
\dots(\lambda - r^{1/2} \sqrt{\tilde{\mu}_{n-1}/M})(\lambda + r^{1/2} \sqrt{\tilde{\mu}_{n-1}/M} ) \nonumber
\end{align}

For the bound (i), suppose $|\lambda| \geq C/X$ and that the $\epsilon-$ball condition holds. For the $\lambda^2$ term in \eqref{detAlambda}, we use the lower bound $|\lambda| \geq C/X$. Since the pairs $\pm \mu_j$ are symmetric across the origin, we have the lower bound for each pair
\begin{align*}
(\lambda - r^{1/2} \sqrt{\tilde{\mu}/M} )(\lambda + r^{1/2} \sqrt{\tilde{\mu}_1/M} )
&\geq \epsilon \sqrt{ |\lambda|^2 + |r^{1/2} \sqrt{\tilde{\mu}_1/M}|^2 } \\
&\geq C \frac{r^{1/4}}{X} \sqrt{ |\lambda|^2 + r } \\
\end{align*}
Combining these, we have the bound
\begin{align*}
|\det(A - \lambda^2 M I)|
&\geq C \frac{1}{X^2} \left( \frac{r^{1/4}}{X} \right)^{n-1} \left( |\lambda|^2 + r \right)^{(n-1)/2} \\
\end{align*}

For bound (ii), suppose $|\lambda| \geq 2 r^{1/2} \sqrt{\tilde{\mu}_M/M}$. For the pairs $\pm \mu_j$ in \eqref{detAlambda}, we have the lower bound
\begin{align*}
(\lambda - r^{1/2} \sqrt{\tilde{\mu}/M} )(\lambda + r^{1/2} \sqrt{\tilde{\mu}_1/M} )
&\geq \frac{|\lambda|}{2} \sqrt{ |\lambda|^2 + |r^{1/2} \sqrt{\tilde{\mu}_1/M}|^2 } \\
&\geq C |\lambda| \sqrt{ |\lambda|^2 + r } \\
\end{align*}
Combining these, we have the bound
\[
|\det(A - \lambda^2 M I)|
\geq C |\lambda|^{n+1} \left( |\lambda|^2 + r \right)^{(n-1)/2}
\]
\end{proof}
\end{lemma}

In the next lemma, we derive a bound for $(A - \lambda^2 M I)^{-1}$.

% bound on (A - \lambda^2 M I)^{-1}
\begin{lemma}\label{Ainvboundlemma}
Choose $\lambda \in \C$ such that $|\lambda| \geq C/X$ and the $\epsilon-$ball condition holds. Then we have the following bounds for $(A - \lambda^2 M I)^{-1}$.
\begin{enumerate}[(i)]
\item If $|\lambda| \leq 2 r^{1/2} \sqrt{\tilde{\mu}_M/M}$, where $\tilde{\mu}_M = \max\{\tilde{\mu}_1, \dots, \tilde{\mu}_{n-1}\}$, then
\begin{align}\label{Ainvbound1}
||(A - \lambda^2 M I)^{-1}|| &\leq C X ^{n+1} r^{(n-1)/4}
\end{align}

\item If $|\lambda| \geq 2 r^{1/2} \sqrt{\tilde{\mu}_M/M}$, then
\begin{equation}\label{Ainvbound2}
||(A - \lambda^2 M I)^{-1}|| \leq \frac{C}{|\lambda|^2}
\end{equation}
\end{enumerate}
\begin{proof}
When it is nonsingular, the inverse $(A - \lambda^2 M I)^{-1}$ is given by the formula
\[
(A - \lambda^2 M I)^{-1} = \frac{1}{\det(A - \lambda^2 M I)}\text{Adj}(A - \lambda^2 M I)
\]
where $\text{Adj}(A - \lambda^2 M I)$ is the adjugate matrix (the transpose of the cofactor matrix) corresponding to $A - \lambda^2 M I$. Since $A$ is $n \times n$, each entry in $\text{Adj}(A - \lambda^2 M I)$ involves sums of products of $n-1$ of the entries of $A - \lambda^2 M I$, each of which is $\mathcal{O}(r + |\lambda|^2)$, thus $||\text{Adj}(A - \lambda^2 M I)|| = \mathcal{O}(r + |\lambda|^2)^{n-1}$.

For the bound (i), we use lower bound \eqref{detAbound1} on $\det(A - \lambda^2 M I)$ from Lemma \ref{detAboundlemma} to get
\begin{align*}
||(A - \lambda^2 M I)^{-1}|| &\leq C X^2 \left(\frac{X}{r^{1/4}}\right)^{n-1} 
\frac{\left( |\lambda|^2 + r \right)^{n-1}}{\left( |\lambda|^2 + r \right)^{(n-1)/2}} \\
&= C X^2 X^{n - 1} r^{-(n-1)/4}\left( |\lambda|^2 + r \right)^{(n-1)/2} \\
&\leq C X ^{n+1} r^{(n-1)/4}
\end{align*}
where the last inequality holds since $|\lambda| \leq C r^{1/2}$.

For the bound (ii), we have $|\lambda| \geq C r^{1/2}$, and we use the lower bound \eqref{detAbound2} on $\det(A - \lambda^2 M I)$ from Lemma \ref{detAboundlemma} to get
\begin{align*}
||(A - \lambda^2 M I)^{-1}|| &\leq C \frac{1}{|\lambda|^{n+1}} 
\frac{\left( |\lambda|^2 + r \right)^{n-1}}{\left( |\lambda|^2 + r \right)^{(n-1)/2}} \\
&= C \frac{1}{|\lambda|^{n+1}} \left( |\lambda|^2 + r \right)^{(n-1)/2} \\
&\leq C \frac{1}{|\lambda|^{n+1}} |\lambda|^{n-1} \\
&=\frac{C}{|\lambda|^2}
\end{align*}
\end{proof}
\end{lemma}

\subsection{``Essential spectrum'' eigenvalues}

To find the ``essential spectrum'' eigenvalues, we solve the second line of of the block matrix equation \eqref{blockeq} for $d$ and plug it into the first line. In the next lemma, we derive an equation we can solve to obtain the ``essential spectrum'' eigenvalues.

% lemma : equation for c
\begin{lemma}\label{ceqlemma}
Let $\lambda \in \C$ such that $|\lambda| \geq C/X$ and the $\epsilon-$ball condition holds. Then sufficiently small $r$, the ``essential spectrum'' eigenvalues are the values of $\lambda$ for which 
\begin{align}\label{eqforc}
(K(\lambda) + C_5 K_3(\lambda)c &= 0
\end{align}
has a nontrivial solution, where $K_3(\lambda)$ has the same form as $K_1(\lambda)$, and
\begin{align*}
||C_5|| &\leq C
\end{align*}

\begin{proof}
As long as $\lambda$ is not one of the $2n - 1$ points $\{0, \pm \sqrt{\mu_1/M}, \dots, \pm \sqrt{\mu_{n-1}/M}$ where $A - \lambda^2 MI$ is singular, we can invert $A - \lambda^2 MI$ and write the bottom line of \eqref{blockeq} as 
\begin{align}\label{blockeqbottom}
(C_2 K(\lambda) + K_2(\lambda))c 
+ (A - \lambda^2 MI)(I + (A - \lambda^2 MI)^{-1} D_2))d = 0
\end{align}

To continue, we need to bound $(A - \lambda^2 MI)^{-1} D_2$, which we will do for the two cases in Lemma \ref{Ainvboundlemma}. For $|\lambda| \geq 2 r^{1/2} \sqrt{\tilde{\mu}_M/M}$, we have
\begin{align*}
|| (A - \lambda^2 MI)^{-1} D_2 || &\leq \frac{C}{|\lambda|^2} (|\lambda| + r^{1/2})^3 \\ 
&\leq C |\lambda|
\end{align*}
Since $|\lambda| < \delta$, for sufficiently small $\delta$ this will be less than 1. 

For $|\lambda| \leq 2 r^{1/2} \sqrt{\tilde{\mu}_M/M}$,
\begin{align*}
|| (A - \lambda^2 MI)^{-1} D_2 || &\leq C X^{n+1}\left( |\lambda|^2 + r \right)^{(n-1)/4} (|\lambda| + r^{1/2})^3 \\
&\leq C X^{n+1} r^{(n-1)/4} r^{3/2} \\ 
&= C X^{n+1} r^{(n+1)/4} r \\
&= C r (r^{1/4} X)^{n+1}
\end{align*}
Using the expression for $X$ from Lemma \ref{reparam}, this becomes
\begin{align*}
|| (A - \lambda^2 MI)^{-1} D_2 ||
&\leq C r \left( r^{1/4} (n |\log r| + C_b )\right)^{n+1}
\end{align*}
Since $r^{1/4} |\log r| \rightarrow 0$ as $r \rightarrow 0$, we can find $r_2 \leq r_1$ such that for $r \in \mathcal{R}$ with $\leq r_2$, $|| (A - \lambda^2 MI)^{-1} D_2 || < 1$. 

Thus for both cases, $(I + (A - \lambda^2 MI)^{-1} D_2)$ is invertible. Let $D_3 = (I + (A - \lambda^2 MI)^{-1} D_2)^{-1}$. Then we can solve \eqref{blockeqbottom} for $d$ to get
\begin{align*}
d &= -(A - \lambda^2 MI)^{-1} D_3 (C_2 K(\lambda) + K_2(\lambda))c
\end{align*}

Plugging this in for $c$ in the first line of the block matrix equation, we get
\begin{align*}
(K(\lambda) + C_1 K(\lambda) + K_1(\lambda))c + D_1 d &= 0 \\
(K(\lambda) + C_1 K(\lambda) + K_1(\lambda))c - D_1 (A - \lambda^2 MI)^{-1} D_3 (C_2 K(\lambda) + K_2(\lambda))c &= 0 \\
(K(\lambda) + C_1 K(\lambda) + K_1(\lambda) - D_1 (A - \lambda^2 MI)^{-1} D_3 C_2 K(\lambda) - D_1 (A - \lambda^2 MI)^{-1} D_3 K_2(\lambda))c &= 0 \\
(I + C_1 - D_1 (A - \lambda^2 MI)^{-1} D_3 C_2) K(\lambda)c + (K_1(\lambda) - D_1 (A - \lambda^2 MI)^{-1} D_3 K_2(\lambda))c &= 0
\end{align*}

Next, we will obtain bounds on the terms in the above equation. For the term $D_1 (A - \lambda^2 MI)^{-1}$, we consider the same two cases we did above. For $|\lambda| \geq 2 r^{1/2} \sqrt{\tilde{\mu}_M/M}$, we have
\begin{align*}
|| D_1 (A - \lambda^2 MI)^{-1} || &\leq \frac{C}{|\lambda|^2} (|\lambda| + r^{1/2})^2 \\ 
&\leq C
\end{align*}
For $|\lambda| \leq 2 r^{1/2} \sqrt{\tilde{\mu}_M/M}$, we have
\begin{align*}
|| D_1 (A - \lambda^2 MI)^{-1} || &\leq C r^{1/2} \left( r^{1/4} (n |\log r| + |\log(b_0\cdots b_{n-1})| \right)^{n+1}
\end{align*}
If necessary, decrease $r_2$ so that $X r^{(2 \tilde{\gamma} - 1)/4} \leq 1$. Thus for $r \leq r_2$ we will always have $|| D_1 (A - \lambda^2 MI)^{-1} || \leq C$.

We can now bound $C_1 - D_1 (A - \lambda^2 MI)^{-1} D_3 C_2$ to get
\begin{align*}
|| C_1 - D_1 (A - \lambda^2 MI)^{-1} D_3 C_2 || &\leq C \left( r^{\tilde{\gamma}/2} (|\lambda| + r^{1/2}) + r^{\tilde{\gamma}/2} (|\lambda| + r^{1/2})^2 \right) \\
&\leq C r^{\tilde{\gamma}/2} (|\lambda| + r^{1/2}) 
\end{align*}

Since $|\lambda| < \delta$, for sufficiently small $\delta$ and $r \leq r_2$ (again, decreasing $r_2$ if necessary), $|| C_1 - D_1 (A - \lambda^2 MI)^{-1} D_3 C_2 || < 1$, thus $I + C_1 - D_1 (A - \lambda^2 MI)^{-1} D_3 C_2$ is invertible. Let $C_4 = (I + C_1 - D_1 (A - \lambda^2 MI)^{-1} D_3 C_2)^{-1}$, where we have the bound $||C_4|| \leq C$. Then our equation becomes
\begin{align*}
K(\lambda)c + C_4(K_1(\lambda) - D_1 (A - \lambda^2 MI)^{-1} D_3 K_2(\lambda))c &= 0
\end{align*}

Let $D_4 = -C_4 D_1 (A - \lambda^2 MI)^{-1} D_3$. This has bound
\begin{align*}
||D_4|| &\leq C (|\lambda| + r^{1/2})^3
\end{align*}
Substituting this in, our equation becomes
\begin{align*}
(K(\lambda) + C_4 K_1(\lambda) + D_4 K_2(\lambda))c &= 0
\end{align*}

Finally, we note that since $K_1(\lambda)$ and $K_2(\lambda)$ have the same form, and $C_4$ is a weaker bound than $D_4$, we can write this as
\begin{align*}
(K(\lambda) + C_5 K_3(\lambda))c &= 0
\end{align*}
where $K_3(\lambda)$ has the same form as $K_1(\lambda)$ and $||C_5|| \leq C$.
\end{proof}
\end{lemma}

In the next lemma, we find the ``essential spectrum'' eigenvalues.
% find essential spectrum eigs
\begin{lemma}\label{essspeclemma}
There exists $r_2 \leq r_1$ such that for $r \in \mathcal{R}$ with $r \leq r_2$, the following is true. For all positive integers $k$ with $|\lambda^K(X,k)| < \delta$, there is a pair of purely imaginary ``essential spectrum'' eigenvalues which are given by $\lambda = \pm \lambda^{ess}(X,k; r)$, where
\begin{equation}\label{lambdaess}
\lambda^{ess}(X, k; r) = c \frac{k \pi i }{X} \left( 1 + \mathcal{O}\left( \frac{1}{X} \right)\right) + \mathcal{O}\left( \frac{r^{1/2}}{X} \right)
\end{equation}
The remainder terms cannot move this off of the imaginary axis.

\begin{proof}
From Lemma \ref{ceqlemma}, we have a nontrivial solution to \eqref{eqford} if and only if 
\begin{align*}
E(\lambda) = \det (K(\lambda) + C_5 K_3(\lambda)) = 0
\end{align*}
Since we know the form of $K_3(\lambda)$, we have the following expression for $C_5 K_3(\lambda)$
\[
C_5 K_3(\lambda) = 
\begin{pmatrix}
c_{1,1}^- e^{-\nu(\lambda)X_1} - c_{1,1}^+ e^{\nu(\lambda)X_1} 
& \dots & 
c_{1, n-1}^- e^{-\nu(\lambda)X_{n-1}} - c_{1,n-1}^+ e^{\nu(\lambda)X_{n-1}} &
c_{1,0}^- e^{-\nu(\lambda)X_0} - c_{1,0}^+ e^{\nu(\lambda)X_0}  \\
\vdots & & \vdots & \\
c_{n,1}^- e^{-\nu(\lambda)X_1} - c_{n,1}^+ e^{\nu(\lambda)X_1}
& \dots & 
c_{1, n-1}^- e^{-\nu(\lambda)X_{n-1}} - c_{1,n-1}^+ e^{\nu(\lambda)X_{n-1}} &
c_{n,0}^- e^{-\nu(\lambda)X_0} - c_{n,0}^+ e^{\nu(\lambda)X_0} 
\end{pmatrix}
\]
where $c_{i,j}$ are constants with $|c_{i,j}| \leq C(|\lambda| + r^{1/2})$. 

To solve $E(\lambda) = 0$, we use the definition of the determinant of an $n \times n$ matrix $A$.
\begin{align*}
\det A = \sum_{\sigma \in S_n} \left( \text{sgn}(\sigma) \prod_{i=1}^n a_{i, \sigma(i)} \right)
\end{align*}
where $S_n$ is the symmetric group on $n$ elements. Applying this to $K(\lambda) + C_5 K_3(\lambda)$ and simplifying, we get
\begin{align}\label{Elambdaess}
E(\lambda)
&= -2 \sinh(\nu(\lambda)X) + \sum_{\tau \in T_n}
c_\tau \prod_{j = 0}^{n-1} e^{\tau(j) \nu(\lambda)X_j}
\end{align}

where $T_n = \{ (\pm 1, \pm 1, \dots, \pm 1 \}$ and $c_\tau = \mathcal{O}(|\lambda| + r^{1/2})$. From Lemma \ref{detKlemma}, $\det K(\lambda^K(X,k)) = 0$. Since we are looking for a small perturbation of $\lambda^K(X,k)$, let
\begin{equation}\label{tildelambdadef}
\lambda = \lambda^K(X,k) + \frac{\tilde{\lambda}}{X}
\end{equation}
where $k \in \Z$ with $|\lambda^K(X,k)| < \delta$. From the proof of Lemma \ref{detKlemma}, 
\begin{align*}
\nu\left( \lambda^K(X, k) + \frac{\tilde{\lambda}}{X} \right) 
&= \frac{k \pi i}{X} + \frac{1}{c}\frac{\tilde{\lambda}}{X} \left( 1 + \mathcal{O} \left(\frac{k}{X}\right)^2 \right) + \mathcal{O}\left( \frac{\tilde{\lambda}}{X}\right)^2 \\
&= \frac{k \pi i}{X} + C_k \frac{\tilde{\lambda}}{X} + \mathcal{O}\left( \frac{\tilde{\lambda}}{X}\right)^2 
\end{align*}
where $C_k = 1/c = \mathcal{O}(1)$. 

Substituting this into the term $\sinh(\nu(\lambda)X)$ in \eqref{Elambdaess}, we have
\begin{align*}
\sinh\left(\nu\left(\lambda^K(X, k) + \frac{\tilde{\lambda}}{X}\right)X\right)
&= \sinh\left(\left(\frac{k \pi i}{X} + C_k \frac{\tilde{\lambda}}{X} + \mathcal{O}\left( \frac{\tilde{\lambda}}{X}\right)^2 \right) X\right) \\
&= \sinh\left( k \pi i + C_k \tilde{\lambda} + \mathcal{O}\left( \frac{\tilde{\lambda}^2}{X}\right) \right) \\
&= (-1)^k \left( C_k \tilde{\lambda} + \mathcal{O}\left( \frac{\tilde{\lambda}^2}{X}\right) \right) + \mathcal{O}\left( \tilde{\lambda} + \frac{\tilde{\lambda}^2}{X} \right)^3 \\
&= (-1)^k C_k \tilde{\lambda} + \mathcal{O}\left( \frac{\tilde{\lambda}^2}{X} + \tilde{\lambda}^3 \right)
\end{align*}

For the remainder term of $E(\lambda)$, we have
\begin{align*}
c_\tau \prod_{j = 0}^{n-1} &\exp\left( {\tau(j) \nu(\lambda)X_j} \right)
= c_\tau \prod_{j = 0}^{n-1} 
\exp\left( \tau_j \left( \frac{k \pi i}{X} + C_k \frac{\tilde{\lambda}}{X} + \mathcal{O}\left( \frac{\tilde{\lambda}}{X}\right)^2 \right) X_j\right) \\
&= c_\tau \exp\left( \left( \sum_{j=0}^{n-1} \frac{\tau_j X_j}{X} \right)
\left( k \pi i + C_k \tilde{\lambda} + \mathcal{O}\left( \frac{\tilde{\lambda}^2}{X} \right) \right) \right) \\
&= c_\tau \exp\left( r_\tau
\left( k \pi i + C_k \tilde{\lambda} + \mathcal{O}\left( \frac{\tilde{\lambda}^2}{X} \right) \right) \right) \\ 
&= c_\tau e^{i k \pi r_\tau} \exp \left( r_\tau C_k \tilde{\lambda} + \mathcal{O}\left( \frac{\tilde{\lambda}^2}{X} \right) \right)
\end{align*}
where $r_\tau = \left( \sum_{j=0}^{n-1} \frac{\tau_j X_j}{X} \right)$ and $|r_\tau| \leq 1$ for all $\tau \in T_n$. Expanding the exponential in a Taylor series, we have
\begin{align*}
c_\tau \prod_{j = 0}^{n-1} \exp\left( {\tau(j) \nu(\lambda)X_j} \right)
&= \tilde{c}_\tau \left( 1 + r_\tau C_k \tilde{\lambda} + \mathcal{O}\left(\tilde{\lambda}^2 \right) \right) 
\end{align*}
where $\tilde{c}_\tau = c_\tau e^{i k \pi r_\tau} = \mathcal{O}(|\lambda| + r^{1/2})$. 
Since the remainder term of \eqref{Elambdaess} consists of a finite sum of terms of this form, we can write it as
\begin{align*}
\sum_{\tau \in T_n} c_\tau \prod_{j = 0}^{n-1} \exp\left( {\tau(j) \nu(\lambda)X_j} \right)
&= \mathcal{O}\left( (|\lambda| + r^{1/2}) \left( 1 + r_\tau C_k \tilde{\lambda} + \mathcal{O}\left(\tilde{\lambda}^2 \right) \right)\right) \\
&= \mathcal{O} \left( \frac{k \pi}{X} + \frac{\tilde{\lambda}}{X} + r^{1/2} \right)
\end{align*}

Combining all of these, we obtain an expression for $E(\lambda)$ which is entirely terms of $\tilde{\lambda}$.
\begin{align*}
E(\tilde{\lambda})
&= (-1)^k C_k \tilde{\lambda} + \mathcal{O}\left( \frac{\tilde{\lambda}^2}{X} + \tilde{\lambda}^3 \right) + \mathcal{O} \left( \frac{k \pi}{X} + \frac{\tilde{\lambda}}{X} + r^{1/2} \right) \\
&= (-1)^k C_k \tilde{\lambda} + \mathcal{O}\left( \frac{\tilde{\lambda}}{X} + \tilde{\lambda}^3 \right) + \mathcal{O} \left( \frac{k \pi}{X} + r^{1/2} \right) \\
&= (-1)^k C_k \tilde{\lambda} + \mathcal{O}\left( \frac{\tilde{\lambda}}{n|\log r| + \log|b_0\cdots b_{n-1}|} + \tilde{\lambda}^3 \right) + \mathcal{O} \left( \frac{k \pi}{X} + r^{1/2} \right)
\end{align*}

where we used our expression for $X$ from Lemma \ref{reparam}. Note that the last term on the RHS does not involve $\tilde{\lambda}$. We wish to solve $E(\tilde{\lambda}) = 0$. To do this, define
\[
F(\tilde{\lambda}) = (-1)^k C_k \tilde{\lambda} + \mathcal{O}\left( \frac{\tilde{\lambda}}{n|\log r| + \log|b_0\cdots b_{n-1}|} + \tilde{\lambda}^3 \right)
\]
so that we wish to solve
\[
F(\tilde{\lambda}) = \mathcal{O} \left( \frac{k \pi}{X} + r^{1/2} \right)
\]
Note that $F(0) = 0$ and for sufficiently small $r$, 
\[
\frac{\partial}{\partial\tilde{\lambda}}F(\tilde{\lambda})\big|_{\tilde{\lambda} = 0}
= (-1)^k C_k + \mathcal{O}\left( \frac{1}{n|\log r| + \log|b_0\cdots b_{n-1}|} \right) \neq 0
\]
since $C_k = \mathcal{O}(1)$. Thus by the inverse function theorem, $F$ is invertible in a neighborhood of 0. Recall that $|\frac{k \pi}{X}| < \delta$. Thus, decreasing $r_2$ and $\delta$ if needed, we can solve uniquely for $\tilde{\lambda}$, i.e. we have
\[
\tilde{\lambda} = F^{-1}\left( \mathcal{O} \left( \frac{k \pi}{X} + r^{1/2} \right)\right)
\]
In particular, since $F^{-1}$ is smooth with $F^{-1}(0) = 0$,
\[
\tilde{\lambda} = \mathcal{O}\left( \frac{k \pi}{X} + r^{1/2} \right)
\]
Substituting this into \eqref{tildelambdadef}, we have eigenvalues $\lambda$ at
\begin{align*}
\lambda &= \lambda^K(X,k) + \frac{\tilde{\lambda}}{X} \\
&= \lambda^K(X,k) + \mathcal{O}\left( \frac{1}{X} \left( \frac{k \pi}{X} + r^{1/2} \right) \right)\\
&= c \frac{k \pi i }{X} \left( 1 + \mathcal{O}\left( \frac{1}{X} \right)\right) + \mathcal{O}\left( \frac{r^{1/2}}{X} \right)
\end{align*}

By Hamiltonian symmetry, these must be purely imaginary since they cannot come in quartets. Thus, the ``essential spectrum'' eigenvalues are given by $\lambda = \pm \lambda^{ess}(X, k; r)$
\[
\lambda^{ess}(X, k; r) = c \frac{k \pi i }{X} \left( 1 + \mathcal{O}\left( \frac{1}{X} \right)\right) + \mathcal{O}\left( \frac{r^{1/2}}{X} \right)
\]
where $k$ is a positive integer with $k \pi/X < \delta$. The remainder terms cannot move these off of the imaginary axis.
\end{proof}
\end{lemma}

\subsection{Eigenvalue counts}

Finally, we will perform two counts of the small eigenvalues so that we can conclude that we have accounted for everything. First, we will count the eigenvalues with $|\lambda| < \delta$.

\begin{lemma}\label{eigcount}
There are $2n + 2 k_M + 1$ eigenvalues inside the circle $|\lambda| = \delta$, where $k_M$ is the largest positive integer $k$ such that $|\lambda^K(k,X) < \delta$. There are no other eigenvalues inside $|\lambda| = \delta$ besides these.

\begin{proof}
Use the radius $\delta$ from Theorem \ref{blockmatrixtheorem}. If necessary, decrease $\delta$ a little so that the circle of radius $\delta$ about the origin in the complex plane cuts exactly halfway between consecutive points $\lambda^K(X, k)$.

Take $\lambda$ with $|\lambda| = \delta$. Then $K(\lambda)$ is invertible. For all $k$ such that $|\lambda^K(X, k)| \leq \delta$, this implies
\[
| \lambda - \lambda^K(X, k)| \geq C \frac{1}{X}
\]
Following the proofs of Lemma \ref{detKlemma} and \ref{Kinvboundslemma}, we have
\[
|\det K(\lambda)| \geq C
\]
which implies
\begin{equation}\label{Kinvbounddelta}
||K(\lambda)^{-1}|| \leq C
\end{equation}

Since the $2n-2$ nonzero roots of $\det(A - \lambda^2 M I)$ are $\mathcal{O}(r^{1/2})$, we can find $r_3 \leq r_2$ such that for all $r \leq r_3$, $2 r^{1/2} \sqrt{\tilde{\mu}_M/M} \leq \delta$, where $\tilde{\mu}_M = \max\{|\tilde{\mu}_1|, \dots, |\tilde{\mu}_{n-1}| \}$. Thus for $|\lambda| = \delta$, using the bound \eqref{Ainvbound2} from Lemma \ref{Ainvboundlemma}, 
\begin{equation}\label{Ainvbounddelta}
||(A - \lambda^2 M I)^{-1}|| \leq \frac{C}{\delta^2}
\end{equation}

Factoring $K(\lambda)$ out of the top left, write \eqref{blockeq} as
\begin{equation}\label{blockeq2}
\begin{pmatrix}
(I + C_1 + K_1(\lambda)K(\lambda)^{-1})K(\lambda) & D_1 \\
C_2 K(\lambda) + K_2(\lambda) & A - \lambda^2 MI + D_2
\end{pmatrix}
\begin{pmatrix} c \\ d \end{pmatrix} = 0
\end{equation}
From the proof of Lemma \ref{deqlemma}, $I + C_1 + K_1(\lambda)K(\lambda)^{-1}$ is invertible. As in that lemma, let $C_3 = (I + C_1 + K_1(\lambda)K(\lambda)^{-1})$. Multiplying the top row of \eqref{blockeq2} by $C_3$, we obtain the equivalent formulation
\begin{equation}\label{blockeq3}
\begin{pmatrix}
K(\lambda) & C_3 D_1 \\
C_2 K(\lambda) + K_2(\lambda) & A - \lambda^2 MI + D_2
\end{pmatrix}
\begin{pmatrix} c \\ d \end{pmatrix} = 0
\end{equation}
Thus finding $\lambda$ for which \eqref{blockeq} has a nontrivial solution is equivalent to finding the zeros of $E(\lambda)$, where
\begin{equation}
E(\lambda) = \det 
\begin{pmatrix}
K(\lambda) & C_3 D_1 \\
C_2 K(\lambda) + K_2(\lambda) & A - \lambda^2 MI + D_2
\end{pmatrix}
\end{equation}

Using a standard determinant identity, since $K(\lambda)$ and $A - \lambda^2 M I$ are both invertible, we can write $E(\lambda)$ as
\begin{align*}
E(\lambda) &= \det(K(\lambda))
\det ( A - \lambda^2 MI + D_2 - (C_2 K(\lambda) + K_2(\lambda))K(\lambda)^{-1}C_3 D_1 ) \\
&= \det(K(\lambda))\det(A - \lambda^2 MI)
\det ( I + (A - \lambda^2 MI)^{-1}(D_2 - (C_2 + K_2(\lambda)K(\lambda)^{-1})C_3 D_1 ) \\
&= \det(K(\lambda))\det(A - \lambda^2 MI)\det(I + R(\lambda))
\end{align*}
where
\[
R(\lambda) = 
(A - \lambda^2 MI)^{-1}(D_2 - (C_2 + K_2(\lambda)K(\lambda)^{-1})C_3 D_1)
\]
Using the bounds \eqref{Ainvbounddelta} and \eqref{Kinvbounddelta} together with the bounds from Lemma \ref{reparam}, and recalling that $r^{1/2} < |\lambda| = \delta$, we have the bound on $R(\lambda)$
\begin{align*}
||R(\lambda)|| \leq C \frac{1}{\delta^2}
( |\delta|^3 + (r^{\tilde{\gamma}/2}\delta + \delta)\delta^2) = C \delta
\end{align*}

Thus we can write $R(\lambda) = \delta \tilde{R}(\lambda)$, where $\tilde{R}(\lambda) = \mathcal{O}(1)$. From a standard expansion of the determinant, 
\begin{align*}
\det(I + R(\lambda)) &= 1 + \delta \text{Tr}(\tilde{R}(\lambda)) + \mathcal{O}(\delta^2) \\
&= 1 + \mathcal{O}(\delta)
\end{align*}
For sufficiently small $\delta$, $\det(I + R(\lambda)) = 1 + \tilde{\delta}$, where $|\tilde{\delta}| < 1$. This gives us 
\begin{equation}
E(\lambda) = \det(K(\lambda))\det(A - \lambda^2 MI) + \tilde{\delta} \det(K(\lambda))\det(A - \lambda^2 MI)
\end{equation}

Since $\tilde{\delta} < 1$ and we are taking $\lambda = \delta$, by Rouch\'e's Theorem, $E(\lambda)$ and $\det(K(\lambda))\det(A - \lambda^2 MI)$ have the same number of zeros (counting multiplicty) inside the circle $|\lambda| = \delta$. By our choice of $\delta$, 
\begin{enumerate}[(i)]
\item $\det(A - \lambda^2 MI)$ has exactly $2n$ zeros inside the circle $|\lambda| = \delta$, which are given by $\{ 0, \pm r^{1/2} \sqrt{ \tilde{\mu}_1 /M}, \dots, \pm r^{1/2} \sqrt{\tilde{\mu}_{n-1}/M} \}$, where 0 has algebraic multiplicty 2.

\item Let $k_M$ be the largest positive integer $k$ such that $\lambda^K(k,X) < \delta$. Then $\det(K(\lambda))$ has exactly $2 K_M + 1$ zeros inside the circle $|\lambda| = \delta$, which are given by $\{0, \pm \lambda^K(1,X), \dots, \lambda^K(k_M,X)\}$, where 0 has algebraic multiplicity 1.
\end{enumerate}

Thus there are exactly $2n + 2 k_M + 1$ eigenvalues inside the circle $|\lambda| = \delta$. 
\end{proof}
\end{lemma}

We will also count the eigenvalues in a small ball around the 0 

\begin{lemma}\label{eigcount2}
Let 
\begin{equation}\label{xiradius}
\xi = \min\left\{ \frac{\pi}{2X}, \frac{r^{1/2}\sqrt{\tilde{\mu}_m/M} }{2} \right\} = 
\min\left\{ \frac{\pi}{2 C( n |\log X| + |\log(b_1\cdots b_{n-1}|)}, \frac{r^{1/2} \sqrt{\tilde{\mu}_m/M}}{2} \right\} 
\end{equation}
where $\tilde{\mu}_m = \min \{ |\tilde{\mu_1}|, \dots, |\tilde{\mu}_{n-1}| \}$. Then for sufficiently small $r$, there are exactly 3 eigenvalues inside the circle of radius $\xi$ in the complex plane.

\begin{proof}
For $|\lambda| = \xi$, $K(\lambda)$ is invertible. We look for nontrivial solutions to \eqref{blockeq}, which, as in the previous lemma, we rewrite as \eqref{blockeq2}.

To bound $K_1(\lambda)K(\lambda)^{-1}$, we follow the proof of Lemma \ref{Kinvboundslemma}. By our choice of $\xi$, the $\epsilon$ criterion in that lemma is automatically satisfied. Replacing the condition that $|\lambda| \geq C r^{1/2}$ with $|\lambda| = \delta$, we have the bound
\begin{align*}
||K_1(\lambda)K(\lambda)^{-1}|| &\leq C( \xi + r^{1/2})\max\left\{ r^{-1/4}, \frac{1}{\xi X} \right\} \\
&\leq C r^{1/2} \max\left\{ r^{-1/4}, \frac{1}{\xi X} \right\} \\
&\leq C r^{1/2} \max\left\{ r^{-1/4}, 1, \frac{r^{-1/2}}{X} \right\} \\
&\leq C \max\left\{ r^{1/4}, \frac{1}{X} \right\} \\
&\leq C \max\left\{ r^{1/4}, \frac{1}{n|\log r| + |\log(b_1 \cdots b_{n-1})| } \right\} 
\end{align*}
where we used the expression for $X$ from Lemma \ref{reparam}. Since the bound for $C_1$ is stronger than this, for sufficiently small $r$, $I + C_1 + K_1(\lambda)K(\lambda)^{-1}$ is invertible. Let $C_3 = (I + C_1 + K_1(\lambda)K(\lambda)^{-1})$. As in the previous lemma, multiply the top row of \eqref{blockeq2} by $C_3$ to get \eqref{blockeq3}. Then finding a nontrivial soluton to \eqref{blockeq3} is equivalent to solving $E(\lambda) = 0$, where 
\begin{equation}
E(\lambda) = \det 
\begin{pmatrix}
K(\lambda) & C_3 D_1 \\
C_2 K(\lambda) + K_2(\lambda) & A - \lambda^2 MI + D_2
\end{pmatrix}
\end{equation}

As in the previous lemma, since $K(\lambda)$ and $A - \lambda^2 M I$ are invertible for $|\lambda| = \xi$, we use a standard determinant identity to write $E(\lambda)$ as 
\begin{align*}
E(\lambda)
&= \det(K(\lambda))\det(A - \lambda^2 MI)\det(I + R(\lambda))
\end{align*}
where
\[
R(\lambda) = 
(A - \lambda^2 MI)^{-1}(D_2 - (C_2 + K_2(\lambda)K(\lambda)^{-1})C_3 D_1)
\]

First, we bound $(D_2 - (C_2 + K_2(\lambda)K(\lambda)^{-1})C_3 D_1)$. Since $K_2(\lambda)$ is of the same form as $K_1(\lambda)$, we can use the bound for $K_1(\lambda)K(\lambda)^{-1}$ here.
\begin{align*}
||(D_2 &- (C_2 + K_2(\lambda)K(\lambda)^{-1})C_3 D_1)|| \\
&\leq C \left( (\xi + r^{1/2})^3 + (\xi + r^{1/2})^2 \left( r^{\tilde{\gamma}/2}(\xi + r^{1/2})^2 + \max\left\{ r^{1/4}, \frac{1}{n|\log r| + |\log(b_1 \cdots b_{n-1})| } \right\} \right) \right) \\
&\leq C \left( r^{3/2} + r \left( r^{1 + \tilde{\gamma}/2} + \max\left\{ r^{1/4}, \frac{1}{n|\log r| + |\log(b_1 \cdots b_{n-1})| } \right\} \right) \right) \\
&\leq C r \max\left\{ r^{1/4}, \frac{1}{n|\log r| + |\log(b_1 \cdots b_{n-1})| } \right\} 
\end{align*}

For a bound on $(A - \lambda^2 MI)^{-1}$, we use Lemma \ref{Ainvboundlemma}. The bound depends on $\xi$. If $\xi = \frac{\pi}{2X}$, we can go ahead and use the bound \eqref{Ainvbound1} from Lemma \ref{Ainvboundlemma}, which is
\[
||(A - \lambda^2 MI)^{-1}|| \leq C X^{n+1} r^{(n-1)/4}
\]

If $\xi = \frac{\tilde{\mu}_m r^{1/2}}{2}$, then we need to compute a new bound. For $\det(A - \lambda^2 MI)$, we can adapt the proof of Lemma \ref{detAboundlemma} to get
\[
|\det(A - \lambda^2 MI)| \leq C \xi^2 (\xi + r^{1/2})^{n-1} r^{(n-1)/2}
\]
Using this in the proof of Lemma \ref{Ainvboundlemma}, we get
\begin{align*}
||(A - \lambda^2 MI)^{-1}|| &\leq C \frac{(r + \xi^2)^{n-1}}{\xi^2 (\xi + r^{1/2})^{n-1} r^{(n-1)/2}} \\
&= C \frac{r^{n-1}}{r \: r^{(n-1)/2} r^{(n-1)/2}} \\
&= \frac{C}{r} 
\end{align*}

Combining all of these, we can get a bound on $R(\lambda)$. If $\xi = \frac{\pi}{2X}$, then
\begin{align*}
||R(\lambda)|| &\leq C X^{n+1} r^{(n-1)/4} r \max\left\{ r^{1/4}, \frac{1}{n|\log r| + |\log(b_1 \cdots b_{n-1})| } \right\} \\
&\leq C r^{1/2} (r^{1/4} X)^{n+1} \max\left\{ r^{1/4}, \frac{1}{n|\log r| + C_b } \right\} \\
&\leq C r^{1/2} \left(r^{1/4}(n|\log r| + |\log(b_1 \cdots b_{n-1})|)\right)^{n+1} \max\left\{ r^{1/4}, \frac{1}{n|\log r| + |\log(b_1 \cdots b_{n-1})| } \right\}
\end{align*}

This can be made arbitrarily small by taking $r$ sufficiently small. If $\xi = \frac{\tilde{\mu}_m r^{1/2}}{2}$, then 
\begin{align*}
||R(\lambda)|| &\leq \frac{C}{r} r \max\left\{ r^{1/4}, \frac{1}{n|\log r| + |\log(b_1 \cdots b_{n-1})|} \right\} \\
&= C \max\left\{ r^{1/4}, \frac{1}{n|\log r| + |\log(b_1 \cdots b_{n-1})|} \right\} 
\end{align*}
This can also be made arbitrarily small. Choose $r_3$ sufficiently small so that for $r \leq r_3$,
\[
||R(\lambda)||_{max} \leq \frac{1}{3n}
\] 
so that $\text{Tr}(R(\lambda)) \leq 1/3$. From a standard expansion of the determinant, 
\begin{align*}
\det(I + R(\lambda)) &= 1 + \text{Tr}(R(\lambda)) + \mathcal{O}\left(\frac{1}{9n^2} \right) \\
&= 1 + p
\end{align*}
where $0 < |p| < 1$. Thus we have
\begin{align*}
E(\lambda) &= \det(K(\lambda))\det(A - \lambda^2 MI)\det(I + R(\lambda)) \\
&= \det(K(\lambda))\det(A - \lambda^2 MI)(1 + p) \\
&= \det(K(\lambda))\det(A - \lambda^2 MI) + p \det(K(\lambda))\det(A - \lambda^2 MI)
\end{align*}

Since $|p| < 1$, by Rouch\'e's Theorem, $E(\lambda)$ and $\det(K(\lambda))\det(A - \lambda^2 MI)$ have the same number of zeros (counting multiplicty) inside the circle $|\lambda| = \xi$. By our choice of $\xi$, $\det(K(\lambda))\det(A - \lambda^2 MI)$ has exactly 3 zeros inside $|\lambda| = \xi$, all of which occur at $\lambda = 0$. We conclude that $\tilde{E}(\lambda)$ (and thus $E(\lambda)$) has exactly 3 zeros inside $|\lambda| = \xi$.\\
\end{proof}
\end{lemma}

\subsection{Proof of Theorem \ref{locateeigtheorem}}

For part (i), it follows from \eqref{Arelations} that $Q_{np}'(x)$ is an eigenfunction with eigenvalue 0 and $T_{np}(x)$ is a generalized eigenfunction with eigenvalue 0 corresponding to $Q_{np}'(x)$. By Hypothesis \ref{Melnikov2hyp}, this Jordan chain cannot continue. By Lemma \ref{varadjsolutions}, there is another eigenfunction $V^c(x)$ with eigenvalue 0; this eigenfunction is bounded but does not decay to 0. Although Lemma \ref{varadjsolutions} states this for $A(Q(x))$, i.e. the linearization about the primary pulse, the same argument holds for $A(Q_{np}(x))$, since it only depends on the rest state.

Part (ii) follows from Lemma \eqref{inteigslemma} and part (iii) follows from Lemma \ref{essspeclemma}. Part (iv) follows from Lemmas \ref{eigcount} and \ref{eigcount2}. Using these lemmas, a complete account of the eigenvalues inside $|\lambda| = \delta$ is as follows.
\begin{enumerate}
	\item At $\lambda = 0$, there is an eigenvalue with algebraic multiplicity 3. The eigenfunctions are those from part (i).
	\item There are $2n - 2$ interaction eigenvalues, which come in pairs $\pm \lambda$. Each pair is real or purely imaginary.
	\item There $2 k_M$ purely imaginary ``essential spectrum'' eigenvalues, which also come in pairs.
\end{enumerate}

\subsection{Proof of Theorem \ref{inteigsparity}}

Let $r_1$ and $b^*$ be as in Theorem \ref{unifperexist}. Let $\tilde{A}_0$ be the tri-diagonal, symmetric matrix 
\begin{align*}
\tilde{A}_0 &= \begin{pmatrix}
-\tilde{a}_0 & \tilde{a}_0 \\
\tilde{a}_0 & -\tilde{a}_0 - \tilde{a}_1 &  \tilde{a}_1 \\
& \tilde{a}_1 & -\tilde{a}_1 - \tilde{a}_2 &  \tilde{a}_2 \\
& & \ddots & & \ddots \\
& & & & & \tilde{a}_{n-2} & -\tilde{a}_{n-2} \\
\end{pmatrix}
\end{align*}
which is obtained from the matrix $\tilde{A}$ by taking $\tilde{a}_{n-1} = 0$. The matrix $\tilde{A}_0$ is symmetric, so its eigenvalues are real, and $(1, 1, \dots, 1)^T$ is an eigenvector of $\tilde{A}_0$ with eigenvalue 0. Let $0, \mu^0_1, \dots, \mu^0_{n-1}$ be the eigenvalues of $\tilde{A}_0$. Let $n_+$ be the number of positive $\tilde{a}_j$ and $n_i = n - n_+ - 1$ be the number of negative $\tilde{a}_j$. By Lemma 5.4 of \cite{Sandstede1998} (noting that $\tilde{A}_0$ is the matrix $-A_0$ in that lemma),
\begin{enumerate}[(i)]
\item $\tilde{A}_0$ has $n_+$ negative eigenvalues (counting multiplicity)
\item $\tilde{A}_0$ has $n_-$ positive eigenvalues (counting multiplicity)
\end{enumerate}

For $\tilde{a}_{n-1}$ small, $\tilde{A}$ is a small perturbation of $\tilde{A}_0$. Since characteristic polynomials are smooth functions of matrix entries, the eigenvalues of a matrix are also smooth functions of the matrix entries. In particular, the eigenvalues of $\tilde{A}$ depend smoothly on $\tilde{a}_{n-1}$, and as $\tilde{a}_{n-1}$ approaches 0, the eigenvalues of $\tilde{A}$ approach those of $\tilde{A}_0$. In particular, as $\tilde{a}_{n-1}$ is increased from 0, the eigenvalues of $\tilde{A}$ can only change sign by crossing through 0. Thus for sufficiently small $\tilde{a}_{n-1}$, the signs of the eigenvalues of $\tilde{A}$ are determined by the signs of the $n-1$ matrix elements $\tilde{a}_0, \dots, \tilde{a}_{n-2}$. We will now obtain this result in terms of $r$ and the baseline length parameters used to contruct the periodic $n-$pulse.

From the proof of Lemma \eqref{reparam}, $\tilde{a}_j$ is given by
\begin{align}\label{tildeaj2}
\tilde{a}_j
&= (-1)^{-\rho \log r / \pi} s_0 e^{\alpha \phi/\beta} \left( \beta b_j(r; m_j, \theta) \cos\left( -\rho \log b_j(r; m_j, \theta) \right) - \alpha b_j(r; m_j, \theta) \sin \left( -\rho \log b_j(r; m_j, \theta) \right) \right)
\end{align}
As $r \rightarrow 0$, $b_j(r; m_j, \theta) \rightarrow b_j^*(m_j, \theta)$ by Theorem \ref{perexist}. As $m_{n-1} \rightarrow \infty$, $b_j^*(m_j, \theta) \rightarrow b_j^0$ by Lemma \ref{thetaparamlemma}. Finally, since $b_{n-1}^0 = \exp\left(-\frac{m_{n-1} \pi}{\rho}\right)$, $b_{n-1}(r; m_{n-1}, \theta) \rightarrow 0$ (and so $\tilde{a}_{n-1} \rightarrow 0$) as $(r, m_{n-1}) \rightarrow (0, \infty)$. Thus we can find a positive integer $M_1$ and $\tilde{r}_1 \leq r_1$ such that for all $m_{n-1} \geq M_1$ and $r \leq \tilde{r}_1$, 
\begin{enumerate}[(i)]
	\item $\tilde{a}_{n-1}$ is sufficiently small so that the eigenvalues of $\tilde{A}$ have the same sign as those of $\tilde{A}_0$.
	\item For $j = 0, \dots, n-2$ and for all $\theta \in [-\arctan \rho, \pi - \arctan \rho)$,
	\begin{equation}\label{bjunif}
	b_j(r; m_j, \theta) = b_j^0 e^{ -\frac{1}{\rho} \theta^*_j(r, m_{n-1}) } = e^{ -\frac{1}{\rho}(m_j \pi + \theta^*_j(r, m_{n-1})) } 
	\end{equation}
	where $|\theta^*_j(r, m_{n-1})| < \arctan \rho$.
\end{enumerate}

We can now determine the signs of the $\tilde{a}_j$ for $j = 0, \dots, n-2$. Since
\begin{align*}
\cos\left( -\rho \log b_j(r; m_j, \theta) \right) 
&= \cos\left( -\rho \log e^{ -\frac{1}{\rho}(m_j \pi + \theta^*_j(r, m_{n-1})) } \right) \\
&= \cos\left( m_j \pi + \theta^*_j(r, m_{n-1})\right) \\
&= (-1)^{m_j} \cos \theta^*_j(r, m_{n-1})
\end{align*}
and
\begin{align*}
\sin\left( -\rho \log b_j(r; m_j, \theta) \right) 
&= \sin\left( m_j \pi + \theta^*_j(r, m_{n-1})\right) \\
&= (-1)^{m_j} \sin \theta^*_j(r, m_{n-1})
\end{align*}
upon substituting \eqref{bjunif} into equation \eqref{tildeaj2} we obtain
\begin{align*}
\tilde{a}_j 
&= (-1)^{-\rho \log r / \pi} (-1)^{m_j} s_0 e^{\alpha \phi/\beta} b_j^0 e^{ -\frac{1}{\rho} \theta^*_j(r, m_{n-1}) } \left( \beta \cos\theta^*_j(r, m_{n-1}) - \alpha \sin \theta^*_j(r, m_{n-1}) \right) \\
&= (-1)^{m + m_j} \left[ s_0 \alpha e^{\alpha \phi/\beta} b_j^0 e^{ -\frac{1}{\rho} \theta^*_j(r, m_{n-1}) } \cos\theta^*_j(r, m_{n-1}) \right] \left( \rho - \tan \theta^*_j(r, m_{n-1}) \right)
\end{align*}
where we have let $r \in \mathcal{R}$ as $r = e^{-\frac{1}{\rho}m \pi}$ for some nonnegative integer $m$. The term in brackets is always positive since $|\theta^*_j(r, m_{n-1})| < \arctan \rho$. Thus the sign of $\tilde{a}_j(0)$ is completely determined by the term $(-1)^{m + m_j}$, and we have
\begin{align*}
\tilde{a}_j(0) &> 0 && \text{if } m + m_j \text{ is even} \\
\tilde{a}_j(0) &< 0 && \text{if } m + m_j \text{ is odd}
\end{align*}
From this, it follows that if $m$ is even,
\begin{enumerate}[(i)]
\item $\tilde{A}_0$ has $n_{\text{even}}$ negative eigenvalues (counting multiplicity)
\item $\tilde{A}_0$ has $n_{\text{odd}}$ positive eigenvalues (counting multiplicity)
\end{enumerate}
This is reversed if $m$ is odd. The result follows from equation \eqref{inteigsformula} from Lemma \ref{inteigslemma}, where we note the dependence on the sign of $M$. The condition that $m_{n-1} \geq M_1$ is equivalent to $b_{n-1}^0 \leq \tilde{b}^*$ for some $\tilde{b}^* \leq b^*$.

\subsection{Interaction Eigenvalues: Specific Cases}

There are specific cases in which we can compute the eigenvalues of $\tilde{A}$ in terms of the $\tilde{a}_j$.

Recall that the $\tilde{a}_j$ are given by
\begin{align*}
\tilde{a}_j(r)
&= (-1)^{-\rho \log r / \pi} s_0 e^{\alpha \phi/\beta} \left( \beta b_j(r; m_j; \theta) \cos\left( -\rho \log b_j(r; m_j; \theta) \right) - \alpha b_j(r; m_j; \theta) \sin \left( -\rho \log b_j(r; m_j; \theta)  \right) \right) + \mathcal{O}(r^{\gamma/2\alpha})
\end{align*}

First, we consider the case when $n = 2$. In that case, the eigenvalues of $\tilde{A}$ are $\{0, \tilde{a} \}$, where
\begin{align*}
\tilde{a} = \tilde{a}_0 + \tilde{a}_1
\end{align*}
For a symmetric 2-periodic pulse, $m_0 = m_1 = 0$ (recall that one of them must be 0). Then by Theorem \ref{2pulsebifurcation}, for sufficiently small $r$ we have symmetric solutions with equal length parameters $b_0(\theta) = b_1(\theta) = e^{-\theta/\rho}$. These length parameters do not depend on $r$. Thus we have
\begin{align*}
\tilde{a}_0(r) = \tilde{a}_1(r)  
&= (-1)^{-\rho \log r / \pi} s_0 e^{\alpha \phi/\beta} e^{-\theta/\rho} \left( \beta \cos \theta - \alpha \sin \theta \right) + \mathcal{O}(r^{\gamma/2\alpha}) \\
&= (-1)^{-\rho \log r / \pi}  \frac{s_0 e^{\alpha \phi/\beta} }{\alpha}  e^{-\theta/\rho} \left( \rho \cos \theta - \sin \theta \right) + \mathcal{O}(r^{\gamma/2\alpha})
\end{align*}

For $r = 0$, $\tilde{a}(0) = 0$ at the pitchfork bifurcation point $\theta = p^*(0)$. 





For small $r$, we should be able to argue that $\tilde{a}_j(r) = 0$ at the pitchfork bifurcation point $p_0(r)$. Thus there is a pitchfork bifurcation in the integration eigenvalues at $\theta = p_0(r)$, in which the interaction eigenvalues collide at the origin and switch from a real pair to a purely imaginary pair or vice versa. 

\iffulldocument\else
	\bibliographystyle{amsalpha}
	\bibliography{thesis.bib}
\fi

\end{document}