\documentclass[thesis.tex]{subfiles}

\begin{document}

\iffulldocument\else
	\chapter{KdV5}
\fi

In this chapter, we look at the spectral stability of the periodic multi-pulses which we constructed in \cref{chapter:kdv5periodic}. To do this, we will start with the framework in \cref{sec:genspectrum}. For a periodic multi-pulse $Q_n(x)$, we will use Lin's method to construct eigenfunctions as piecewise perturbations of the kernel eigenfunctions $\partial_x Q_n(x)$ and $\partial_c Q_n(x)$. As in \cite{Sandstede1998}, we will reduce the problem of finding eigenvalues to evaluating the determinant of a matrix. In this case, since there is a center direction, we will have $2n \times 2n$ block matrix where the diagonal blocks represent, to leading order, interaction eigenvalues and essential spectrum eigenvalues. We will then use this block matrix equation to actually locate the eigenvalues, first for the periodic 2-pulse and then for general periodic multi-pulses.

\section{Setup of problem}

Let $Q_n(x)$ be a periodic $n-$pulse solution constructed according to Theorem \ref{perexist}. As in \cref{chapter:kdv5periodic}, we will use Lin's method to construct an eigenfunction $V(x)$ as a small perturbation of a piecewise linear combination of the kernel eigenfunctions $\partial_x Q_n(x)$ and $\partial_c Q_n(x)$. For $X_m = \min\{X_0, \dots, X_{n-1} \}$ sufficiently large and $\lambda$ sufficiently small, Lin's method provides a unique function $V(x)$ which solves \cref{PDEeigsystem} but which generically has $n$ jumps. Since we are not using an exponential weight, equation \cref{PDEeigsystem} has a center direction, which will imply that these $n$ jumps are in the two-dimensional space spanned by $\Psi(0)$ and $\Psi^c(0)$. This gives us $2n$ jump conditions. Finding the eigenvalues amounts to solving the $2n$ jump conditions.
 
The first step is to rewrite the eigenvalue problem \cref{PDEeigsystem} as
\begin{equation}\label{PDEeig3}
V' = A(Q_n(x); \lambda)V 
\end{equation} 
where 
\begin{equation}
A(Q_n(x); \lambda) = A(Q_n(x)) + \lambda B
\end{equation}
Let $A(\lambda) = A(0; \lambda)$. For $\lambda = 0$, the asymptotic matrix $A(0)$ has a simple eigenvalue at 0. The next lemma states that for small $\lambda$, $A(0)$ has a simple eigenvalue $\nu(\lambda)$ near 0.

% nu(lambda) lemma

\begin{lemma}\label{nulambdalemma}
There exists $\delta_0 > 0$ such that for $|\lambda| < \delta_0$, the $A(\lambda)$ has a simple eigenvalue $\nu(\lambda)$. $\nu(\lambda)$ is smooth in $\lambda$, $\nu(0) = 0$, $\nu'(0) = 1/c$, and for $|\lambda| < \delta_0$,
\begin{equation}\label{nulambda}
\nu(\lambda) = \frac{1}{c} \lambda + \mathcal{O}(|\lambda|^3)
\end{equation}
In addition,
\begin{enumerate}[(i)]
\item $\nu(-\lambda) = -\nu(\lambda)$, i.e. $\nu(\lambda)$ is an odd function.
\item If $\lambda$ is pure imaginary, $\nu(\lambda)$ is also pure imaginary.
\item The corresponding eigenvector to $\nu(\lambda)$ is $V_0(\lambda)$ which is smooth in $\lambda$ and has Taylor expansion
\begin{equation}\label{V0expansion}
V_0(\lambda) = V_0 + \lambda V_0'(0) + \mathcal{O}(\lambda^2),
\end{equation}
where $V_0'(0) = (0, 1/c^2, 0, \dots, 0)^T$. Furthermore, $V_0(-\lambda) = R V_0(\lambda)$, where $R$ is the standard reversor operator.
\end{enumerate}

Similarly, the matrix $-A(\lambda)^*$ has an eigenvalue $-\overline{\nu(\lambda)}$ with corresponding eigenvector $W_0(\overline{\lambda})$, both of which are smooth in $\overline{\lambda}$. $W_0(\overline{\lambda})$ has Taylor expansion
\begin{equation}\label{W0expansion}
W_0(\lambda) = W_0 + \overline{\lambda} W'(0) + \mathcal{O}(\overline{\lambda}^2),
\end{equation}
where 
\begin{equation}\label{W0prime}
W_0'(0) = \frac{1}{c} \left( 0, -c_3, 0, -c_5, 0, \dots, 0, -c_{2m-1}, 0, 1, 0\right)^T
\end{equation}
and symmetry property $W_0(-\overline{\lambda}) = R W_0(\overline{\lambda})$.
\end{lemma}

\section{Block matrix theorem}

We can now state the main theorem of this chapter which provides a condition for $\lambda$ to be an eigenvalue of \eqref{PDEeig3}. This theorem is analogous to \cite[Theorem 2]{Sandstede1998}, where the matrix $A$ replaced by a block matrix.

% block matrix theorem
\begin{theorem}\label{blockmatrixtheorem}
Assume Hypotheses \ref{Ehyp}, \ref{Hhyp}, \ref{hypeqhyp}, \ref{Qexistshyp}, and \ref{H0transversehyp}. Let $Q(x)$ be the primary pulse solution, an let $Q_n(x)$ be a periodic $n-$pulse solution constructed according to Theorem \ref{perexist} with pulse lengths $X_0, \dots, X_{n-1}$. Then there exists $\delta > 0$ with the following property. There exists a bounded, nonzero solution $V$ of \eqref{PDEeig3} for $|\lambda| < \delta$ if and only if the $(2n \times 2n)$ block matrix equation
\begin{equation}\label{blockeq}
\begin{pmatrix}
K(\lambda) + C_1 K(\lambda) + K_1(\lambda) + C_2 & 2 \lambda \tilde{A} + D_1 \\
-\lambda M^c \tilde{K}(\lambda) + C_3 K(\lambda) + K_2(\lambda) + C_4 & A - \lambda^2 MI + D_2
\end{pmatrix}
\begin{pmatrix} c \\ d \end{pmatrix} = 0
\end{equation}
has a nontrivial solution. The individual terms in the block matrix are as follows.

\begin{enumerate}
\item $K(\lambda)$ is the banded matrix
\begin{equation}
K(\lambda) = 
\begin{pmatrix}
e^{-\nu(\lambda)X_1} & & & & & -e^{\nu(\lambda)X_0} \\
-e^{\nu(\lambda)X_1} & e^{-\nu(\lambda)X_2} \\
& -e^{\nu(\lambda)X_2} & e^{-\nu(\lambda)X_3} \\
& \ddots & \ddots & &&  \\
& & & & -e^{\nu(\lambda)X_{n-1}} & e^{-\nu(\lambda)X_0} 
\end{pmatrix}
\end{equation}
where $\nu(\lambda)$ is defined in Lemma \ref{nulambdalemma}. $\tilde{K}(\lambda)$ is the same matrix with all terms positive.

\item $A$ is the symmetric banded matrix
\begin{align}\label{Asymm}
A &= \begin{pmatrix}
-a_0 -a_1 & a_0 + a_1 \\
a_0 + a_1 & -a_0 - a_1
\end{pmatrix} && n = 2 \\
A &= \begin{pmatrix}
-a_{n-1} - a_0 & a_0 & & &  & a_{n-1}\\
a_0 & -a_0 - a_1 &  a_1 \\
& a_1 & -a_1 - a_2 &  a_2 \\
& \ddots & \ddots & \ddots \\
a_{n-1} & & & & a_{n-2} & -a_{n-2} - a_{n-1} \\
\end{pmatrix} && n > 2 \nonumber
\end{align}
where
\begin{align*}
a_i &= \langle \Psi(X_i), Q'(-X_i) \rangle
\end{align*}

\item $\tilde{A}$ is the matrix
\begin{align*}
\tilde{A} &= \begin{pmatrix}
-e^{-\nu(\lambda)X_1} q(X_1) & e^{-\nu(\lambda)X_1} q(X_1) \\
& -e^{-\nu(\lambda)X_2} q(X_2) & e^{-\nu(\lambda)X_2} q(X_2) \\
& \ddots \\
e^{-\nu(\lambda)X_0} q(X_0) & &  & -e^{-\nu(\lambda)X_0} q(X_0) & 
\end{pmatrix}
\end{align*}
where $q(x)$ is the first component of the primary pulse $Q(x)$. $\tilde{A}$ has uniform bound
\begin{align*}
\tilde{A} &= \mathcal{O}( e^{-\alpha X_m})
\end{align*}

\item $M$ and $M^c$ are the Melnikov-type integrals
\begin{align*}
M &= \int_{-\infty}^\infty q(y) \partial_c q(y) dy \\
M^c &= \int_{0}^\infty q(y) v^c(y) dy
\end{align*}
where $v^c(y)$ is the first component of $V^c(y)$, which is defined in \cref{varadjsolutions}.

\item The remaining terms are remainder matrices which are analytic in $\lambda$ and have uniform bounds
\begin{align*}
C_1 &= \mathcal{O}(|\lambda|e^{-\alpha X_m}(|\lambda| + e^{-\alpha X_m})) \\
C_2 &= \mathcal{O}(|\lambda|e^{-\alpha X_m}) \\
C_3 &= \mathcal{O} (|\lambda| + e^{-\alpha X_m})^2) \\
C_4 &= \mathcal{O}((|\lambda| + e^{-\alpha X_m})^2) \\
D_1 &= \mathcal{O}(|\lambda|(|\lambda| + e^{-\alpha X_m})^2) \\
D_2 &= \mathcal{O}((|\lambda| + e^{-\alpha X_m})^3)
\end{align*}
for $\alpha$ slightly smaller than $\alpha_0$ (which is defined precisely in the proof), and $X_m = \min\{X_0, \dots, X_{n-1}\}$.

\item $K_1(\lambda)$ and $K_2(\lambda)$ are obtained from $K(\lambda)$ by multiplying each nonzero entry by $\mathcal{O}(|\lambda|(|\lambda| + e^{-\alpha X_m}))$ and $\mathcal{O}((|\lambda| + e^{-\alpha X_m})^2)$ (respectively). They are defined precisely in the proofs of Lemmas \ref{jumpcenteradj} and \ref{jumpadj}
\end{enumerate}
\end{theorem}

We expect to find eigenvalues near where the two leading order blocks, $A - \lambda^2 MI$ and $K(\lambda)$, are singular. As in \cref{chapter:kdv5homoclinic}, points where $A - \lambda^2 MI$ is singular give us the interaction eigenvalues as well as two kernel eigenvalues. $K(\lambda)$ is singular for a discrete set of points on the imaginary axis. These are the periodic analogue to the essential spectrum in the $n$-homoclinic case. Although they are technically still point spectrum, we will refer to them as essential spectrum eigenvalues to distinguish them from the interaction eigenvalues.

We also note that although the expressions for the remainder matrices in \cref{blockmatrixtheorem} seem tediously detailed, this will be useful for the proof of results in a later section.

\section{Interaction eigenvalues of periodic 2-pulses}

We will now use Theorem \ref{blockmatrixtheorem} to locate the interaction eigenvalues of \eqref{PDEeig3}. First we consider the case of the periodic 2-pulse. Although the computation is tedious, we can compute the determinant of \cref{blockeq} exactly. 

WE WILL STATE (AND PROVE AT THE END) THE FOLLOWING THEOREMS HERE.
\begin{enumerate}
\item Find interaction eigenvalues if not in a Krein bubble.
\item Find essential spectrum eigenvalues if not in a Krein bubble.
\item Find the Krein bubble
\item Demonstrate the eigenvalue bifurcation for periodic 2-pulses with equal period and show that it follows the pitchfork bifurcation structure from the previous chapter.
\end{enumerate}

\section{Eigenvalues of periodic multi-pulses}

In this section, we generalize the results to periodic multi-pulses. Since computing the determinant of \cref{blockeq} is unfeasible for $n \geq 3$, this will require a different approach. First, we note that the matrix $A$ from Theorem \ref{blockmatrixtheorem} is symmetric, thus its eigenvalues are real. $A$ has an eigenvalues of 0 corresponding to eigenvector $(1, 1, \dots, 1)^T$. In the next hypothesis, we assume that the eigenvalues of $A$ distinct. 

\begin{hypothesis}\label{Adistincteigs}
The eigenvalues of $A$ are given by $(0, \mu_1, \dots, \mu_{n-1})$, all of which are distinct.
\end{hypothesis}

\noi This is not true in general. For example, if $n \geq 3$ and all the terms $a_i$ in $A$ are identical, $A$ is a circulant matrix, and it is not hard to show that $A$ will have some eigenvalues of algebraic multiplicity 2.

We expect to find eigenvalues near where the leading order matrices $K(\lambda)$ and $A - \lambda^2 MI$ are singular. This happens at the following values of $\lambda$. 
\begin{itemize}
	\item $A - \lambda^2 M I$ is singular for $\lambda = 0$ (algebraic multiplicity 2) and $\lambda \in \{ \pm \sqrt{\mu_1/M}, \dots, \pm \sqrt{\mu_{n-1}/M}\}$. The interaction eigenvalues will be located near these points.	In terms of the scaling parameter $r$, the interaction eigenvalues will be order $\mathcal{O}(r^{1/2})$.

	\item $K(\lambda)$ is singular at $\lambda = \pm \lambda^K(X,k)$ for integer $k$, where $\lambda(X, 0) = 0$ (algebraic multiplicity 1) and
	\begin{align}\label{lambdaXkapprox}
	\lambda^K(X,k) \approx c \frac{k \pi i }{X} 
	\end{align}
	The essential spectrum eigenvalues will be close to these points. Using equation \cref{Xdomain}, in terms of the scaling parameter $r$, $\lambda^K(X,k) = \mathcal{O}(1/\log|r|)$. Since \cref{blockmatrixtheorem} only holds for $|\lambda| < \delta$, we will restrict our analysis to $|\lambda^K(X,k)| < \delta$.
\end{itemize}  

For the analysis to work, we need to ensure that nonzero singular points of the two leading order matrices are sufficiently isolated from each other. To that end, we take the following definition.

\begin{definition}\label{epsilonballs}
A periodic $n-$pulse $Q_n$ parameterized as in Theorem \ref{perexist} satisfies the \emph{$\epsilon-$ball condition} if the two sets of points 
\begin{itemize}
\item $S_1 = \{ \pm \sqrt{\mu_1/M}, \dots, \pm \sqrt{\mu_{n-1}/M} \}$
\item $S_2 = \{ \lambda^K(X,k) : k \in \Z \text{ and } |\lambda^K(X,k)| < \delta \}$
\end{itemize}
are separated by at least $\epsilon$, where
\[
\epsilon = \frac{r^{1/2}}{X^{1/2}} = \mathcal{O} \left( \frac{r^{1/2}}{|\log r| } \right)
\]
\end{definition}

The following lemma states that this condition can always be satisfied by taking $r$ sufficiently small

\begin{lemma}\label{epsilonballlemma}
Choose baseline length parameters $b^0 = \{ b_0^0, \dots, b_{n-1}^0 \}$ and phase parameter $\theta$ and use them to construct a periodic multi-pulse according to \cref{perexist}. Then there exists $r(b^0, \theta) \leq r_0$ such that for $r \leq r(b, \theta)$, the $\epsilon-$ball condition is satisfied.
\end{lemma} 

Although we can always choose sufficiently small $r$ so that the $\epsilon$-ball condition is satisfied, this definition. The proof of Lemma \ref{epsilonballlemma} is constructive and guarantees that for all sufficiently small $r$, any purely imaginary interaction eigenvalues will be smaller in magnitude than all nonzero essential spectrum eigenvalues. In the previous section, we discussed Krein bubbles in the periodic 2-pulse, which occur when an essential spectrum eigenvalue passes through an interaction eigenvalue. We would like our theory to handle the case where such a passage has occurred.

With this taken care of, we have the following theorem, which we can to locate the eigenvalues of \eqref{PDEeig3} near the origin.

% eigenvalue location theorem

\begin{theorem}\label{locateeigtheorem}
Assume Hypotheses \ref{Ehyp}, \ref{Hhyp}, \ref{hypeqhyp}, \ref{Qexistshyp}, \ref{H0transversehyp}, \ref{Melnikov2hyp}, and \ref{Adistincteigs}. Let $Q_n(x)$ be a periodic $n-$pulse solution constructed according to Theorem \ref{perexist} with scaling parameter $r \leq r_0$. Let $\delta > 0$ be defined as in Theorem \ref{blockmatrixtheorem}. Then the following are true.

\begin{enumerate}[(i)]

\item There is an eigenvalue at 0 with (at minimum) geometric multiplicity 2 and algebraic multiplicity 3. The corresponding eigenfunctions are the kernel eigenfunction $\partial_x Q_n(x)$ from translation invariance; its generalized kernel eigenfunction $\partial_C Q_n(x)$; and a third kernel eigenfunction $V_n^c(x)$ which is bounded but does not decay exponentially.

\item There exists $r_1 \leq r_0$ such that for every $r \leq r_1$ for which the $\epsilon-$ball condition is satisfied, there are $n - 1$ pairs of interaction eigenvalues given by $\lambda = \pm \lambda^{\text{int}}_j(r)$ for $j = 1, \dots, n-1$, where
\begin{align*}
\lambda^{\text{int}}_j(r) = \sqrt{\frac{\mu_j}{M}} + \mathcal{O}(r^{3/4})
\end{align*}
These interaction eigenvalue pairs are either real or purely imaginary, and by Hamiltonian symmetry, the remainder term cannot move them off of the real or imaginary axis.

\item There exists $r_2 \leq r_1$ such that for every $r \leq r_2$ for which the $\epsilon-$ball condition is satisfied, there are pairs of purely imaginary essential spectrum eigenvalues given by $\lambda = \pm \lambda^{ess}(X,k; r)$ for every positive integer $k$ with $\frac{c \pi k}{X} < \delta$, where
\begin{equation}\label{lambdaess}
\lambda^{ess}(X, k; r) = c \frac{k \pi i }{X} \left( 1 + \mathcal{O}\left( \frac{1}{X} \right)\right) + \mathcal{O}\left( \frac{r^{1/2}}{X} \right)
\end{equation}
In terms of the parameters $r$ and the $b_j$, these are located at approximately
\begin{equation}\label{lambdaessr}
\lambda^{ess}(k; r) = C \frac{k \pi i }{n |\log r| + |\log (b_0 b_1 \cdots b_{n-1})|}  \left( 1 + \mathcal{O}\left( \frac{1}{n |\log r|} \right)\right) + \mathcal{O}\left( \frac{r^{1/2}}{n |\log r|} \right)
\end{equation}
The remainder terms cannot move these off of the imaginary axis.

\item For sufficiently small $r$, we have the following two eigenvalue counts.
\begin{itemize}
	\item There exists a small radius $\xi$ (which excludes the interaction eigenvalues and essential spectrum eigenvalues) such that there are exactly 3 eigenvalues inside the circle of radius $\xi$ in the complex plane. These must be the three eigenvalues from part (i).

	\item There are exactly $2n + 2 k_M + 1$ eigenvalues inside the circle of radius $\tilde{\delta}$ (which may be slightly smaller than $\delta$) in the complex plane, where $k_M$ is the largest positive integer $k$ such that $\lambda^K(k,X) < \tilde{\delta}$. 
\end{itemize}

If the $\epsilon-$ball condition is satisfied, there are no eigenvalues inside the circle of radius $\tilde{\delta}$ other than the ones already accounted for.
\end{enumerate}
\end{theorem}

THIS THEOREM AND PROOF NEEDS TO BE REVISED SINCE I CHANGED THE DEFINITION OF THE EPSILON BALL AS A RESULT OF THE KREIN BUBBLE NUMERICS.

From \cref{locateeigtheorem}(ii), the interaction eigenvalue pattern is determined by the signs of the eigenvalues $\mu_j$. In general, there is no easy way to determine these signs. However, if we take one of the baseline length parameters $b_j^0$ to be small compared to the others, the matrix $A$ is approximately tri-diagonal, in which case we can use a similar argument to \cite[Theorem 3(iv)]{Sandstede1998} to determine the interaction eigenvalue pattern. Since we are on a periodic domain, we can without loss of generality take $b_{n-1}^0$ to be the small parameter. Recall from \cref{perexist} that the other baseline length parameters are given by
\begin{align*}
b_j^0 &= e^{-\frac{1}{\rho}m_j \pi} && j = 0, \dots, n-2
\end{align*}
for nonnegative integers $m_j$. The following theorem gives conditions under which the interaction eigenvalue pattern is determines by the parity of the integers $m_0, \dots, m_{n-2}$.

\begin{theorem}\label{inteigsparity}
Assume Hypotheses \ref{Ehyp}, \ref{Hhyp}, \ref{hypeqhyp}, \ref{Qexistshyp}, \ref{H0transversehyp}, \ref{Melnikov2hyp}, and \ref{Adistincteigs}. Let $r_1$ and $b^*$ be as in Theorem \ref{unifperexist}. Choose
\begin{itemize}
\item An integer $n \geq 2$ 
\item A sequence of $n-1$ baseline length parameters $b_0^0, \dots, b_{n-2}^0$, where 
\[
b_j^0 = \exp\left(-\frac{1}{\rho}m_j \pi\right) \in \mathcal{B}
\]
and the $m_j$ are nonnegative integers.
\item Any phase parameter $\theta$.
\end{itemize}

Then there exists $\tilde{r_1} \leq r_1$ and $\tilde{b}^* \leq b^*$ such that for any $r \leq \tilde{r}_1$ and $b_{n-1}^0 \leq \tilde{b}^*$, we have the following result. 

Let $n_{\text{even}}$ be the number of even $\{m_0, \dots, m_{n-2}\}$ and $n_{\text{odd}}$ be the number of odd $\{m_0, \dots, m_{n-2}\}$. 

\begin{itemize}
\item There are $n_{\text{odd}}$ pairs of purely imaginary interaction eigenvalues.
\item There are $n_{\text{odd}}$ pairs of real of interaction eigenvalues.
\end{itemize}
If $M < 0$, these are reversed.
\end{theorem} 

We note that since we took $b_{n-1}^0$ small, the parity of $m_{n-1}$ is irrelevant to the eigenvalue pattern. In the following sections, we provide proofs for these results.

\section{Proof of \cref{nulambdalemma} }

Using \eqref{defAphi} and \eqref{fpartials0},
\begin{equation}\label{Alambdaform}
A(\lambda) = 
\begin{pmatrix}
0 & 1 & 0 & \dots & 0 & 0 & 0 \\
0 & 0 & 1 & \dots & 0 & 0 & 0\\
& \vdots && \vdots \\
0 & 0 & 0 & \dots & 0 & 1 & 0 \\
- c & 0 & c_3 & \dots & c_{2m-1} & 0 & 1 \\
\lambda & 0 & 0 & \dots & 0 & 0 & 0
\end{pmatrix}
\end{equation}
which has characteristic polynomial
\begin{equation}\label{charpolyA0lambda}
p(\nu; \lambda) = \lambda = -\nu^{2m+1} + c_{2m-1} \nu^{2m-1} + \dots + c_3 \nu^3 - c \nu + \lambda
\end{equation}
When $\lambda = 0$, $p(0; \nu)$ has a root at $\nu = 0$. From Lemma \ref{nondegenlemma}, $\nu = 0$ is simple root and is located a distance $\sqrt{\alpha_0^2 + \beta_0^2}$ from the next smallest roots. 

Since $p(0; 0) = 0$ and $\partial_\nu p(0; 0) = -c \neq 0$, we can use the IFT to can solve for $\nu$ in terms of $\lambda$ near $\lambda = 0$. In other words, there exists $\delta_0 > 0$ and a smooth function $\nu(\lambda)$ such that $\nu(0) = 0$ and for $|\lambda| < \delta$, $p(\nu(\lambda)$ is the unique solution to $p(\nu; \lambda) = 0$. Using the IFT to differentiate $\nu(\lambda)$ at $\lambda = 0$,
\begin{align*}
\nu'(0) &= -\frac{1}{\partial_\nu p_2(\nu(\lambda); \lambda) } \partial_\lambda p ( \nu(\lambda); \lambda ) \Big|_{\lambda = 0}\\
&= -\frac{1}{\partial_\nu p(\nu(0); 0) } \\
&= \frac{1}{c}
\end{align*}
By reversibility, $p(\nu; \lambda)$ only involves odd powers of $\nu$, thus $p(\nu; \lambda) = 0$ implies $p(-\nu; -\lambda) = 0$. Uniqueness of the solution $\nu(\lambda)$ from the IFT implies that $\nu(-\lambda) = -\nu(\lambda)$, i.e. $\nu(\lambda)$ is an odd function of $\lambda$. Since $\nu(\lambda)$ is the root of a polynomial which is smooth in $\lambda$, we can expand $\nu(\lambda)$ in a Taylor series about $\lambda = 0$. Since $\nu(\lambda)$ is an odd function, the Taylor series involves only odd powers of $\lambda$ and is given by
\begin{align*}
\nu(\lambda) = \frac{1}{c} \lambda + \mathcal{O}(|\lambda|^3)
\end{align*}

Next, we consider the case when $\lambda$ is pure imaginary. Take $\lambda = i \gamma$ and $\nu = i s$ in $p(\nu; \lambda)$ to get
\begin{align*}
p(i s; i \gamma) = (-1)^{m+1} i s^{2m+1} + c_{2m-2} (-1)^m i s^{2m-1} + \dots - c_2 i s^3 - c i s + i \gamma
\end{align*}
Since we want to solve $p(i s; i \gamma) = 0$, we can divide by $i$ to get the equivalent problem
\begin{align*}
\tilde{p}(s; \gamma) = (-1)^{m+1} s^{2m+1} + c_{2m-2} (-1)^m s^{2m-1} + \dots - c_2 s^3 - c s + \gamma = 0
\end{align*}
Since $\tilde{p}(0; 0) = 0$ and $\partial_s \tilde{p}(0; 0) = -c \neq 0$, we can use the IFT again to solve for $s$ in terms of $\gamma$. Thus we can find a smooth function $s(\gamma)$ such that $s(0) = 0$, $s(\gamma)$ is real, and $s(\gamma)$ is the unique solution to $\tilde{p}(s; \gamma) = 0$ for sufficiently small $\gamma$. Undoing these substitutions, this implies that $p(\lambda, i s(-i \lambda)) = 0$. By uniqueness of the IFT solution $\nu(\lambda)$, we conclude that $\nu(\lambda) = \nu(i \gamma) = i s(-i \lambda)$, which is pure imaginary.

For the eigenvectors, let $V_0(\lambda)$ be an eigenvector of $A(\lambda)$ corresponding to $\nu(\lambda)$. This is only unique up to scalar multiple, but since $V_0(\lambda)$ is smooth in $\lambda$, scale $V_0(\lambda)$ so that $V(0) = V_0$. We can verify by direct calculation that $A(\lambda) = -R(A(-\lambda)R$, where $R$ is the standard reversor operator. Thus we have
\begin{align*}
[A(\lambda) - \nu(\lambda) I]V = 0 &\iff -R(A(-\lambda) + \nu(\lambda) I)RV = 0 \\
&\iff (A(-\lambda) + \nu(\lambda) I)RV = 0 
\end{align*}
Thus $RV_0(\lambda)$ is an eigenvector of $A(-\lambda)$ corresponding to $-\nu(\lambda)$. Since $V_0(0) = RV_0(0)$, we conclude that $V_0(-\lambda) = R V_0(\lambda)$. Differentiating $A(\lambda)V_0(\lambda) = \nu(\lambda) V_0(\lambda)$ with respect to $\lambda$,
\begin{align*}
A'(\lambda)V_0(\lambda) + A(\lambda)V_0'(\lambda) = \nu'(\lambda)V_0(\lambda) + \nu(\lambda)V_0'(\lambda)
\end{align*}
Evaluating this at $\lambda = 0$ and using $A'(\lambda) = b$, $\nu(0) = 0$, and $\nu'(0) = 0$, this becomes
\begin{align*}
B V_0(0) + A(0)V_0'(0) = \frac{1}{c} V_0(0)
\end{align*}
which rearranges to get
\begin{align*}
A(\lambda)V_0'(0) = -B V_0(0) + \frac{1}{c} V_0(0)
= (1/c^2, 0, \dots, 0)^T
\end{align*}
Using \eqref{Alambdaform} for $A(\lambda)$, this has a solution $V_0'(0) = (0, 1/c^2, 0, \dots, 0)^T$ which is orthogonal to $V_0$. Thus we have the Taylor expansion
\[
V_0(\lambda) = V_0 + \lambda V_0'(0) + \mathcal{O}(\lambda^2),
\]

We can follow the same procedure with the adjoint asymptotic matrix $-A(\lambda)^*$, which has an eigenvalue $-\overline{\nu(\lambda)}$ that is smooth in $\overline{\lambda}$. To find $W_0'(\overline{\lambda})$, we follow the same procedure as above to get the equation
\begin{align*}
A(0)^* W_0'(0) = \frac{1}{c} W_0(0) - B^* W_0(0)
= (-1, 0, \dots, 0, 1/c)
\end{align*}
which has a solution 
\[
W_0'(0) = \frac{1}{c} \left( 0, -c_3, 0, -c_5, 0, \dots, 0, -c_{2m-1}, 0, 1, 0\right)^T
\]
that is orthogonal to $W_0$.

\section{Piecewise Formulation}

In this section, we write the eigenvalue problem \cref{PDEeig3} in a piecewise form similar to \cite{Sandstede1998} and what we did in \cref{sec:kdvRpiecewise}. Let $Q_n(x)$ be a periodic $n$-pulse solution to \eqref{genODE} constructed according to Theorem \ref{perexist}. From Theorem \ref{perexist}, we can write $Q_n(x)$ piecewise as
\begin{equation}\label{Qnppiece}
\begin{aligned}
Q_i^-(x) &= Q^-(x; \beta_i^-) + \tilde{Q}_i^-(x) && x \in [-X_{i-1}, 0] \\
Q_i^+(x) &= Q^+(x; \beta_i^+) + \tilde{Q}_i^+(x) && x \in [0, X_i]
\end{aligned}
\end{equation}
where $Q_i^-: [-X_{i-1}, 0] \rightarrow \R$ and $Q_i^+: [0, X_i]$. We note that this is different notation than used in \cref{chapter:kdv5periodic}

Similarly to \eqref{Qprimevarsol} and \eqref{Qcvarsol}, we have
\begin{equation}\label{Arelations}
\begin{aligned}
(\partial_x Q_n(x))' &= A(Q_n(x)) (\partial_x Q_n(x)) \\
(\partial_c Q_n(x))' &= A(Q_n(x)) (\partial_c Q_n(x)) + B (\partial_x Q_n(x)) 
\end{aligned}
\end{equation}
To exploit the relations \eqref{Arelations}, we take the following piecewise ansatz for the eigenfunction $V(x)$
\begin{equation}\label{Vpiecewise}
\begin{aligned}
V_i^-(x) &= d_i (\partial_x Q_i^-(x) + \lambda \partial_c Q_i^-(x)) + W_i^-(x) && x \in [-X_{i-1}, 0] \\
V_i^+(x) &= d_i (\partial_x Q_i^+(x) + \lambda \partial_x Q_i^+(x)) + W_i^+(x) && x \in [0, X_i] 
\end{aligned}
\end{equation}
where $d_i \in \C$ are constants. Substituting this ansatz into \eqref{PDEeig3} and simplifying, we obtain the following equation for $W_i^\pm(x)$.
\begin{equation}\label{Wipm1}
(W_i^\pm)'(x) = A(Q_i^\pm(x); \lambda) W_i^\pm(x) + d_i \lambda^2 B (\partial_c Q_i^\pm(x))
\end{equation}
Since $A(Q(x))$ is smooth in $Q(x)$, let 
\begin{align*}
G_i^\pm(x) = A(Q_i^\pm(x)) - A(Q(x))
\end{align*}
Finally, let
\begin{align*}
\tilde{H}_i^\pm(x) &= \partial_c Q_i^\pm(x) \\ 
H(x) &= \partial_c Q(x)
\end{align*}
Using these, equation \eqref{Wipm1} simplifies to
\begin{align}\label{Wipm2}
(W_i^\pm)'(x) &= A(Q(x); \lambda) W_i^\pm(x) + G_i^\pm(x) W_i^\pm(x) + d_i \lambda^2 \tilde{H}_i^\pm(x)
\end{align}

In order to have a solution to \eqref{PDEeig3}, the eigenfunction $V(x)$ must be continuous. In other words, the $n$ jumps at $\pm X_i$ and the $n$ jumps at $0$ must be 0. Thus an eigenfunction must solve the system of equations
\begin{align*}
(W_i^\pm)'(x) &= A(Q(x); \lambda) W_i^\pm(x) + G_i^\pm(x) W_i^\pm(x) + d_i \lambda^2 \tilde{H}_i^\pm(x) \\
W_i^+(X_i) - W_{i+1}^-(-X_i) &= D_i d && i = 0, \dots, n-1 \\
W_i^-(0) &= W_i^+(0) && i = 0, \dots, n-1  \\
\end{align*}
where
\begin{align}\label{defDid}
D_i d &= d_{i+1}[\partial_x Q_{i+1}^-(-X_i) + \lambda \partial_c Q_{i+1}^-(-X_i)]
- d_i [ \partial_x Q_i^+(X_i) + \lambda \partial_c Q_i^+(X_i) ] \\
\end{align}

As in \cite{Sandstede1998}, we will not be able to find a solution to this system for arbitrary $\lambda$. Thus we will instead consider the system
\begin{equation}\label{eigsystem}
\begin{aligned}
(W_i^\pm)'(x) &= A(Q(x); \lambda) W_i^\pm(x) + G_i^\pm(x) W_i^\pm(x) + d_i \lambda^2 \tilde{H}_i^\pm(x) \\
W_i^+(X_i) - W_{i+1}^-(-X_i) &= D_i d && i = 0, \dots, n-1 \\
W_i^\pm(0) &\in \R \Psi(0) \oplus \R \Psi^c(0) \oplus Y^+ \oplus Y^- && i = 0, \dots, n-1  \\
W_i^+(0) - W_i^-(0) &\in \R \Psi(0) \oplus \R \Psi^c(0) && i = 0, \dots, n-1 
\end{aligned}
\end{equation}
where the center matching condition $W_i^-(0) = W_i^+(0)$is relaxed to the requirement that the jumps $W_i^+(0) - W_i^-(0)$ can only be in the directions of $\Psi^c(0)$ and $\Psi(0)$. We do not need to include a component in $Q'(0)$ in the third equation since that is handled by the term $d_i (Q_i^\pm)'(x)$ in the ansatz \eqref{Vpiecewise}. A solution to \eqref{eigsystem} solves \eqref{PDEeig3} if and only if the $n$ jumps at $x = 0$ in the direction of $\Psi(0) \oplus \R \Psi^c(0)$ are 0, i.e.  
\begin{equation}\label{jumpxi}
\begin{aligned}
\xi_i &= \langle \Psi(0), W_i^+(0) - W_i^-(0) \rangle = 0  \\
\xi_i^c &= \langle \Psi^c(0), W_i^+(0) - W_i^-(0) \rangle = 0 
\end{aligned}
\end{equation}
for $i = 0, \dots, n-1$.

Before we continue, we define the following constants, which we will use throughout. By Lemma \ref{eigA0lemma}, $A(0)$ has a simple eigenvalue at 0, a quartet of eigenvalues $\pm \alpha_0 \pm \beta_0 i$, and for all other eigenvalues $\nu$ of $A(0)$, $|\Re \nu| > \alpha_0$. 

\begin{enumerate}
	\item Choose $\alpha_1$ slightly smaller than $\alpha_0$.

	\item Choose $\eta > 0$ sufficiently small so that $\alpha_1 - 4 \eta > 0$.

	\item Let
	\begin{align*}
	\alpha &= \alpha_1 - \eta \\
	\end{align*}

	\item Choose $\delta$ sufficiently small so that for all $|\lambda| < \delta$
	\begin{enumerate}
		\item $|\nu(\lambda)| < \eta$, where $\nu(\lambda)$ defined in Lemma \ref{nulambdalemma}.

		\item The real part of any other eigenvalue of $A(\lambda)$ lies outside the interval $[-\alpha, \alpha]$. This is possible since the eigenvalues of $A(\lambda)$ are smooth functions of $\lambda$.
	\end{enumerate}
	As we proceed we may need to decrease $\delta$. 

	\item Choose $X_m$ sufficiently large so that
	\begin{equation}
	e^{-(\alpha - \eta) X_m} < \delta
	\end{equation}
	where $X_m = \min\{X_0, \dots, X_{n-1}\}$.
\end{enumerate}

To conclude this section, we will collect estimates on the terms involved in \eqref{eigsystem} in the following lemma.

\begin{lemma}\label{stabestimateslemma}
Let $\Delta H_i^\pm(x) = \tilde{H}_i^\pm(x) - H(x)$. Then we have the following estimates
\begin{enumerate}[(i)]
\item $|H(x)|, |\tilde{H}_i^\pm(x)| \leq C e^{-\alpha_1 |x|}$
\item $|\Delta H_i^-(x)| \leq C e^{-\alpha_1 X_{i-1}} e^{-\alpha_1(X_{i-1} + x) } + e^{-2 \alpha_1 X_i} e^{\alpha_1 x}$
\item $|\Delta H_i^+(x)| \leq C e^{-\alpha_1 X_i} e^{-\alpha_1(X_i - x) } + e^{-2 \alpha_1 X_{i-1}} e^{-\alpha_1 x}$
\item $||\Delta H_i^\pm|| \leq C(e^{-\alpha_1 X_i} + e^{-\alpha_1 X_{i-1}} )$
\item $|G_i^-(x)| \leq C e^{-\alpha_1 X_{i-1}} e^{-\alpha_1(X_{i-1} + x) } + e^{-2 \alpha_1 X_i} e^{\alpha_1 x}$
\item $|G_i^+(x)| \leq C e^{-\alpha_1 X_i} e^{-\alpha_1(X_i - x) } + e^{-2 \alpha_1 X_{i-1}} e^{-\alpha_1 x}$
\item $||G|| \leq C e^{-\alpha_1 X_m}$
\item $D_i d = ( Q'(X_i) + Q'(-X_i))(d_{i+1} - d_i ) + \mathcal{O} ( e^{-\alpha_1 X_i} (e^{-\alpha_1 X_m} + |\lambda| )$
\end{enumerate}
\begin{proof}
Recall that $H(x) = \partial_c Q(x)$ and $\tilde{H}(x) = \partial_c Q_i^\pm(x)$. For (i), by Theorem \ref{transverseint}, $H(x)$ is exponentially localized for any $\alpha_1 < \alpha_0$. The result for $\tilde{H}(x)$ is a straightforward adaptation of Theorem \ref{transverseint}. The bounds (ii) and (iii) follow from an adaptation of Lemma \ref{solvewithjumps} to derivative with respect to $c$. The bound (iv) follows from (ii) and (iii). Since $A(Q(x))$ is smooth in $Q(x)$, the bounds (v), (vi), and (vii) come from Lemma \ref{solvewithjumps}. For the estimate (viii), we use \eqref{VQpm} in Lemma \ref{solvewithjumps} and \eqref{Vpiecewise} (with $\tilde{Q}$ in place of $V$) to get
\begin{align*}
Q_{i+1}^-(-X_i) &= Q^-(-X_i; \beta_{i+1}^-) + \tilde{Q}_{i+1}^-(-X_i) \\
&= Q^-(-X_i; \beta_{i+1} ^-) + Q^+(X_i; \beta_i^+) + \mathcal{O}(e^{-2 \alpha_1 X_i}) \\
&= Q(-X_i) + Q(X_i) 
+ \mathcal{O}(e^{-\alpha_1 X_i}(e^{-\alpha_1 X_{i-1}}+e^{-\alpha_1 X_i}+e^{-\alpha_1 X_{i+1}}))
\end{align*}
We note that $\alpha_1$ corresponds to $\alpha$ in that lemma. Since the bounds \ref{solvewithjumps} also apply to derivatives with respect to $x$, we have
\begin{align*}
(Q_{i+1}^-)'(-X_i) &= Q'(-X_i) + Q'(X_i) + \mathcal{O}(e^{-\alpha_1 X_i}e^{-\alpha_1 X_m})
\end{align*}
Similarly,
\begin{align*}
(Q_i^+)'(X_i)' &= Q'(-X_i) + Q'(X_i) + \mathcal{O}(e^{-\alpha_1 X_i}e^{-\alpha_1 X_m})
\end{align*}
To get the estimate (v), we substitute these into \cref{defDid} and use estimate (i) for $\partial_c Q_i^\pm(x)$.
\end{proof}
\end{lemma}

\section{Gap and conjugation Lemmas}

In this section, we state and prove the gap and conjugation lemmas. We will use the conjugation lemma to simplify the system of equations \cref{eigsystem}. The statements and notation of these lemmas are very similar to those in \cite{Zumbrun2009}, except we allow the parameter vector $\Lambda$ to live in a Banach space rather than in a subset of $\C^m$. The proofs follow those in \cite{Zumbrun2009}, but are given in much more detail. In addition, the proof of the conjugation lemma is used to prove a corollary. First, we state and prove the gap lemma.

% Gap lemma
\begin{lemma}[Gap Lemma]\label{gaplemma}
Let $W \in \C^N$, and consider the family of ODEs on $\R$
\begin{equation}\label{LambdaEVP}
W(x)' = A(x; \Lambda) W
\end{equation}
where $\Lambda \in \Omega$ is a parameter vector and $\Omega$ is a Banach space. Assume that
\begin{enumerate}
	\item The map $\Lambda \mapsto A(\cdot; \Lambda)$ is analytic in $\Lambda$.
	\item $A(x; \Lambda) \rightarrow A_\pm(\lambda)$ (independent of $\Lambda$) as $x \rightarrow \pm \infty$, and there exists $\delta > 0$ such that for $|\Lambda| < \delta$ we have the uniform exponential decay estimates 
	\begin{align}\label{ALambdadecay}
	\left| \frac{\partial^k}{\partial x^k} A(x; \Lambda) - A_\pm(\Lambda) \right| 
	&\leq C e^{-\theta |x|} && 0 \leq k \leq K
	\end{align}
	where $\alpha > 0$, $C > 0$, and $K$ is a nonnegative integer.
\end{enumerate}
Suppose $V^-(\Lambda)$ is an eigenvector of $A_-(\Lambda)$ with corresponding eigenvalue $\mu(\Lambda)$, both analytic in $\Lambda$. Then there exists a unique solution of \ref{LambdaEVP} of the form 
\begin{equation}
W(x; \Lambda) = V(x; \Lambda) e^{\mu(\Lambda)x}
\end{equation}
where $V$ is $C^1$ in $x$ and analytic in $\Lambda$ for $|\Lambda| < \delta$, and for any fixed $\tilde{\theta} < \theta$
\begin{align}
V(x; \Lambda) = V^-(\Lambda) + \mathcal{O}(e^{-\tilde{\theta}|x|}|V^-(\Lambda)|) && x \in \R^-
\end{align}
A similar result holds for $A_+(\lambda)$ on $\R^+$.

\begin{proof}
This proof follows \cite{Zumbrun2009}, with the main difference being that the parameter vector $\Lambda$ is in a general Banach space instead of a subset of $C^p$. Let $W(x; \Lambda) = V(x; \Lambda) e^{\mu(\Lambda) x}$. Substituting this into \eqref{LambdaEVP} and simplifying, we obtain the equivalent ODE
\begin{equation}\label{VEVP}
V(x; \Lambda)' = (A_-(\Lambda) - \mu(\Lambda)I)V(x; \Lambda) + \Theta(x; \Lambda) V(x; \Lambda)
\end{equation}
where $\Theta(x; \Lambda) = (A(x; \Lambda) - A_-(\Lambda)) = \mathcal{O}(e^{-\theta|x|})$ by \eqref{ALambdadecay}. Choose any $\tilde{\theta} < \theta_1 < \theta$ such that for $|\Lambda| < \delta$, the real part of the spectrum of $A_-(\Lambda)$ lies either to the left or to the right of the vertical line $\text{Re}(\nu) = \text{Re}(\mu(\Lambda) + \theta_1$ in the complex plane. Since the eigenvalues of $A_(\Lambda)$ are analytic in $\Lambda$, this is possible.

For $|\Lambda| < \delta$, define the spectral projections $P(\Lambda)$ and $Q(\Lambda)$, where $P(\Lambda)$ projects onto the direct sum of all eigenspaces of $A_-(\Lambda)$ corresponding to eigenvalues $\nu$ with $\text{Re}(\nu) < \text{Re}(\mu(\Lambda) + \theta_1$, and $Q(\Lambda)$ projects onto the direct sum of all eigenspaces of $A_-(\Lambda)$ corresponding to eigenvalues $\nu$ with $\text{Re}(\nu) > \text{Re}(\mu(\Lambda) + \theta_1$. $P(\Lambda)$ and $Q(\Lambda)$ are analytic in $\Lambda$ for $|\Lambda| < \delta$, and from the definition of $\theta_1$, we have the estimates
\begin{align*}
\left|e^{(A_-(\Lambda) - \mu(\Lambda)I)x}P \right| &\leq C e^{\theta_1 x} && x \geq 0 \\
\left|e^{(A_-(\Lambda) - \mu(\Lambda)I)x}Q \right| &\leq C e^{\theta_1 x} && x \leq 0
\end{align*}
Note that $P(\Lambda) + Q(\Lambda) = I$. Define the map $T$ on $L^\infty(-\infty, -M]$ by
\begin{align*}
TV(x; \Lambda) &= V^-(\Lambda) 
+ \int_{-\infty}^x e^{(A_-(\Lambda) - \mu(\Lambda)I)(x-y)}P\Theta(y; \Lambda) V(y; \Lambda) dy \\
&- \int_x^{-M} e^{(A_-(\Lambda) - \mu(\Lambda)I)(x-y)}Q\Theta(y; \Lambda) V(y; \Lambda) dy
\end{align*}
Taking the absolute value of both sides, for $x \leq 0$
\begin{align*}
|TV(x; \Lambda)| &\leq |V^-(\Lambda)| + C ||V(x; \Lambda)||_{L^\infty(-\infty, -M]}
\left( \int_{-\infty}^x e^{\theta_1 (x - y)} e^{\theta y} dy + \int_x^{-M} e^{\theta_1 (x - y)} e^{\theta y} dy \right) \\
&\leq |V^-(\Lambda)| + C ||V(x; \Lambda)||_{L^\infty(-\infty, -M]} e^{\theta_1 x} \int_{-\infty}^M e^{(\theta - \theta_1) y} dy \\
&= \leq |V^-(\Lambda)| + C ||V(x; \Lambda)||_{L^\infty(-\infty, -M]} e^{\theta_1 x} \frac{e^{-(\theta - \theta_1)M}}{\theta - \theta_1}\\
&\leq |V^-(\Lambda)| + C ||V(x; \Lambda)||_{L^\infty(-\infty, -M]} e^{\theta_1 x} e^{-(\theta - \theta_1)M} \\
&\leq |V^-(\Lambda)| + C ||V(x; \Lambda)||_{L^\infty(-\infty, -M]} e^{-(\theta - \theta_1)M} \\
& < \infty
\end{align*}
Since the RHS is independent of $x$, we have $T: L^\infty(-\infty, -M] \rightarrow L^\infty(-\infty, -M]$. 

Next, we show $T$ is a contraction.
\begin{align*}
|TV_1(x; \Lambda) - TV_2(x; \Lambda)| &\leq C ||V_1(x; \Lambda) - V_2(x; \Lambda)||_{L^\infty(-\infty, -M]} e^{\theta_1 x} \frac{e^{-(\theta - \theta_1)M}}{\theta - \theta_1}\\
\end{align*}
Since $e^{-(\theta - \theta_1)M} \rightarrow 0$ as $m \rightarrow \infty$, for sufficiently large $M$ we have 
\begin{align*}
|TV_1(x; \Lambda) - TV_2(x; \Lambda)|_{L^\infty(-\infty, -M]} &\leq \frac{1}{2} ||V_1(x; \Lambda) - V_2(x; \Lambda)||_{L^\infty(-\infty, -M]} 
\end{align*}
Thus the map $T$ is a contraction. Since $L^\infty(-\infty, -M]$ is a Banach space, by the Banach fixed point theorem, the map $T$ has a unique fixed point $V = TV$, i.e. we have a function $V \in L^\infty(-\infty, -M]$ such that 
\begin{align*}
V(x; \lambda) &= V^-(\Lambda) 
+ \int_{-\infty}^x e^{(A_-(\Lambda) - \mu(\Lambda)I)x}P\Theta(y; \Lambda) V(y; \Lambda) dy 
- \int_x^{-M} e^{(A_-(\Lambda) - \mu(\Lambda)I)x}Q\Theta(y; \Lambda) V(y; \Lambda) dy
\end{align*}

Differentiating this with respect to $x$, we obtain
\begin{align*}
V'(x; \Lambda) &= P\Theta(x; \Lambda) V(x; \Lambda) +
(A_-(\Lambda) - \mu(\Lambda)I) \int_{-\infty}^x e^{(A_-(\Lambda) - \mu(\Lambda)I)(x-y)}P\Theta(y; \Lambda) V(y; \Lambda) dy \\
&-(-Q\Theta(x; \Lambda) V(x; \Lambda))
-(A_-(\Lambda) - \mu(\Lambda)I) \int_x^{-M} e^{(A_-(\Lambda) - \mu(\Lambda)I)(x-y)}Q\Theta(y; \Lambda) V(y; \Lambda) dy \\
&= P\Theta(x; \Lambda) V(x; \Lambda) + Q\Theta(y; \Lambda) V(x; \Lambda) + (A_-(\Lambda) - \mu(\Lambda)I)(T V(x; \lambda) - V^-(\Lambda) ) \\
&= (P + Q)\Theta(x; \Lambda) V(x; \Lambda) + (A_-(\Lambda) - \mu(\Lambda)I)(V(x; \lambda) - V^-(\Lambda) ) \\
&= (A_-(\Lambda) - \mu(\Lambda)I)V(x; \lambda) + \Theta(x; \Lambda) V(x; \Lambda) - (A_-(\Lambda) - \mu(\Lambda)I)V^-(\Lambda) \\
&= (A_-(\Lambda) - \mu(\Lambda)I)V(x; \lambda) + \Theta(x; \Lambda) V(x; \Lambda)
\end{align*}
where we used the fact that $TV = V$ and $(A_-(\Lambda) - \mu(\Lambda)I)V^-(\Lambda) = 0$. Thus $V(x; \Lambda$ solves \eqref{VEVP}. Since $TV = V$, we let $V_1 = V$ and $V_2 = 0$ in the above to get the estimate
\begin{align*}
|V(x; \Lambda) - V^-(\Lambda)| &= |T(V(x; \Lambda)) - T(0)| \\
&\leq C ||V(x; \Lambda) - 0||_{L^\infty(-\infty, -M]} e^{\theta_1 x} \\
\end{align*}

Similarly, for sufficiently large $M$, we have
\begin{align*}
|V(x; \Lambda)| - |V^-(\Lambda)| &\leq | |V(x; \Lambda)| - |V^-(\Lambda)| | \\
&\leq |V(x; \Lambda) - V^-(\Lambda)| \\
&= |T(V(x; \Lambda)) - T(0)| \\
&\leq \frac{1}{2} ||V(x; \Lambda)||_{L^\infty(-\infty, -M]}
\end{align*}
thus it follows that
\begin{align*}
||V(x; \Lambda)||_{L^\infty(-\infty, -M]} \leq 2 |V^-(\Lambda)|
\end{align*}

Combining these, we have
\begin{align*}
|V(x; \Lambda) - V^-(\Lambda)| &\leq C e^{\tilde{\theta} x}|V^-(\Lambda)| \\
\end{align*}
from which we get
\begin{align*}
|V(x; \Lambda) = V^-(\Lambda) + \mathcal{O}( e^{\tilde{\theta} x}|V^-(\Lambda)| )\\
\end{align*}

Although $V(x; \Lambda)$ is only defined for $x \leq -M$, we extend $V(x; \Lambda)$ to all of $R^-$ using the evolution operator for the system.
\end{proof}
\end{lemma}

As a corollary to this, we state and prove the Conjugation Lemma, which allows us to make a smooth change of coordinates to convert the linear ODE $W'(x) = (x) W(x)$ into a constant coefficient system.

% Conjugation lemma
\begin{lemma}[Conjugation Lemma]
Let $W \in \C^N$, and consider the family of ODEs on $\R$
\begin{equation}\label{EVPconj}
W(x)' = A(x; \Lambda) W(x) + G(x)W(x) + F(x)
\end{equation}
where $\Lambda \in \Omega$ is a parameter vector and $\Omega$ is a Banach space. Take the same assumptions as in the Gap Lemma, i.e. 
\begin{enumerate}
	\item The map $\Lambda \mapsto A(\cdot; \Lambda)$ is analytic in $\Lambda$.
	\item $A(x; \Lambda) \rightarrow A_\pm(\lambda)$ (independent of $\Lambda$) as $x \rightarrow \pm \infty$, and there exists $\delta > 0$ such that for $|\Lambda| < \delta$ we have the uniform exponential decay estimates 
	\begin{align}
	\left| \frac{\partial^k}{\partial x^k} A(x; \Lambda) - A_\pm(\Lambda) \right| 
	&\leq C e^{-\theta |x|} && 0 \leq k \leq K
	\end{align}
	where $\alpha > 0$, $C > 0$, and $K$ is a nonnegative integer.
\end{enumerate}
Then in a neighborhood of any $\Lambda_0 \in \Omega$ there exist invertible linear transformations
\begin{equation}\label{conjlemmaP}
\begin{aligned}
P_+(x, \Lambda) &= I + \Theta_+(x, \Lambda) \\
P_-(x, \Lambda) &= I + \Theta_-(x, \Lambda) 
\end{aligned}
\end{equation}
defined on $\R^+$ and $\R^-$, respectively, such that
\begin{enumerate}[(i)]
\item The change of coordinates $W = P_\pm Z$ reduces \eqref{EVPconj} to the equations on $\R^\pm$
\begin{align}\label{conjZ}
Z'(x) = A^\pm(\Lambda) Z(x) + P_\pm(x, \Lambda)^{-1} G(x) P_\pm(x, \Lambda) + P_\pm(x, \Lambda)^{-1} F(x)
\end{align}

\item For any fixed $0 < \tilde{\theta} < \theta$, $0 \leq k \leq K+1$, and $j \geq 0$ we have the decay rates
\begin{align}\label{conjthetadecay}
\left| \partial_\Lambda^j \partial_x^k \Theta_\pm \right| \leq C(j, k)e^{-\tilde{\theta}|x|}
\end{align}
\end{enumerate}
\begin{proof}
We will prove this for the case where $F(x) = 0$ and $G(x) = 0$. The form of the conjugated system easily follows for general $F$ and $G$. We will also only consider the case on $\R^-$. The other case is similar. 

Let $W = P_-(x, \Lambda) Z$, where we will determine $P_-(x, \Lambda)$ later. Suppose that \cref{conjZ} is true. Substituting $W = P_-(x, \Lambda) Z$ into \eqref{EVPconj} and simplifying, we get
\begin{align*}
[P_-(x, \Lambda) Z(x)]' &= A(x; \Lambda)(P_-(x, \Lambda) Z(x)) \\
P_-'(x, \Lambda) Z(x) + P_-(x, \Lambda) Z'(x)
&= A(x; \Lambda)P_-(x, \Lambda) Z(x) \\
P_-'(x, \Lambda) Z(x) + P_-(x, \Lambda) A_-(\Lambda) Z(x)
&= A(x; \Lambda)P_-(x, \Lambda) Z(x)
\end{align*}
where we used \cref{conjZ} in the third line. Rearranging this, we obtain
\begin{equation*}
P_-'(x, \Lambda) Z(x)
= [A(x; \Lambda)P_-(x, \Lambda) - P_-(x, \Lambda) A_-(
\Lambda)]Z(x)
\end{equation*}
This suggests that $P_-(x, \Lambda)$ should satisfy the ODE
\begin{align}\label{Pprimecriterion}
P_-'(x, \Lambda) = A(x; \Lambda)P_-(x, \Lambda) - P_-(x, \Lambda) A_-(\Lambda)
\end{align}

Suppose that \cref{Pprimecriterion} is true. Substituting $W = P_-(x, \Lambda) Z$ and using \cref{Pprimecriterion}, equation \cref{EVPconj} reduces to
\begin{align*}
[P_-(x, \Lambda) Z(x)]' &= A(x; \Lambda)(P_-(x, \Lambda) Z(x)) \\
P_-'(x, \Lambda) Z(x) + P_-(x, \Lambda) Z'(x)
&= A(x; \Lambda)P_-(x, \Lambda) Z(x) \\
(A(x; \Lambda)P_-(x, \Lambda) - P_-(x, \Lambda) A_-)Z(x) + P_-(x, \Lambda) Z'(x)
&= A(x; \Lambda)P_-(x, \Lambda) Z(x) \\
A(x; \Lambda)P_-(x, \Lambda)Z(x) - P_-(x, \Lambda) A_- Z(x) + P_-(x, \Lambda) Z'(x)
&= A(x; \Lambda)P_-(x, \Lambda) Z(x) \\
P_-(x, \Lambda) Z'(x) &= P_-(x, \Lambda) A_- Z(x) 
\end{align*}
If $P_-(x, \Lambda)$ is invertible, then $Z'(x) = A_- Z(x)$.

Thus all that remains is to find an invertible operator $P_-(x, \Lambda)$ which satisfies \cref{Pprimecriterion}. Equation \cref{Pprimecriterion} has the form 
\begin{equation}\label{solvePminus}
P_-'(x, \Lambda) = \mathcal{A}(x; \Lambda) P_-(x, \Lambda)
\end{equation}
where $\mathcal{A}(x; \Lambda)$ is the linear operator
\[
\mathcal{A}(x; \Lambda) P = A(x; \Lambda) P - P A_-(\Lambda)
\]
By our assumptions on $A(x; \Lambda)$, $\mathcal{A} \rightarrow \mathcal{A}_-$ as $x \rightarrow -\infty$, where the limiting linear operator $\mathcal{A}_-$ is defined by
\[
\mathcal{A}_- P = A_-(\Lambda) P - P A_-(\Lambda)
\]
The limiting operator has analytic eigenvalue/eigenvector pair $0, I$ for all $\Lambda$, thus by the Gap Lemma, there exists a solution of \eqref{solvePminus} of the form 
\begin{equation*}
P_-(x, \Lambda) = I + \mathcal{O}(e^{-\tilde{\theta}|x|})
\end{equation*}
In other words, 
\begin{equation*}
P_-(x, \Lambda) = I + \Theta_-(x, \Lambda)
\end{equation*}
where 
\begin{equation}\label{Thetabound}
|\Theta_-(x, \Lambda)| \leq C e^{-\tilde{\theta}|x|}
\end{equation}
The $x$-derivative bound follow from the derivative bounds in the Gap Lemma, and the $\Lambda$-derivative bounds follow from standard analytic function theory.

Finally, we need to show that $P_-(x, \Lambda)$ is invertible for all $x \in \R^-$. Using \eqref{Thetabound}, we can find $M$ sufficiently large and negative such that for all $x \leq M$,
\[
|\Theta_-(x, \Lambda)| < 1/2
\]
It follows that $P_-(x, \Lambda)$ is invertible for $X \leq M$. To extend invertibility to all $x \in \R^-$, suppose that $P_-(x, \Lambda)^{-1}$ exists for all $x \in R^-$. Then, differentiating $P_-(x, \Lambda)^{-1} P_-(x, \Lambda) = I$ and solving for $[P_-(x, \Lambda)^{-1}]'$ (as in the proof of the inverse function theorem), we have (suppressing the dependence on $\Lambda$ for convenience)
\begin{align*}
(P_-^{-1})'(x) &= -P_-^{-1}(x)P_-'(x)P_-^{-1}(x) \\
&= -P_-^{-1}(x)( A(x)P_-(x) - P_-(x) A_-(\Lambda))P_-^{-1}(x) \\
&= A_-(\Lambda) P_-^{-1}(x) - A(x) P_-^{-1}(x)
\end{align*}
We have a solution to this ODE for $x \leq M$, and by variation of constants, this ODE has a unique solution for all $x \in \R^-$. Thus $P_-(x, \Lambda)^{-1}$ is obtained for all $x \in \R^-$ by evolving this ODE forward from an initial condition at some $x \leq M$. In this manner, we have shown that $P_-(x, \Lambda)^{-1}$ exists for all $x \in \R^-$.
\end{proof}
\end{lemma}

As a corollary, we find a conjugation operator for the adjoint equation.

\begin{corollary}\label{corr:adjconj}
Take the same hypotheses as in the conjugation lemma, and let $P_\pm(x; \Lambda)$ be the conjugation operators for $V(x)' = A(x; \Lambda) V(x)$ on $\R^\pm$. Then for the adjoint equation
\begin{equation}\label{conjadjW}
W'(x) = -A(x; \Lambda)^* W(x),
\end{equation}
the change of coordinates $W = (P_\pm^{-1})^* Z$ reduces \cref{conjadjW} to the equations
\begin{align}\label{ZconjW}
Z'(x) = -A^\pm(\Lambda)^* Z(x) 
\end{align}
on $\R^\pm$.
\begin{proof}
For convenience, let $Y(x) = W(x)^T$ and $S(x) = Z(x)^T$. Taking the transpose, equation \cref{conjadjW} is equivalent to
\begin{equation}\label{adjY}
Y'(x) = -Y(x)A^\pm(x; \Lambda) 
\end{equation}
As in the conjugation lemma, we will only consider the case on $\R^-$. By differentiating $P_-^{-1} P_- = I$, we have the relation
\begin{equation}\label{Pprimeinv}
[P_-^{-1}]' = -P_-^{-1} P_-' P_-^{-1}.
\end{equation}
Substitute $Y(x) = S(x) P^{-1}(x, \Lambda)$ into \cref{adjY}, which is equivalent to substituting $W(x) = [P^{-1}(x, \Lambda)]^* Z(x)$ into \cref{conjadjW}.
\begin{align*}
S'(x) P_-^{-1}(x, \Lambda) + S(x) P_-^{-1}(x, \Lambda)'
&= -S(x) P_-^{-1}(x, \Lambda) A(x, \Lambda)
\end{align*}
Using \cref{Pprimeinv}, this becomes
\begin{align*}
S'(x) P_-^{-1}(x, \Lambda) - S(x) P_-^{-1}(x, \Lambda) P_-'(x, \Lambda) P_-^{-1}(x, \Lambda)
&= -S(x) P_-^{-1}(x, \Lambda) A(x, \Lambda)
\end{align*}
Using \cref{Pprimecriterion} from the conjugation lemma, 
\begin{align*}
S'(x) P_-^{-1}(x, \Lambda) - S(x) P_-^{-1}(x, \Lambda) \left(A(x; \Lambda)P_-(x, \Lambda) - P_-(x, \Lambda) A_-(\Lambda)\right) P_-^{-1}(x, \Lambda)
&= -S(x) P_-^{-1}(x, \Lambda) A(x, \Lambda)
\end{align*}
which simplifies to
\begin{align*}
S'(x) P_-^{-1}(x, \Lambda) - S(x) P_-^{-1}(x, \Lambda) A(x; \Lambda) + S(x) A_-(\Lambda) P_-^{-1}(x, \Lambda)
&= -S(x) P_-^{-1}(x, \Lambda) A(x, \Lambda)
\end{align*}
The second term on the LHS cancels, leaving us with
\begin{align*}
S'(x) P_-^{-1}(x, \Lambda) &= -S(x) A_-(\Lambda) P_-^{-1}(x, \Lambda)
\end{align*}
Applying $P_-(x, \Lambda)$ on the right and taking the transpose, this becomes
\[
Z'(x) = -A_-(\Lambda)^* Z(x)
\]
\end{proof}
\end{corollary}

\section{Conjugation of system}

In this section, we apply the conjugation lemma to the system of equations \cref{eigsystem}. This simplifies the system by applying a change of coordinates that transforms the linear operator $A(Q(x); \lambda)$ into a constant coefficient matrix. For all $\lambda$, $A(Q(x); \lambda)$ decays exponentially to the constant-coefficient matrix $A(\lambda)$. Specifically, we have the exponential decay rate
\[
|A(Q(x); \lambda) - A(\lambda)| \leq C e^{-\alpha_0 |x|}
\]

Using the conjugation lemma on $\R^+$ with $\Lambda = \lambda$ and $\Lambda_0 = 0$, there exists $\delta_1 > 0$ and an invertible linear transformation 
\[
P_+(x; \lambda) = I + \Theta_+(x; \Lambda)
\]
such that for all $|\lambda| < \delta_1$, the change of coordinates $W_i^+ = P_+(x; \lambda) Z_i^+$ conjugates \eqref{Wipm2} into the equation
\begin{align*}\label{Zplus1}
(Z_i^+)'(x) &= A(\lambda) Z_i^+(x) + P_+(x; \lambda)^{-1} G_i^+(x) P_+(x; \lambda) Z_i^+(x) + d_i \lambda^2 P_+(x; \lambda)^{-1} \tilde{H}_i^+
(x)
\end{align*}
Furthermore, the conjugation operator $P_+(x; \lambda)$ has the form
\begin{equation}\label{projTheta}
P_+(x; \lambda) = I + \Theta_+(x; \lambda)
\end{equation}
where $\Theta_+(x; \lambda)$ has the uniform decay rate
\begin{equation}\label{Thetadecay}
|\Theta_+(x; \lambda)| \leq C e^{-\alpha_1 |x|} \\
\end{equation}
which also applies to derivatives with respect to $x$ and $\lambda$ (though with a different constant out front).

For the conjugation on $R^-$, rather than applying the conjugation lemma again on $R^-$, we will use the symmetries of our system to define the conjugation operator on $\R^-$ in terms of the conjugation operator on $\R^+$. We do this in a series of lemmas. First we prove a symmetry relation for $A(Q(x); \lambda)$.

\begin{lemma}\label{AQxsymmetrylemma}
Let $R$ be the standard reversor operator. Then
\begin{enumerate}[(i)]
\item $A(Q(x); \lambda) = -R A(Q(-x); -\lambda)R$
\item If $V(x)$ is a solution to $V'(x) = A(Q(x); \lambda) V(x)$ on $\R^+$, then $R V(-x)$ is a solution to $V'(x) = A(Q(x); -\lambda) V(x)$ on $\R^-$.
\end{enumerate}
\begin{proof}
Since just this one time we will be dealing with reversor operators on two difference spaces, let $R_{n}$ be the matrix for the standard reversor operator on $\R^n$. For part (i), using the symmetry relations $DF(RU) = -RDF(U)R$ and $Q(-x) = RQ(X)$, we have
\begin{align*}
-R_{2m+1} &A(Q(-x); -\lambda) R_{2m+1}
= \begin{pmatrix}-R_{2m} & 0 \\ 0 & -1 \end{pmatrix} 
\begin{pmatrix}
DF(Q(-x)) & \begin{pmatrix} 0 \\ \vdots \\ 0 \\ 1 \end{pmatrix} \\
\begin{pmatrix} -\lambda & 0 & \dots & 0 \end{pmatrix} & 0
\end{pmatrix} \begin{pmatrix}R_{2m} & 0 \\ 0 & 1 \end{pmatrix} \\
&= \begin{pmatrix}
-R_{2m} DF(Q(-x)) R_{2m} & \begin{pmatrix} 0 \\ \vdots \\ 0 \\ 1 \end{pmatrix} \\
\begin{pmatrix} \lambda & 0 & \dots & 0 \end{pmatrix} & 0
\end{pmatrix} 
= \begin{pmatrix}
DF(R_{2m}(Q(-x))) & \begin{pmatrix} 0 \\ \vdots \\ 0 \\ 1 \end{pmatrix} \\
\begin{pmatrix} \lambda & 0 & \dots & 0 \end{pmatrix} & 0
\end{pmatrix} \\
&= \begin{pmatrix}
DF(Q(x)) & \begin{pmatrix} 0 \\ \vdots \\ 0 \\ 1 \end{pmatrix} \\
\begin{pmatrix} \lambda & 0 & \dots & 0 \end{pmatrix} & 0
\end{pmatrix} \\ 
&= A(Q(x); \lambda)
\end{align*}

For part (ii), for $x \in \R^-$,
\begin{align*}
\frac{d}{dx} &\left[ R_{2m+1} V(-x) \right] = -R_{2m+1} V'(-x) \\
&= -R_{2m+1} A(Q(-x); \lambda) V(-x) \\
&= -R_{2m+1} A(Q(-x); \lambda) R_{2m+1} [ R_{2m+1} V(-x)] \\
&= A(Q(x); -\lambda) [R_{2m+1} V(-x)]
\end{align*}
\end{proof}
\end{lemma}

In the next lemma, we define the conjugation operator on $\R^-$ in terms of the conjugation operator on $\R^+$.

\begin{lemma}\label{conjRminuslemma}
For $x \in \R^-$ and $|\lambda| < \delta_1$, define $P_-(x; \lambda)$ by
\begin{equation}\label{defPminus}
P_-(x; \lambda) = RP_+(-x; -\lambda)R
\end{equation}
Then the substitutiton $V(x) = P_-(x; \lambda) Z(x)$ conjugates the ODE 
\begin{equation}\label{Veqminus}
V'(x) = A(Q(x); \lambda) V(x)
\end{equation}
into the constant-coefficient ODE 
\begin{equation}\label{Zeqminus}
Z'(x) = A(\lambda)Z(x).
\end{equation}
\begin{proof}
We substitute $V(x) = P_-(x; \lambda) Z(x)$ into \eqref{Veqminus}. For the LHS, we have
\begin{align*}
\frac{d}{dx}[P_-(x; \lambda) Z(x)] &= \frac{d}{dx}[RP_+(-x; -\lambda)R Z(x)] \\
&= -RP'_+(-x; -\lambda)R Z(x) + RP_+(-x; -\lambda)R Z'(x)
\end{align*}
For the RHS, using part (i) of Lemma \ref{AQxsymmetrylemma} and $R^2 = I$, we have
\begin{align*}
A(Q(x); \lambda)P_-(x; \lambda) Z(x) &= A(Q(x); \lambda)RP_+(-x; -\lambda)R Z(x) \\
&= -R[ -R A(Q(x); \lambda)R ]P_+(-x; -\lambda)R Z(x) 
\end{align*}
which simplifies to
\begin{align}\label{Zeqminus1}
A(Q(x); \lambda)P_-(x; \lambda) Z(x) &= -R A(Q(-x); -\lambda) P_+(-x; -\lambda)R Z(x). 
\end{align}
From the proof of the conjugation lemma,
\begin{align*}
P'_+(x; \lambda) = A(Q(x); \lambda)P_+(x; \lambda) - P_+(x; \lambda) A(\lambda)
\end{align*}
Rearranging this and evaluating it at $-x$ and $-\lambda$, 
\begin{align*}
 A(Q(-x); -\lambda)P_+(-x; -\lambda) = P'_+(-x; -\lambda) + P_+(-x; -\lambda) A(-\lambda)
\end{align*}
Substituting this into \eqref{Zeqminus1}, we have
\begin{align*}
A(Q(x); \lambda)&P_-(x; \lambda) Z(x) = -R [ P'_+(-x; -\lambda) + P_+(-x; -\lambda) A(-\lambda) ] R Z(x) \\
&= -R P'_+(-x; -\lambda) R Z(x) - R P_+(-x; -\lambda) R [ R A(-\lambda) R]Z(x) \\
&= -R P'_+(-x; -\lambda) R Z(x) + R P_+(-x; -\lambda) R A(\lambda) Z(x)
\end{align*}
where in the last line we used $-R A(-\lambda) R = A(\lambda)$, which we show either by direct computation or by taking $x \rightarrow \infty$ in part (i) of Lemma \ref{AQxsymmetrylemma}. Equating the LHS and RHS, we have
\begin{align*}
-RP'_+(-x; -\lambda)&R Z(x) + RP_+(-x; -\lambda)R Z'(x) \\
&= -R P'_+(-x; -\lambda) R Z(x) + R P_+(-x; -\lambda) R A(\lambda) Z(x)
\end{align*}
which simplifies to
\begin{align*}
RP_+(-x; -\lambda)R Z'(x) = R P_+(-x; -\lambda) R A(\lambda) Z(x)
\end{align*}
Since $R$ and $P_+(-x; -\lambda)$ are invertible, this finally reduces to 
\begin{align*}
Z'(x) = A(\lambda) Z(x)
\end{align*}
which is the equation we want.
\end{proof}
\end{lemma}

Using the conjugation operators $P_\pm(x; \lambda)$ and making the substitution $W_i^\pm(x) = P_\pm(x; \lambda) Z_i^\pm(x)$ in \eqref{eigsystem}, we obtain the conjugated system
\begin{subequations}
\begin{align}
(Z_i^\pm(x))' = A(\lambda) Z_i^\pm(x) &+ P_\pm(x; \lambda)^{-1} G_i^\pm(x) P_\pm(x; \lambda) Z_i^\pm(x) + \lambda^2 d_i P_\pm(x; \lambda)^{-1} \tilde{H}_i^\pm(x) \label{systemZ} \\
P_+(X_i; \lambda) Z_i^+(X_i) &- P_-(-X_i; \lambda) Z_{i+1}^-(-X_i; \lambda) = D_i d \label{systemmiddle} \\
P_\pm(0; \lambda) Z_i^\pm(0) &\in Y^+ \oplus Y^- \oplus \C \Psi(0) \oplus \C \Psi^c(0) \label{systemcenter1} \\
P_+(0; \lambda) Z_i^+(0) - P_-(0; \lambda) Z_i^-(0) &\in \C \Psi(0) \oplus \C \Psi^c(0) \label{systemcenter2}
\end{align}
\end{subequations}
and the jump conditions \eqref{jumpxi} become
\begin{equation}\label{jumpcondZ}
\begin{aligned}
\langle \Psi(0), P_+(0; \lambda) Z_i^+(0) - P_-(0; \lambda) Z_i^-(0) \rangle &= 0 \\
\langle \Psi^c(0), P_-(0; \lambda) Z_i^+(0) - P_-(0; \lambda) Z_i^-(0) \rangle &= 0
\end{aligned}
\end{equation}

\section{Exponential Trichotomy}

Since $A(\lambda)$ is a constant coefficient matrix, we know exactly how solutions $Z' = A(\lambda)Z$ evolve. Let $E^{u/s/c}(0)$ be the stable, unstable, and center eigenspaces of $A(0)$. $E^s(0)$ and $E^u(0)$ are $m$-dimensional, and $E^c(0)$ is 1-dimensional. For $|\lambda| < \delta$, let $E^{u/s/c}(\lambda)$ be the corresponding eigenspaces of $A(\lambda)$, which have the same dimensions as $E^{u/s/c}(0)$ since $\lambda$ is small. In particular, 
\[
E^c(\lambda) = \spn\{ V_0(\lambda) \},
\]
where $V_0(\lambda)$ is defined in Lemma \ref{nulambdalemma}. Although $E^c(\lambda)$ is not a true center eigenspace if $\Re \nu(\lambda) \neq 0$, we retain this notation for convenience. Let $P^{u/s/c}(\lambda)$ be eigenprojections for the eigenspaces $E^{u/s/c}(\lambda)$. These are smooth in $\lambda$. 

Next, we set up an exponential trichotomy for this system. Let
\begin{equation}
\Phi(x, y; \lambda) = e^{A(\lambda)(x-y)}
\end{equation}
be the evolution of the constant-coefficient ODE
\[
Z' = A(\lambda) Z
\]
and let $\Phi^{u/s/c}(x, y; \lambda) = \Phi(x, y; \lambda)P^{u/s/c}(\lambda)$ be the evolutions on the respective eigenspaces. For $|\lambda| < \delta$, $|\nu(\lambda)| < \eta$, and we have bounds
\begin{equation}\label{Zevolbounds}
\begin{aligned}
|\Phi^s(x, y; \lambda)| &\leq C e^{-\alpha(x - y)} \\
|\Phi^u(x, y; \lambda)| &\leq C e^{-\alpha(y - x)} \\
|\Phi^c(x, y; \lambda)| &\leq C e^{\eta|x - y|} 
\end{aligned}
\end{equation}
This is an exponential trichotomy on $\R$. Since $E^c(\lambda)$ is one-dimensional, the center evolution $\Phi^c(x, y; \lambda)$ is given by
\begin{align}\label{centerevol}
\Phi^c(x, y; \lambda) V &= e^{\nu(\lambda)(x - y)} P^c(\lambda)V
\end{align}

We relate the evolution of the conjugated system to that of the original system in the following way. Recall that variational for the linearization of the PDE about the primary pulse $Q(x)$ is given by 
\begin{align}\label{vareqstab}
V'(x) &= A(Q(x); 0) V(x)
\end{align}
Let $\tilde{\Phi}(y, x)$ be the evolution operator for the variational equation. The conjugation lemma gives the following relationship between $\tilde{\Phi}(y, x)$ and $\Phi(y, x; 0)$.
\begin{align*}
\tilde{\Phi}(y, x) &= P_+(y; 0) \Phi(y, x; 0) P_+(x; 0)^{-1} && x, y \geq 0 \\
\tilde{\Phi}(y, x) &= P_-(y; 0) \Phi(y, x; 0) P_-(x; 0)^{-1} && x, y \leq 0
\end{align*}

Furthermore, the conjugation operator induces exponential trichotomies on $\R^\pm$ for the variational equation. The stable, unstable, and center projections for this trichotomy are given by
\begin{equation}\label{trichotomyprojunconj}
\tilde{P}^{s/u/c}_\pm(x) = P_\pm(x; 0) P^{s/u/c}(0) P_\pm(x, 0)^{-1}
\end{equation}
The center subspaces on $\R^+$ and $\R^-$ are both one-dimensional and are given by $\C V^c(x)$. 

To conclude this section, we will derive an expression for the center projections $\tilde{P}^c_\pm(x)$. From \cref{trichotomyprojunconj},
\begin{align*}
\tilde{P}^c_\pm(x) u &= P_\pm(x; 0) P^c(0) P_\pm(x, 0)^{-1} u \\
&= P_\pm(x; 0) \langle W_0, P_\pm(x, 0)^{-1} u \rangle V_0 \\
&= \langle W_0, P_\pm(x, 0)^{-1} u \rangle P_\pm(x; 0) V_0\\
&= \langle W_0, P_\pm(x, 0)^{-1} u \rangle V^c(x)
\end{align*}
thus we have
\begin{align}\label{centerproj1}
\tilde{P}^c_\pm(x) u 
&= \langle W_0, P_\pm(x, 0)^{-1} u \rangle V^c(x)
\end{align}\
Since $\Psi^(x) = W_0$ is a constant solution to the adjoint variational equation, we have a simpler form for the center projection for the variational equation. 

\begin{lemma}\label{centerprojlemma}
For the trichotomies of \cref{vareqstab} on $\R^\pm$, the center projection is given by 
\begin{align}\label{centerproj}
\tilde{P}^c_\pm(x) U &= \langle W_0, U \rangle V^c(x)
\end{align}
\begin{proof}
From \cref{corr:adjconj}, $[P_\pm(x, 0)^{-1}]^*$ is the conjugation operator for the adjoint variational equation. In particular, this implies that $[P_\pm(x, 0)^{-1}]^* W_0$ is a solution to the adjoint variational equation. Since $[P_\pm(x, 0)^{-1}]^* \rightarrow I$ as $x \rightarrow \pm \infty$, 
\[
[P_\pm(x, 0)^{-1}]^* W_0 \rightarrow W_0 \text{ as }x \rightarrow \pm \infty
\]
By \cref{varadjsolutions}, the constant solution $\Psi^c(x) = W_0$ is the unique bounded solution to the adjoint variational equation which decays to $W_0$ (constant multiples will decay to something else). Thus we conclude that $[P_\pm(x, 0)^{-1}]^* W_0 = W_0$. Using this with \cref{centerproj1},
\begin{align*}
\tilde{P}^c_\pm(x) u 
&= \langle W_0, P_\pm(x, 0)^{-1} u \rangle V^c(x) \\
&= \langle [P_\pm(x, 0)^{-1}]^*W_0, u \rangle V^c(x) \\
&= \langle W_0, u \rangle V^c(x) 
\end{align*}
\end{proof}
\end{lemma}

To conclude this section, we use use \cref{centerprojlemma} to derive expansions which will be useful later.

\begin{lemma}\label{W0projlemma}
For all $U \in \R^{2m+1}$, we have the following.
\begin{enumerate}[(i)]
	\item $\langle W_0, P_\pm(x, 0) P^{s/u}(0) U \rangle = 0$
	\item $\langle W_0, \Theta_\pm(x, 0) P^{s/u}(0) U \rangle = 0$
	\item $\langle W_0(\lambda), \Theta_\pm(x, \lambda) P^{s/u}(\lambda) U \rangle  = \mathcal{O}(|\lambda|( e^{-\alpha_1 |x|} + |\lambda|)|U|)$
	\item $\langle W_0, P_\pm(0; \lambda) P^{s/u}(0) U \rangle = \mathcal{O}(|\lambda||U|)$
	\item $P^c(0) G_i^\pm(x) U = \langle W_0, G_i^\pm(x) U\rangle = 0$
\end{enumerate}
\begin{proof}
By Lemma \ref{centerprojlemma}, $\tilde{P}_\pm^c(x)U = \langle W_0, U \rangle V^c(x)$, thus $\langle W_0, U \rangle = 0$ for all $U$ in the range of the stable and unstable projections of the trichotomy for \cref{vareqstab}. These ranges are given by $P_\pm(x, 0) E^{s/u}(0)$. This proves (i). For (ii), 
\begin{align*}
\langle W_0, \Theta_\pm(x, 0) P^{s/u}(0) U\rangle &=
\langle W_0, (I - P_\pm(x, 0)) P^{s/u}(0) U\rangle \\
&= \langle W_0, P^{s/u}(0) U \rangle - \langle P_\pm(x, 0)P^{s/u}(0) U \rangle \\
&= 0
\end{align*}
by part (i) and the fact that $\langle W_0, \cdot \rangle$ is the projection on $E^c(0)$. For (iii), since the eigenprojections $P^{s/u}(\lambda)$ are matrices which are smooth in $\lambda$, we can expand them in a Taylor series about $\lambda = 0$ to get 
\[
P^{s/u}(\lambda) = P^{s/u}(0) + \mathcal{O}(\lambda).
\] 
Using the Taylor series for $W_0(\lambda)$ from \cref{nulambdalemma} and the decay rates \cref{Thetadecay} for the conjugation operators,
\begin{align*}
\langle &W_0(\lambda), \Theta_\pm(x; \lambda) P^{s/u}(\lambda) U \rangle = \langle W_0 + \mathcal{O}(\overline{\lambda}), \Theta_\pm(x; 0) + \mathcal{O}(e^{-\alpha_1 |x|}|\lambda|))(P^{s/u}(0) + \mathcal{O}(\lambda)) U \rangle \\
&= \langle W_0, (I + \Theta_\pm(x, 0))P^{s/u}(0)U \rangle +
\mathcal{O}(|\lambda||U|) \\
&= \langle W_0, P_\pm(x, 0)P^{s/u}(0)U \rangle +
\mathcal{O}(|\lambda|( e^{-\alpha_1 |x|} + |\lambda|)|U|)\\
&= \mathcal{O}(|\lambda|( e^{-\alpha_1 |x|} + |\lambda|)|U|) \\
\end{align*}
where for the last equality we used part (ii). 

For (iv), we expand the conjugation operators in $\lambda$ to get
\begin{align*}
\langle W_0, &P_\pm(0; \lambda)P^{s/u}(\lambda) U \rangle =
\langle W_0, (P_\pm(0; 0) + \mathcal{O}(|\lambda|))(P^{s/u}(0) + \mathcal{O}(|\lambda|)) U) \\
&= \langle W_0, P_\pm(0; 0) P^{s/u}(0)U \rangle + \mathcal{O}(|\lambda||U|) \\
&= \langle W_0, \tilde{P}_\pm(0) U \rangle + \mathcal{O}(|\lambda||U|) \\
&= \mathcal{O}(|\lambda||U|) 
\end{align*}
where in the last line we used Lemma \ref{centerprojlemma}. For (v), since the bottom row of $G_i^\pm(x)$ is all zeros, the last component of $G_i^\pm(x) U$ is 0 for all $U$. It follows that $\langle W_0, G_i^\pm(x) U\rangle = 0$. 
\end{proof}
\end{lemma}

\section{Solutions in center subspace}

The constant-coefficient ODE
\[
Z(x)' = A(\lambda)Z(x)
\]
has a solution $Z(x) = V_0(\lambda)e^{\nu(\lambda)x}$. In the next lemma, we show that the ODE
\[
V(x)' = A(Q(x); \lambda)V(x)
\]
has solutions on $\R^+$ and $\R^-$ which approach $V_0(\lambda)e^{\nu(\lambda)x}$ as $x \rightarrow \pm \infty$.

\begin{lemma}\label{lemma:Vpm}
For sufficiently small $|\lambda|$ and any $\alpha_1 < \alpha_0$, the equation $V' = A(Q_n(x); \lambda)V$ has solutions $V^\pm(x; \lambda)$ on $\R^\pm$ which are given by
\begin{align}\label{Vpmlambda}
V^\pm(x; \lambda) &= e^{\nu(\lambda)x}(V_0(\lambda) + V_1^\pm(x; \lambda)),
\end{align}
where
\begin{equation}\label{Vpmdecay}
|V_1^\pm(x; \lambda)| \leq C e^{-\alpha_1 |x|}
\end{equation}
and we have the symmetry relationship
\begin{equation}\label{Vpmsymmetry}
V^-(x; \lambda) = R V^+(-x; -\lambda).
\end{equation}
\begin{proof}
Let $Z(x) = e^{\nu(\lambda)x}V_0(\lambda)$, which is a solution to $Z'(x) = A(\lambda)Z$ contained in the center subspace $E^c(\lambda)$. Let
\[
V^+(x; \lambda) = P_+(x; \lambda) Z(x) = e^{\nu(\lambda)x}P_+(x; \lambda)V_0(\lambda)
\]
Then by the expansion \eqref{conjlemmaP} from the Conjugation Lemma,
\begin{align*}
V^+(x; \lambda) &= e^{\nu(\lambda)x}(I + \Theta_+(x; \lambda))V_0(\lambda) \\
&= e^{\nu(\lambda)x}( V_0(\lambda) + V_1^+(x; \lambda))
\end{align*}
where $V_1^+(x; \lambda) = \Theta_+(x; \lambda) V_0(\lambda)$. This is \eqref{Vpmlambda}, and the decay result \eqref{Vpmdecay} comes from the conjugation lemma.

Similarly, we define 
\begin{align*}
V^-(x; \lambda) &= P_-(x; \lambda) Z(x) \\
&= RP_+(-x; -\lambda)R e^{\nu(\lambda)x} V_0(\lambda) \\
&= e^{\nu(\lambda)x} R(I + \Theta_+(-x; -\lambda))R V_0(\lambda) \\
&= e^{\nu(\lambda)x}( V_0(\lambda) + R\Theta_+(-x; -\lambda) V_0(-\lambda) )
\end{align*}
Letting $V_1^-(x; \lambda) = R\Theta_+(-x; -\lambda) V_0(-\lambda)$, we have
\begin{align*}
V^-(x; \lambda) &= e^{\nu(\lambda)x}( V_0(\lambda) + V_1^-(x; \lambda))
\end{align*}
For the symmetry relation, using Lemma \ref{nulambdalemma}, we also have
\begin{align*}
V^-(x; \lambda) &= e^{\nu(\lambda)x}( V_0(\lambda) + R\Theta_+(-x; -\lambda) V_0(-\lambda) ) \\
&= e^{\nu(\lambda)x}( R V_0(-\lambda) + R\Theta_+(-x; -\lambda) V_0(-\lambda) ) \\
&= R e^{-\nu(\lambda)(-x)}( V_0(-\lambda) + \Theta_+(-x; -\lambda) V_0(-\lambda) ) \\
&= R e^{\nu(-\lambda)(-x)}( V_0(-\lambda) + \Theta_+(-x; -\lambda) V_0(-\lambda) ) \\
&= R V^+(-x; -\lambda)
\end{align*}
\end{proof}
\end{lemma}

We now use this result to provide the proof for the existence of $V^c(x)$ in part (i) of Lemma \ref{varadjsolutions}. It then follows that $V^\pm(x; 0) = V^c(x)$.

\begin{lemma}\label{lemma:Vcexists}
There exists a solution $V^c(x)$ to \eqref{vareq2} which is given by
\[
V^c(x) = V_0 + V^c_1(x)
\]
where $V^c_1(x) = \mathcal{O}(e^{-\alpha_1 |x|})$. In particular $V^c(x) \rightarrow V_0$ as $|x| \rightarrow \infty$. $V^c$ is symmetric with respect to the standard reversor operator $R$, i.e. $V^c(-x) = R V^c(x)$. Finally, $V^c(x) = V^\pm(x; 0)$, where $V^\pm(x; 0)$ is defined in \cref{lemma:Vpm}. 

\begin{proof}
We employ the following dimension counting argument. Recall that $\dim W^{s/u}(0) = m$ and $\dim W^c(0) = 1$, thus $\dim W^{cs}(0) = m + 1$, and $\dim W^{cu}(0) = m + 1$. $\Psi(0) \perp T_{Q(0)}W^{cs}(0) + T_{Q(0)}W^{cu}(0)$, so $\dim T_{Q(0)}W^{cs}(0) + T_{Q(0)}W^{cu}(0) \leq 2m$. By counting dimensions, this implies that $\dim T_{Q(0)}W^{cs}(0) \cap T_{Q(0)}W^{cu}(0) = 2$. Since $\dim T_{Q(0)}W^s(0) \cap T_{Q(0)}W^s(0) = 1$ by Lemma \ref{nondegenlemma}, we conclude that there exists $Y_0 \in T_{Q(0)}W^{cs}(0) \cap T_{Q(0)}W^{cu}(0)$ which is linearly independent from $Q'(0)$, and $Y^0 \notin T_{Q(0)}W^s(0) \cap T_{Q(0)}W^s(0)$.

Using \cref{lemma:Vpm}, let
\[
V^\pm(x) = V^\pm(x; 0) = V_0 + V_1^\pm(x)
\]
where $V_1^\pm(x) = \mathcal{O}(e^{-\alpha_0 |x|})$. From the construction in \cref{lemma:Vpm},
\[
V^\pm(x) = P_\pm(x; 0) V_0,
\]
and we also have the symmetry relation $V^-(x) = R V^+(-x)$. We will show that $V^+(0) = V^-(0)$.

By the trichotomy for the variational equation, at $x = 0$ there is a one-dimensional center subspace for $\R^+$ and a one-dimensional center subspace for $\R^-$. By the dimension counting argument above, both of the center subspaces must be spanned by $Y^0$, thus they are the same subspace. Using the projections \eqref{trichotomyprojunconj}, $V^\pm(0)$ are in these center subspaces, thus $V^\pm(0) \in \Span \{Y^0 \}$. In particular, this implies that $V^+(0)$ and $V^-(0)$ are scalar multiples of each other. 

From Lemma \ref{varadjsolutions}, $\Psi^c(x) = W_0$ is a solution to the adjoint variational equation on $\R$. By Lemma \ref{eigadjoint}(i), the inner product $\langle W_0, V^\pm(x) \rangle$ is constant in $x$. Sending $x \rightarrow \pm \infty$ and recalling that $V_0 = (1/c, 0, \dots, 0, 1)^T $ and $W_0 = (0, \dots, 0, 1)^T$,
\[
\langle W_0, V^+(0) \rangle = \langle W_0, V^-(0) \rangle
= \langle W_0, V(0) \rangle = 1
\]
This implies that the last component of $V^+(0)$ and $V^-(0)$ is 1. Since $V^+(0)$ and $V^-(0)$ are scalar multiples of each other, they must in fact be equal. Let
\[
V^c(x) = \begin{cases}
V^+(x) & x \geq 0 \\
V^-(-x) & x \leq 0 
\end{cases}
\]
which is well-defined since $V^+(0) = V^-(0)$ and has the properties stated in the lemma.
\end{proof}
\end{lemma}

In the next lemma, we characterize the derivative of $V^\pm(x; \lambda)$ with respect to $\lambda$.

\begin{lemma}\label{lemma:Vpmderiv}
The derivative $\partial_\lambda V^+(x; 0)$ satisfies the equation
\begin{equation}\label{Vderivsolves}
Y'(x) = A(Q(x)) Y(x) + B V^c(x).
\end{equation}
and $\partial_\lambda V^-(x; 0) = -R \partial_\lambda V^+(-x, 0)$. Let 
\begin{equation}\label{deftildeV}
V^+(x; \lambda) = e^{\nu(\lambda)x}\tilde{V}^+(x; \lambda).
\end{equation}
Then we have the relationship
\begin{equation}\label{VtildeVderiv}
\partial_\lambda V^+(x; 0)
= \partial_\lambda \tilde{V}^+(x; 0) + \frac{1}{c} x V^c(x) 
\end{equation}
Finally, $\partial_\lambda \tilde{V}^+(x; 0)$ satisfies the equation
\begin{equation}\label{tildeVderivsolves}
Y'(x) = A(Q(x)) Y(x) + \left( B - \frac{1}{c}I \right) V^c(x).
\end{equation}
and $\partial_\lambda \tilde{V}^-(x; 0) = -R \partial_\lambda \tilde{V}^+(-x, 0)$.

\begin{proof}
We only need to do this for $V^+(x; \lambda)$ since the result for $V^-(x; \lambda)$ follows by symmetry. Recall that $V^+(x; \lambda)$ solves the equation
\begin{equation}\label{Vplussolves1}
V^+(x; \lambda)' = A(Q(x); \lambda) V^+(x; \lambda)
\end{equation}
Taking the derivative of both sides with respect to $\lambda$, we have
\begin{align*}
[\partial_\lambda V^+(x; \lambda)]' 
&= A(Q(x); \lambda) \partial_\lambda V^+(x; \lambda) + [\partial_\lambda A(Q(x); \lambda)] V^+(x; \lambda) \\
&= A(Q(x); \lambda) \partial_\lambda V^+(x; \lambda) + B V^+(x; \lambda)
\end{align*}
At $\lambda = 0$, this becomes
\[
[\partial_\lambda V^+(x; 0)] = A(Q(x)) \partial_\lambda V^+(x; 0) + B V^c(x),
\]
thus $\partial_\lambda V^+(x; 0)$ solves \cref{Vderivsolves}. For the symmetry relationship, we have
\begin{align*}
\partial_\lambda V^-(x; \lambda) &= \partial_\lambda R V^+(-x, -\lambda) \\
&= -R \partial_\lambda V^+(-x, -\lambda)
\end{align*}
At $\lambda = 0$, this is
\begin{align*}
\partial_\lambda V^-(x; 0) 
&= -R \partial_\lambda V^+(-x, 0)
\end{align*}

Using \cref{deftildeV} and taking the derivative with respect to $\lambda$,
\begin{align*}
\partial_\lambda V^+(x; \lambda)
&= e^{\nu(\lambda)x} \partial_\lambda \tilde{V}^+(x; \lambda) + \nu'(\lambda)x \tilde{V}^+(x; \lambda) \\
&= e^{\nu(\lambda)x} \partial_\lambda \tilde{V}^+(x; \lambda) + \frac{1}{c} x \tilde{V}^+(x; \lambda) 
\end{align*}
Taking $\lambda = 0$ and noting that $\tilde{V}^+(x; 0) = V^c(x)$, we obtain \cref{VtildeVderiv}.

Finally, plug in \cref{deftildeV} to \cref{Vplussolves1} to get
\begin{align*}
e^{\nu(\lambda)x} [\tilde{V}^+(x; \lambda)]' + \nu(\lambda) e^{\nu(\lambda)x} \tilde{V}^+(x; \lambda) &= 
A(Q(x); \lambda) e^{\nu(\lambda)x} \tilde{V}^+(x; \lambda)
\end{align*}
which simplifies to
\begin{align*}
[\tilde{V}^+(x; \lambda)]' &= 
[A(Q(x); \lambda) - \nu(\lambda) I ] \tilde{V}^+(x; \lambda)
\end{align*}
Taking the derivative of this with respect to $\lambda$,
\begin{align*}
[\partial_\lambda \tilde{V}^+(x; \lambda)]' &= 
[A(Q(x); \lambda) - \nu(\lambda) I ] \partial_\lambda \tilde{V}^+(x; \lambda) + \partial_\lambda [A(Q(x); \lambda) - \nu(\lambda) I ]\tilde{V}^+(x; \lambda) \\
&= [A(Q(x); \lambda) - \nu(\lambda) I ] \partial_\lambda \tilde{V}^+(x; \lambda) + [B - \nu'(\lambda)I]\tilde{V}^+(x; \lambda) 
\end{align*}
At $\lambda = 0$, this becomes
\begin{align*}
[\partial_\lambda \tilde{V}^+(0; \lambda)]' &= A(Q(x))\partial_\lambda \tilde{V}^+(x; \lambda) + \left( B - \frac{1}{c}I\right) V^c(x),
\end{align*}
thus $\partial_\lambda \tilde{V}^+(x; \lambda)$ solves \cref{tildeVderivsolves}. The symmetry relationship for $\partial_\lambda \tilde{V}^-(0; \lambda)$ is the same as for $\partial_\lambda V^-(0; \lambda)$
\end{proof}
\end{lemma}

In the next lemma, we evaluate two important inner products involving $\partial_\lambda \tilde{V}^+(x; 0)$ which will show up in our analysis.

\begin{lemma}\label{lemma:VderivIPs}
We have the following inner products involving $\partial_\lambda \tilde{V}^+(0; 0)$. 
\begin{enumerate}[(i)]
\item
\begin{align*}
\langle W_0, \partial_\lambda \tilde{V}^+(0; 0) \rangle
= -\int_0^{\infty} \left(v_0(y) - \frac{1}{c}\right) dy
\end{align*}
which is finite.
\item $\langle \Psi(0), \partial_\lambda \tilde{V}^+(0; 0) \rangle = - M^c$,
where $M^c$ is the ``center'' Melnikov-type integral
\begin{equation}
M^c = \int_0^\infty q(y) v^c(y) dy
\end{equation}
with $q(y)$ the first component of $Q(y)$ and $v^c(y)$ the first component of $V^c(y)$.

\end{enumerate}
\begin{proof}
For convenience, let $Y(x) = \partial_\lambda \tilde{V}^+(x; 0)$. By Lemma \ref{lemma:Vpmderiv}, $Y(x)$ solves equation \cref{tildeVderivsolves}. Using the variation of constants formula and the exponential trichotomy projections \cref{trichotomyprojunconj}, we can formally write $Y(x)$ in integrated form as
\begin{equation*}
\begin{aligned}
Y(x) &= \tilde{\Phi}^s_+(x,0)\tilde{P}^s_+(0) Y(0) 
+ \int_0^x \tilde{\Phi}^s(x,y)\left( B - \frac{1}{c}I \right) V^c(y) dy \\
&+ \int_{\infty}^x \tilde{\Phi}^u(x,y)\left( B - \frac{1}{c}I \right) V^c(y) dy + \int_{\infty}^x \tilde{\Phi}^c(x,y)\left( B - \frac{1}{c}I \right) V^c(y) dy 
\end{aligned}
\end{equation*}
Using \cref{centerprojlemma}, this becomes
\begin{equation}\label{Yintegform}
\begin{aligned}
Y(x) &= \tilde{\Phi}^s_+(x,0)\tilde{P}^s_+(0) Y(0) 
+ \int_0^x \tilde{\Phi}^s(x,y)\left( B - \frac{1}{c}I \right) V^c(y) dy \\
&+ \int_{\infty}^x \tilde{\Phi}^u(x,y)\left( B - \frac{1}{c}I \right) V^c(y) dy + V^c(x) \int_{\infty}^x \langle W_0, \left( B - \frac{1}{c}I \right) V^c(y) \rangle dy 
\end{aligned}
\end{equation}
In order to prove that this is a valid expression for $Y(x)$, we need to show that the integrals converge for all $x$. The convergence of the first and second integrals is straightforward since $V^c(x)$ is bounded and we have exponentially decaying estimates for $\tilde{\Phi}^s(x,y)$ and $\tilde{\Phi}^u(x,y)$ for the direction in which we are integrating. Thus it remains to check the third integral. Let 
\[
V^c(x) = (v^c(x), \partial_x v^c(x), \dots, \partial_x^{2m-1} v^c(x), 1)
\]
Then it follows that
\[
\langle W_0, \left( B - \frac{1}{c}I \right) V^c(y) \rangle = v_0(y) - \frac{1}{c}
\]
By Lemma \cref{lemma:Vcexists}, $|V^c(y) - V_0|\leq Ce^{-\alpha_1 y}$. Since the first component of $V_0$ is $1/c$, $|v_0(y) - 1/c| \leq Ce^{-\alpha_1 |x|}$. Using this with the third integral of \cref{Yintegform}, 
\begin{align*}
\left| \int_{-\infty}^x \langle W_0, \left( B - \frac{1}{c}I \right) V^c(y) \rangle dy \right| 
\leq C \int_x^{\infty} e^{-\alpha_1 y} dy = C \frac{e^{-\alpha_1 x}}{\alpha_1}
\end{align*}
which is finite for all $x$. Thus the expression \cref{Yintegform0} is valid. Evaluating this at $x = 0$, we have
\begin{equation}\label{Yintegform0}
\begin{aligned}
Y(0) &= \tilde{P}^s_+(0) Y(0) 
+ \int_{\infty}^0 \tilde{\Phi}^u(0,y)\left( B - \frac{1}{c}I \right) V^c(y) dy + V^c(0) \int_{\infty}^0 \langle W_0, \left( B - \frac{1}{c}I \right) V^c(y) \rangle dy 
\end{aligned}
\end{equation}

For the inner product with $\Psi^c(0) = W_0$, only the center integral term survives, so we have
\begin{align*}
\langle W_0, Y(0) \rangle
&= \langle W_0, V^c(0) \rangle \int_{\infty}^0 \left(v_0(y) - \frac{1}{c}\right) dy 
= -\int_0^{\infty} \left(v_0(y) - \frac{1}{c}\right) dy 
\end{align*}
which is finite. For the inner product with $\Psi(0)$, only the first integral term survives. First, we note that 
\begin{align*}
\tilde{\Phi}^u(0,y) V^c(y) = \tilde{P}_+^u(y) \tilde{P}_+^c(y)V^c(y) = 0
\end{align*}
Thus for the inner product with $\Psi(0)$ we have
\begin{align*}
\langle \Psi(0), Y(0) \rangle
&= \int_{\infty}^0 \langle \Psi(0), \tilde{P}_+^u(0) \tilde{\Phi}(0,y) B V^c(y) \rangle dy \\
&= \int_{\infty}^0 \langle \tilde{P}_+^u(0)^*\Psi(0), \tilde{\Phi}(0,y) B V^c(y) \rangle dy \\
\end{align*}

All that remains is to evaluate the $\tilde{P}_+^u(0)^*$. Since
\begin{align*}
\ker \tilde{P}_+^u(0)^* &\perp \ran \tilde{P}_+^u(0) \\
\ran \tilde{P}_+^u(0)^* &\perp \ker \tilde{P}_+^u(0)
\end{align*}
and $\ker \tilde{P}_+^u(0) = T_{Q(0)} W^s(0) \oplus T_{Q(0)} W^c(0)$, $\tilde{P}_+^u(0)^*$ acts as the identity on $(T_{Q(0)} W^s(0) \oplus T_{Q(0)} W^c(0))^\perp$. Since $\Psi(0) \perp T_{Q(0)} W^s(0)$, $\Psi(0) \in \ran \tilde{P}_+^u(0)^*$, thus $\tilde{P}_+^u(0)^* \Psi(0) = \Psi(0)$. Substituting this above, we have
\begin{align*}
\langle \Psi(0), Y(0) \rangle
&= \int_{\infty}^0 \langle \Psi(0), \tilde{\Phi}(0,y) B V^c(y) \rangle dy \\
&= -\int_0^\infty \langle \Psi(y), B V^c(y) \rangle dy \\
&=-\int_0^\infty q(y) v^c(y) dy
\end{align*}
\end{proof}
\end{lemma}

As a corollary, we use the previous lemma to evaluate two important inner products.

\begin{corollary}\label{lemma:VpmPsiIP}
\begin{equation}\label{VpmIPs}
\begin{aligned}
\langle \Psi^c(0), V^\pm(0; \lambda) \rangle &= 1 + \mathcal{O}(|\lambda|) \\
\langle \Psi(0), V^+(0; \lambda) \rangle &= -M^c \lambda + \mathcal{O}(|\lambda|^2) \\
\langle \Psi(0), V^-(0; \lambda) \rangle &= M^c \lambda + \mathcal{O}(|\lambda|^2)
\end{aligned}
\end{equation}
where $p_1 = \langle \Psi(0), \partial_\lambda V^+(0; 0) \rangle$.
\begin{proof}
Using Lemma \ref{centersolutionslemma} and Lemma \ref{varadjsolutions}, we can expand $V^+(0; \lambda)$ in a Taylor series about $\lambda = 0$ to get
\begin{align*}
V^+(0; \lambda) &= V^+(0; 0) + \partial_\lambda V^+(0; 0) + \mathcal{O}(|\lambda|^2) \\
&= V^c(0) + \partial_\lambda V^+(0; 0) \lambda + \mathcal{O}(|\lambda|^2) 
\end{align*}
For the inner product with $\Psi^c(0)$,
\begin{align*}
\langle \Psi^c(0), V^\pm(0; \lambda) \rangle &= 
\langle \Psi^c(0), V^c(0) \rangle + \mathcal{O}(|\lambda|) \\
&= 1 + \mathcal{O}(|\lambda|)
\end{align*}
For the inner product $\langle \Psi(0), V^+(0; \lambda) \rangle$, we use \cref{lemma:VderivIPs} and \cref{VtildeVderiv} to get
\begin{align*}
\langle \Psi(0), V^+(0; \lambda) \rangle 
&= \langle \Psi(0), V^c(0) \rangle + \langle \Psi(0), \partial_\lambda V^+(0; 0) \lambda \rangle + \mathcal{O}(|\lambda|^2) \\
&= (\partial_\lambda \tilde{V}^+(0; 0)) \lambda + \mathcal{O}(|\lambda|^2) \\
&= -M^c \lambda + \mathcal{O}(|\lambda|^2)
\end{align*}
For the inner product $\langle \Psi(0), V^-(0; \lambda) \rangle$, using \cref{lemma:Vpm} and \cref{lemma:VderivIPs},
\begin{align*}
\langle \Psi(0), V^-(0; \lambda) \rangle &= 
\langle \Psi(0), R V^+(0; -\lambda) \rangle \\
&= \langle R \Psi(0), -\partial_\lambda V^+(0; 0) \lambda \rangle + \mathcal{O}(|\lambda|^2) \\
&= -\lambda \langle \Psi(0), \partial_\lambda \tilde{V}^+(0; 0) \rangle + \mathcal{O}(|\lambda|^2) \\
&= M^c \lambda + \mathcal{O}(|\lambda|^2)
\end{align*}
\end{proof}
\end{corollary}


\iffulldocument\else
	\bibliographystyle{amsalpha}
	\bibliography{thesis.bib}
\fi

\end{document}